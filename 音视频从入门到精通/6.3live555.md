# 2.live555安装
- [编译安装](#1)
- [live555使用](#2)
- [源码分析](#3)
- [应用](#4)

参考链接：  
1.编译安装  
https://blog.csdn.net/aobai219/article/details/5841951

## <a id="1">编译安装与测试</a>
- 1、编译安装  
下载源码下，解压。  
./genMakefiles linux-64bit     
make  
make install   

- 2、运行  
cd mediaServer  
./live555MediaServer。各种测试代码在testProg目录下。如果没出什么错的话，那么恭喜你安装成功！  
安装成功时，会在当前目录生成一个mediaServer文件夹。    

- 3.测试  
cd mediaServer  ，./live555MediaServer。请先打开配置防火墙打开端口 554 和 8000 ，如果是云服务器，请注意云服务器供应商是否有安全组等设置，也需要到安全组配置端口的。下面上传文件到live555MediaServer同级目录。

## <a id="2">live555使用</a>
- [openRTSP](#2.1)
- [live555MediaServer](#2.2)
- [testRTSPClient](#2.3)      


参考链接：  
1.live555中openRTSP用法  
https://www.cnblogs.com/lifan3a/articles/7284204.html  

### <a id="2.1">openRTSP</a>
参考链接：  
1.官方链接  
http://www.live555.com/openRTSP/  

openRTSP  
openRTSP是一个命令行程序，它可以用来打开，流化，接收并且录制指定的RTSP视频链接媒体流(如rtsp://开头的URL)。(一个相关的程序“playSIP”可以用来播放或者录制一个SIP会话)。  
```shell
Basic operation  基本操作
Playing without receiving 播放不接收
Playing-time options 播放时选项
Streaming access-controlled sessions 流式访问控制选项
Outputting a ".mov", ".mp4", or ".avi"-format file 输出一个mov, mp4, avi 格式文件
Periodic file output  周期的文件输出
'Trick play' options “特技播放”选项
Other options  其他选项
A note about RealAudio and RealVideo sessions 关于RealVideo和RealAudio会话注意事项
Source code  源码
Support and customization 支持与定制
Summary of command-line options 选项汇总

Summary of command-line options
(for "openRTSP" and "playSIP")
-4 output a '.mp4'-format file (to 'stdout', unless the "-P <interval-in-seconds>" option is also given)
输出一个“mp4”格式文件（到'标准输出'，除非同时有选项“-P <interval-in-seconds>”给定区间时间间隔）

-a play only the audio stream (to 'stdout', unless the "-P <interval-in-seconds>" option is also given)
只输出音频流（到'标准输出'，除非同时有选项“-P <interval-in-seconds>”给定区间时间间隔）

-A <codec-number> specify the static RTP payload format number of the audio codec to request from the server 
("playSIP" only)
指定从服务器请求的音频编解码器的静态RTP负载格式数量（仅用于“playSIP”）

-b <buffer-size> change the output file buffer size
更改输出文件的缓冲区大小 

-B <buffer-size> change the input network socket buffer size
更改输入网络套接字缓冲区大小

-c play continuously
连续播放 

-C Explicitly ask for a multicast stream even if the server's "DESCRIBE" response doesn't specify a multicast address. (Note that not all servers will support this.) ("openRTSP" only)
明确要求多播流，即使服务器的“DESCRIBE”响应不指定多播地址。（注意，并非所有的服务器都将支持此功能。）（仅对于“openRTSP”）

-d <duration>  specify an explicit duration
指定一个明确的持续时间

-D <maximum-inter-packet-gap> specify a maximum period of inactivity to wait before exiting
指定退出之前要等待的最长非活动状态时间

-f <frame-rate> specify the video frame rate (used only with "-q", "-4", or "-i")
指定的视频帧速率（仅用于“-q”，“-4”或“ -i”）

-F <fileName-prefix> specify a prefix for each output file name
指定每个输出文件名前缀

-g <user-agent-name> specify a user agent name to use in outgoing requests
指定输出请求中使用的用户代理名

-h <height>  specify the video image height (used only with "-q", "-4", or "-i")
指定视频图像的高度（仅用于“-q”，“-4”或“ -i”）

-H output a QuickTime 'hint track' for each audio/video track (used only with "-q" or "-4")
为每个音频/视频轨道输出QuickTime的“索引轨道”

-i output a '.avi'-format file (to 'stdout', unless the "-P <interval-in-seconds>" option is also given)
输出一个“avi”格式文件（到'标准输出'，除非同时有选项“-P <interval-in-seconds>”给定区间时间间隔）

-I <interface-name-or-address> specify a particular network interface on which to receive data
指定要接收数据的特定网络接口

-k <username> <password> specify a user name and password thats required to authenticate an incoming "REGISTER" command (used with "-R" only)
指定验证到来的“REGISTER”命令需要的用户名和密码（仅和“-R”使用） 

-l try to compensate for packet losses (used only with "-q", "-4", or "-i")
尽量弥补丢包（仅和“-q”，“-4”或“ - i”使用）

-m output each incoming frame into a separate file
输出的每个到来的帧到一个单独的文件

-M <MIME-subtype> specify the MIME subtype of a dynamic RTP payload format for the audio codec to request from the server ("playSIP" only)
为音频编解码器指定一个动态的RTP负载格式的MIME子类型来请求服务器（仅用于“playSIP”）

-n be notified when RTP data packets start arriving
RTP数据包到达时通知

-o request the servers command options, without sending "DESCRIBE" ("openRTSP" only)
请求服务器的命令选项，而不发送“DESCRIBE”（仅用于“openRTSP”）

-O dont request the servers command options; just send "DESCRIBE" ("openRTSP" only)
不请求服务器的命令选项;只要发送“DESCRIBE”（仅用于“openRTSP”）

-p <starting-port-number> specify the client port number(s)
指定客户端的端口号

-P <interval-in-seconds> write new output files every <interval-in-seconds> seconds
每一个时间区间写新的输出文件

-q output a QuickTime '.mov'-format file (to 'stdout', unless the "-P <interval-in-seconds>" option is also given)
输出一个“.mov”格式文件（到'标准输出'，除非同时有选项“-P <interval-in-seconds>”给定区间时间间隔）

-Q output 'QOS' statistics about the data stream (when the program exits)
输出关于数据流统计的QoS“（程序退出时）

-r play the RTP streams, but dont receive them
播放RTP流，但不接收他们

-R (or -R <port-number>) Waits for an incoming "REGISTER" command, specifying a "rtsp://" URL to play. This option is used instead of a "rtsp://" URL on the command line. ("openRTSP" only)
等待到来的“注册”命令，指定要播放端口号. 这个选项是用来代替命令行中的“rtsp://”URL。 （仅用于“openRTSP”）

-s <initial-seek-time> request that the server seek to the specified time (in seconds) before streaming
请求服务器在流化前搜索到指定的时间（以秒为单位） 用于“trick play”

-S <byte-offset> assume a simple RTP payload format (skipping over a special header of the specified size)
假设一个简单的RTP负载格式（跳过指定大小的特殊头）

-t stream RTP/RTCP data over TCP, rather than (the usual) UDP. ("openRTSP" only)
通过TCP流化RTP / RTCP数据，而不是通过（通常的）的UDP。 （仅用于“openRTSP”） 

-T <http-port-number> like "-t", except using RTSP-over-HTTP tunneling. ("openRTSP" only)
类似“-t”，除了使用RTSP-over-HTTP隧道。 （仅用于“openRTSP”） 

-u <username> <password> specify a user name and password for digest authentication
指定摘要式身份验证的用户名和密码 

-U <initial-absolute-seek-time> request that the server seek to the specified absolute time (format: "YYYYMMDDTHHMMSSZ" or "YYYYMMDDTHHMMSS.<frac>Z") before streaming
请求服务器在流化前搜索到指定的绝对时间

-v play only the video stream (to 'stdout', unless the "-P <interval-in-seconds>" option is also given)
只输出视频流（到'标准输出'，除非同时有选项“-P <interval-in-seconds>”给定区间时间间隔）

-V print less verbose diagnostic output
打印更加简洁诊断输出 

-w <width>  specify the video image width (used only with "-q", "-4", or "-i")
指定视频图像的宽度

-y try to synchronize the audio and video tracks (used only with "-q" or "-4")
尝试同步音频和视频轨道

-z <scale>  request that the server scale the stream (fast-forward, slow, or reverse play)
请求服务器缩放数据流（快进，慢速，或者反向播放）
```

比如：  
openRTSP -4 -d 15 rtsp://athnkk:athnkk@139.196.137.60/20190329203822_hint.mp4 >./a.mp4 

### <a id="2.2">live555MediaServer</a>
live555服务端程序，编译后在mediaServer目录下，make install安装后可以直接运行，在运行程序的目录下添加264文件，就可以完成点播了。播放地址为：   
rtsp://139.196.137.60:8554/yd.264    

### <a id="2.3">testRTSPClient</a>
live555客户端程序，编译后在testProgs目录下，make install安装后可以直接运行，运行如下：  
testRTSPClient rtsp://139.196.137.60:8554/yd.264   

## <a id="3">源码分析</a>
- [1.Live555流媒体模块及服务端的处理流程](#3.1)
- [2.continueAfterDESCRIBE函数的流程](#3.2)
- [3.testRTSPClient建立RTSP连接的过程（RTSP客户端）](#3.3)
- [4.VLC对Live555 RTSP视频直播稳定性问题](#3.4)
- [5.live555修改接受缓存](#3.5)
- [6.live555处理H264的数据和live555播放](#3.6)



参考链接：   
1.Live555流程分析  
https://blog.csdn.net/u013898698/article/details/56284534?locationNum=12&fps=1   

2.event  
https://blog.csdn.net/windeal3203/article/details/52770759   

3.testRTSPClient建立RTSP连接的过程（RTSP客户端）  
https://www.cnblogs.com/jqctop1/p/4396301.html   

4.基于Live555实现RtspServer及高清高码率视频传输优化  
https://www.cnblogs.com/haibindev/p/9503639.html  


UsageEnvironment目录：生成的静态库为libUsageEnvironment.lib，这个库主要包含一些基本数据结构以及工具类的定义  
groupsock目录：生成的静态库为libgroupsock.lib，这个库主要包含网络相关类的定义和实现  
liveMedia目录：生成的静态库为libliveMedia.lib，这个库包含了Live555核心功能的实现  
BasicUsageEnvironment目录：生成的静态库为libBasicUsageEnvironment.lib，这个库主要包含对UsageEnvironment库中一些类的实现  

<img src="./image/6-6.jpg" style="zoom:100%" />

### <a id="3.1">1.Live555流媒体模块及服务端的处理流程</a>
Live555的流媒体模块基本分为Source和Sink两大部分，当然他们也有一个共同的基类Medium。对服务器来说，Source为数据来源，Sink为数据输出，视频数据就通过MediaSource传递给MediaSink，最终通过RTPInterface网络传输给客户端。一下为服务端所用到的模块以及继承关系:
<img src="./image/6-7.jpg" style="zoom:100%" />

对高清高码率的视频画面，每一帧的视频数据就会比较大，这个数值往往会超出live555内部默认的内存处理大小，因为对于live555的优化，主要就是集中在内存缓冲大小的扩大，以及避免内存数据拷贝。以下为根据实际开发和测试所总结出来的有效的优化点：
- 扩展帧解析buffer大小，即BANK_SIZE，默认值为150k，根据传输的H264数据帧大小，至少设置为300k。否则超出大小，可能会被Live555抛弃。
- 增加OutPacketBuffer::maxSize大小，同样为了容纳超大帧数据，否则可能会导致数据丢失。
- 在RTPInterface中，增加socket发送缓冲区大小，即increaseSendBufferTo函数的参数值
- 对MultiFramedRTPSink::sendPacketIfNecessary中，可以直接调用sendNext尝试组建RTP报文发送数据包，这样修改的优点是已读取的数据会被尽快发送出去，不过也多占用一些线程时间。
- 对于应用程序将数据从自己的线程传递给Live555的时候，应该尽量减少内存拷贝，最好是通过内存池的形式，以避免拷贝内存阻塞Live555事件循环

### <a id="3.2">2.continueAfterDESCRIBE函数的流程</a>
#### 2.1continueAfterDESCRIBE函数的流程  

<img src="./image/6-2.png" style="zoom:100%" />

>注： session = MediaSession::createNew(*env, sdpDescription); //MediaSession类似于个个容器、链表用来管理MediaSubsession。  
在创建MediaSession的时候，顺便根据SDP信息中的m=部分创建了一个MediaSubsession对象。  
// We have a “m=” line, representing a new sub-session:  
MediaSubsession* subsession = createNewMediaSubsession();  
```c++
这里面重点说一下initiate函数  
subsession->initiate(simpleRTPoffsetArg)  
先根据sdp信息协商，采用UDP传输方式,创建两个Groupsock对象用于RTP/RTCP的接收，fRTPSocket，fRTCPSocket.  
之后在创建source，“fRTPSource”and “fReadSource”，和live555服务端原理一样，source和sink，分管生产和消费。  
createSourceObjects();  
这里根据返回的SDP信息创建对应的source，我这边是H264视频源，所以
 } else if (strcmp(fCodecName, "H264") == 0) {
    fReadSource = fRTPSource
      = H264VideoRTPSource::createNew(env(), fRTPSocket,
                      fRTPPayloadFormat,
                      fRTPTimestampFrequency);
      } 
然后创建一个fRTCPInstance = RTCPInstance::createNew，如果是多录视频fRTCPInstance 专门接受RTCP包，

基本上初始化工作已经完成了。但是疑问是我们接受server端发送的rtp包的前提，我们必须要监听一个端口才行啊。其实这一步是放在创建fRTPSocket，fRTCPSocket.的时候就完成了。
new Groupsock()->OutputSocket(env,port)->Socket(env,port)->setupDatagramSocket(fEnv,port)->bind ;
```

总结一下，获取SDP信息之后，我们干了哪些事。  
1.创建MediaSession MediaSubsession  
2.创建套接字fRTPSocket，fRTCPSocket.  
3.bind接收端口，为后面做准备  
4.初始化一个大的接收buffer  
5.创建source  
6.创建RTCPInstance  

<img src="./image/6-3.jpg" style="zoom:100%" />

#### 2.2continueAfterDESCRIBE函数的流程  
1.live555源码分析---- DESCRIBE命令处理   
https://blog.csdn.net/gavinr/article/details/7026497    

```c++
DECRIBE命令的处理过程比较复杂，这里简单的概括一下
1)创建了session及subsession，一个媒体文件将对应一个session，媒体文件中的每一个流对应一个subssion。session中，记录了一个subsession的链表。
2)为了获取SDP信息，做了大量的工作，不但创建了sink、source等实例, 还需要从媒体文件中获取信息。需要注意的是，这里创建的sink、source只是临时的，只是为了获取SDP信。

调用DynamicRTSPServer::lookupServerMediaSession查找对应资源(URl)的session,  这个过程将实例化服务过程中所需的对像,sink,source等。接着调用session的generateSDPDescription,生成SDP信息。SDP很可能需要从文件中获取部分信息，所以在这其中必定有读文件的部分。然后就是组装响应包了。

DynamicRTSPServer是RTSPServer的子类，继承关系为：DynamicRTSPServer->RTSPServerSupportingHTTPStreaming->RTSPServer
调用RTSPServer::lookupServerMediaSession，查询streamName对应的session是否存在。若不存在，则需要创建新的session,并将其加入到链表中，否则直接返回已经存在的session。

session是通过hash表存放的，由RTSPServer类进行管理,fServerMediaSessions 为RTSPServer的成员，是一个Hash表，定义为HashTable* fServerMediaSessions。session添加到Hash表.Hash表中已经存在对应的session,则会移除原来的session

由于不同的媒体类媒，需要创建不同的session，为了便于修改，创建session的代码被放到一个独立的函数createNewSMS。ServerMediaSession::createNew将实例化一个ServerMediaSession对象.

live555支持的复合容器类型只有*.mpg、*.mkv、webm，可以看到程序为容器中的每一个流建立一个subseesion,然后通过ServerMediaSession::addSubsession函数，将subsession 加入到ServerMediaSession。

生成SDP的媒体描述部分,sdpLines为定义在ServerMediaSubsession中的纯虚函数，在OnDemandServerMediaSubsession::sdpLines()中实现。这个函数代码较少，但却完成了一些重要函数的调用，创建了source、RTPSink、Groupsock等实例。这里调用的createNewStreamSource函数，定义为OnDemandServerMediaSubsession类的一个纯虚函数, 不同类型的媒体有不同的实现。函数createNewRTPSink被定义为OnDemandServerMediaSubsession的一个虚函数。

上面的代码中还有一个问题，用H264VideoStreamFramer::getSPSandPPS函数获取SPS与PPS
```



### <a id="3.3">3.testRTSPClient建立RTSP连接的过程（RTSP客户端）</a>
参考链接：  
https://www.cnblogs.com/lidabo/p/4483524.html   

https://www.cnblogs.com/freezlz/p/5324220.html   

1.live555 env->taskScheduler().doEventLoop()解析/自己实现  
https://blog.csdn.net/zhangjikuan/article/details/38403401

以testRTSPClient.cpp为例讲解：   
Medium<-MediaSource<-FramedSource<-RTPSource<-MultiFramedRTPSource<-H264VideoRTPSource   
其中，我们要重点关注的类是下面几个：   
FramedSource,RTPSource,MultiFramedRTPSource。   

### <a id="3.4">4.VLC对Live555 RTSP视频直播稳定性问题</a>
参考链接：  
1.vlc  
http://www.itnotepad.cn/Home/Article/num/60.html  
2.vlc包  
http://ftp.icm.edu.pl/packages/videolan/  
3.vlc相关问题  
https://www.xuebuyuan.com/2080121.html  

若使用live555做为网络直播流媒体服务，当网络延迟造成播放速度快于流下载的速度，即doGetNextFrame函数调用时流还没有传输过来；此时，用户需要增加延迟N毫秒执行任务，不能直接return返回；否则公网/内网播放是很容易中断的（不是立即断，所以是很多人长时间纠结也不容易发现中断的原因），同时也会造成崩溃情况。   
总的来说：doGetNextFrame当没流时，不能直接return，一定要加延迟执行 。也不能什么 都不处理，直接让执行到函数中末尾FramedSource::afterGetting(this)代码（很容易崩溃）;  
```c
下面列出关键延迟执行代码（来自网络开发人员启发）
定义延迟执行回调函数
void LiveStreamSource::GetNextFrame(void* clientData)
{
 LiveStreamSource* obj=(LiveStreamSource*)clientData;
 if(obj!=NULL)
 {
  if (!obj->isCurrentlyAwaitingData()) return;
  obj->doGetNextFrame();
 }
}
接着，在doGetNextFrame函数中视具体情况适当位置增加
void LiveStreamSource::doGetNextFrame() {
//TODO 其他代
if(xxx)
{
  delayTask=envir().taskScheduler().scheduleDelayedTask(30000,GetNextFrame,this);//没数据延迟30毫秒
return;
}
//TODO 其他代码
}
```
 
### <a id="3.5">5.live555修改接受缓存</a>
参考链接：   
1.Live555:MultiFramedRTPSink::afterGettingFrame1(): The input frame data was too large for our buffer   
https://blog.csdn.net/xionglifei2014/article/details/90266162  

```cpp
优化：
- 1.在“live/liveMedia/StreamParser.cpp”中扩展帧解析buffer大小，即BANK_SIZE，默认值为150k，根据传输的H264数据帧大小，至少设置为300k。否则超出大小，可能会被Live555抛弃。
- 2.在“live/liveMedia/MediaSink.cpp”中增加OutPacketBuffer::maxSize大小，同样为了容纳超大帧数据，否则可能会导致数据丢失，设置为 600000。
- 1.在“live/liveMedia/MultiFramedRTPsource.cpp”中--129行MultiFramedRTPSourcez函数中，增加socket发送缓冲区大小，即increaseSendBufferTo函数的参数值--increaseRecieveBufferTo(env, RTPgs-> socketNUm(), 2000000)”

在调度方面的优化:MultiFramedRTPSink::sendPacketIfNecessary--402行
#if 0
    // We have more frames left to send.  Figure out when the next frame
    // is due to start playing, then make sure that we wait this long before
    // sending the next packet.
    struct timeval timeNow;
    gettimeofday(&timeNow, NULL);
    int secsDiff = fNextSendTime.tv_sec - timeNow.tv_sec;
    int64_t uSecondsToGo = secsDiff*1000000 + (fNextSendTime.tv_usec - timeNow.tv_usec);
    if (uSecondsToGo < 0 || secsDiff < 0) { // sanity check: Make sure that the time-to-delay is non-negative:
      uSecondsToGo = 0;
    }

    // Delay this amount of time:
    nextTask() = envir().taskScheduler().scheduleDelayedTask(uSecondsToGo, (TaskFunc*)sendNext, this);
#else
      sendNext(this);
#endif


国外网站解决的方法原文如下：
/////////////////////////////////////////////BUFFER SIZE PROBLEM///////////////////////////////////////////////////////////
    Generally, while running HD stream, one can face packet loss due to ridiculously large NAL units. This will create an error stating something like -> 
“MultiFramedRTPSink::afterGettingFrame1(): The input frame data was too large for our buffer size (60753).  34086 bytes of trailing data was dropped!  Correct this by increasing "OutPacketBuffer::maxSize" to at least 94839, *before* creating this 'RTPSink'.  (Current value is 60000.)”

    To solve this, you need to make the following changes to your code:

    1. Go to “live/liveMedia/MultiFramedRTPsource.cpp” and change “increaseRecieveBufferTo(env, RTPgs-> socketNUm(), 2000000)”
    2. Go to “live/liveMedia/StreamParser.cpp” and increase BANK_SIZE to 300000 or 600000.
(This will also help remove StreamParser Error that you might encounter!)
    3. In “live/liveMedia/MediaSink.cpp” change the value of unsigned OutPacketBuffer::max size to 600000.
    4. In “mediaServer/DynamicRTSPServer.cpp”, change OutPacketBuffer::maxSize to 600000 at all places where it is defined.
    5. *Most importantly, you need to change the code in your own program, more so than the libraries themselves. This means that if you just do what pt.5 tells you to, you can avoid pts 1-4. Therefore, in your code change OutPacketBuffer::maxsize to 600000 (Our code file is testH264VideoStreamer.cpp so we made the above change in that. Note: testH264VideoStreamer.cpp lies in testProgs directory). You NEED to change this value in your code before you create any RTP sinks.
    6. Also to allow for more holistic changes, go to “testProgs/playCommon.cpp” and change the value of fileSinkBufferSize to 600000.
 ///////////////////////////////////////////////WHAT ELSE CAN BE DONE////////////////////////////////////////////////////////
    It is more advisable to reduce the buffer size of your stream by increasing compression levels. Also you would need to increase the buffer size of vlc player to allow for smooth streaming. To do so, go to VLC-> Tools -> Preferences -> Select “Show All” -> Click on Stream Output -> Change “stream output muxer caching” to a suitable value in ms. (1500 - 3000)
```



### <a id="3.6">6.live555处理H264的数据和live555播放</a>
1.live555源码分析----H264的数据处理   
https://blog.csdn.net/gavinr/article/details/7042228   

```c++
一、live555处理H264的数据
主要包括从文件中读取数据进行并进行frame(NALU)的分割，然后对frame进行分片，这些工作都是在frame交给RTP sink之前完成的。MultiFramedRTP::packFrame函数进行分析。

getNextFrame是定义在FramedSource中的非虚函数，从source中获取下一个frame，然后调用回调函数afterGettingFrame。afterGettingFrame被定义为静态函数，因为在C++中类成员函数是不能作为回调用函数的。不过这里为什么要用回调函数回？
注意，对于H264来说，上面的fSource并不是MPEGVideoStreamFramer类，因为在 H264VideoRTPSink::continuePlaying()函数中改变了fSource的值。    

fSource被指向了H264FUAFragmenter类，这个类主要实现了H264按照RFC3984进行RTP分包，不过这里的实现每个RTP中最多只包含一个NALU，没有实现组合封包的情形。这个类的继承关系如下：H264FUAFragmenter->FramedFilter->FramedSource。很明显，这是一个filter,包装了MPEGVideoStreamFramer类的对像。

函数中首先调用了MPEGVideoStreamParser::parse函数，将一个完整的Frame分析出来，并copy到了fTo(fTo就是OutPacketBuffer中的缓冲区)中，这其中肯定也实现了从文件中读取数据的过程。这里的fNumTruncatedBytes变量需要注意，fNumTruncatedBytes>0的话，表明Frame的实际长度大于fTo的最大长度，这将导致数据丢失，这时就要考虑增加缓冲区的长度了。成功获取一个Frame后，将调用afterGetting函数处理后续工作。

先来看parse函数，parse是定义在MPEGVideoStreamParser中的纯虚函数，在子类H264VideoStreamParser中实现。parse主要是从文件的字节流中，分离出一个个的Frame,对于H264而言其实就是对一个个的NALU。*.264文件的格式非常简单，每个NALU以 0x00000001 作为起始符号(中间的NALU也可以以0x000001作为起始符)，顺序存放。



二、live555播放
continuePlaying()函数中调用Source类（以MultiFramedRTPSource为例，因为它以实现doGetFrame()函数）的getNextFrame()函数以得到发送数据，而getNextFrame()是通过调用doGetNextFrame()，继而是doGetNextFrame1()，最终在doNextFrame1中由语句fReorderingBuffer->getNextCompletedPacket()将存放在fReorderingBuffer中的数据取出交给Sink类来发送。

分析live555客户端解析play等命令流程 
首先RTSPClient 新建一个createNew，RTSPClient.cpp/RTSPClient 调用taskScheduler，新建task绑定incomingDataHandler，到handleResponseBytes，到handlePLAYResponse，等待接受到发送的play发送响应。具体rtsp交互流程可查看rtsp中相关章节。

rtsp时间戳计算方法
Rtsp的拖放和快放、慢放都是先pause，再play，根据play消息包中的range和scale参数来实现。

对应到live555中，pause时调用sendPauseCommand函数。Play时调用unsigned sendPlayCommand(MediaSubsession& subsession, responseHandler* responseHandler,double start = 0.0f, double end = -1.0f, float scale = 1.0f, Authenticator*authenticator = NULL)，在该函数中可以设置start，end（对应range，以秒为单位）和scale（对应快慢放）。end设为-1表示播放到文件结尾。在pause后的resume播放时，start可以设为-1，表示继续从当前位置播放；在seek时，start设为拖放到的播放时间点（以秒为单位）。

rtsp客户端对于每个收到的rtp数据包，可以根据时间戳调用doubleMediaSubsession::getNormalPlayTime(struct timeval const& presentationTime)获得当前相对文件开始（从0起始）的播放时间（秒）。

该函数的计算公式为：PlayStartTime+(currnpt-startnpt)*scale
PlayStartTIme：从收到服务器play应答中的range域中提取。
Currnpt：当前rtp包的时间戳
Startnpt：play后收到的第一个数据包的npt。

rtspserver中的rtp包时间戳实现：
每收到一个play请求后（seek和resume的play请求同样如此），取当前时间作为时间戳。以后每个数据包的时间戳计算公式为：play后第一个包的时间戳+当前包与第一个包在文件中的相对时间间隔。注意在Play的响应包中音视频的seq和rtptime分别要取pause时的值，否则客户端的时间条指示会不对。

在live555的rtspserver中，收到resume的play请求时（range域start为-1），返回的应答包不会置range域start时间，导致客户端提取PlayStartTIme为0。为了避免这种问题，客户端在resume的play请求中最好置start为当前播放时间，这样其实类似于seek操作了。
在读取文件结束时（音频或视频结束），构造RTCP  BYE包，通过RTCP通道往客户端发送一个BYE包，客户端会结束播放。
```



## <a id="4">应用</a>
- [rtsp直播基于live555的实现](#4.1)


### <a id="4.1">rtsp直播基于live555的实现</a>
参考链接：    
1.rtsp直播基于live555的实现    
https://blog.csdn.net/xiaoxiaoniaoge/article/details/50505425?depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1&utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromBaidu-1     

http://blog.csdn.net/nkmnkm （道长的文章，分析的很不错）   

http://blog.csdn.net/gavinr （这里面的文章容易让人理清思路）   

```c++
1）、在mediaSever目录下面有个live555MediaServer.exe，这是live555自带生成的服务器端，我们可以做一个简单的测试：
将一个264文件比如test.264拷贝到exe文件所在的目录下（就是mediaSever目录下）；   
双击打开这个exe服务器端；   
在另外一台机器上打开vlc;    

2）、还有一个目录就是testProgs，这里面的代码都是实例代 码，对于改写你自己需要的程序一定会有很大的借鉴作用，编译live555之后会产生testOnDemandRTSPServer.exe，这也是一个 服务器端，还有很多别的可以自己慢慢研究，后面设计的基于live555的直播的服务端就是借鉴于testOnDemandRTSPServer.cpp 来改写的。

```


## links
  * [目录](<音视频入门到精通目录.md>)