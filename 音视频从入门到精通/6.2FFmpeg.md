# 6.2FFmpeg.md
- [FFmpeg安装](#6.1)
- [FFmpeg命令行](#6.2)
- [FFmpeg函数](#6.3)
- [FFmpeg使用注意点](#6.4)
- [Window集成ffmpeg环境搭建](#6.5)
- [FFmpeg应用](#6.6)



## <a id="6.1">FFmpeg安装</a>
### 一般安装
```shell
./configure  --prefix=$PROJECT_DIR/3rdpart/ffmpeg/build \
            --prefix=/usr/local/ffmpeg 
            --enable-shared          \
            --enable-static          \
            --disable-doc
make -j4&& make install  
export PATH=$PROJECT_DIR/3rdpart/ffmpeg-4.2.2/build/bin:$PATH
export LD_LIBRARY_PATH=/home/lj/gy/MediaPlat/3rdpart/ffmpeg-4.2.2/build/lib

如果没有make install到usr/local目录下需要配置lib bin
/etc/profile  export PATH=/usr/local/ffmpeg/bin:$PATH    配置bin路径
/etc/ld.so.conf  /usr/local/ffmpeg/lib  ldconfig         配置lib路径
```

### 编译FFMpeg集成H264 H265
```shell
下载资源：
1.前往FFmpeg官方网站下载自己需要版本的FFMpeg:
https://ffmpeg.org/download.html

2.x264官方下载页:
https://www.videolan.org/developers/x264.html

3.如果下载最新版本，可以直接从如下地址获取:
ftp://ftp.videolan.org/pub/x264/snapshots/last_x264.tar.bz2

4.x265官方下载页:
https://bitbucket.org/multicoreware/x265/downloads/
```

#### 编译x264
./configure  --disable-asm --enable-shared
make -j4  
make install  

#### 编译x265
cd x265_3.1.1/source/  
cmake CMakeLists.txt  
make -j4  
make install  

#### ffmpeg编译选项
基本参数选项：  
<table><tbody><tr><td valign="top">
<p><strong><span style="color:rgb(49,132,155);">命令</span></strong></p>
</td>
<td valign="top">
<p><strong><span style="color:rgb(49,132,155);">帮助信息</span></strong></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--help</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>显示此帮助信息<span style="font-family:Calibri;">|print this message</span></p>
</td>
</tr><tr><td valign="top">
<p>--log[=FILE|yes|no]</p>
</td>
<td valign="top">
<p>记录测试并输出到<span style="font-family:Calibri;">config.err</span><span style="font-family:'宋体';">文件</span><span style="font-family:Calibri;">|log tests and output to FILE [config.err]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--prefix=PREFIX</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>安装程序到指定目录（默认<span style="font-family:Calibri;">/usr/local</span><span style="font-family:'宋体';">）</span><span style="font-family:Calibri;">|install in PREFIX [/usr/local]</span></p>
</td>
</tr><tr><td valign="top">
<p>--libdir=DIR</p>
</td>
<td valign="top">
<p>安装库到指定目录（默认<span style="font-family:Calibri;">prefix/lib</span><span style="font-family:'宋体';">）</span><span style="font-family:Calibri;">|install libs in DIR [PREFIX/lib]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--shlibdir=DIR</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>指定共享库路径（默认<span style="font-family:Calibri;">prefix/lib</span><span style="font-family:'宋体';">）</span><span style="font-family:Calibri;">|install shared libs in DIR [PREFIX/lib]</span></p>
</td>
</tr><tr><td valign="top">
<p>--incdir=DIR</p>
</td>
<td valign="top">
<p>指定<span style="font-family:Calibri;">includes</span><span style="font-family:'宋体';">路径（默认</span><span style="font-family:Calibri;">prefix/include/ffmpeg</span><span style="font-family:'宋体';">）</span><span style="font-family:Calibri;">|install includes in DIR[PREFIX/include/ffmpeg]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--mandir=DIR</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>指定<span style="font-family:Calibri;">man page</span><span style="font-family:'宋体';">路径（默认</span><span style="font-family:Calibri;">prefix/man</span><span style="font-family:'宋体';">）</span><span style="font-family:Calibri;">install man page in DIR [PREFIX/man]</span></p>
</td>
</tr><tr><td valign="top">
<p>--enable-mp3lame</p>
</td>
<td valign="top">
<p>启用<span style="font-family:Calibri;">mp3</span><span style="font-family:'宋体';">编码</span><span style="font-family:Calibri;">libmp3lame</span><span style="font-family:'宋体';">（默认关闭）</span><span style="font-family:Calibri;">enable MP3 encoding via libmp3lame[default=no]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--enable-libogg</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>启用<span style="font-family:Calibri;">ogg</span><span style="font-family:'宋体';">支持</span><span style="font-family:Calibri;">libogg</span><span style="font-family:'宋体';">（默认关闭）</span><span style="font-family:Calibri;">enable Ogg support via libogg [default=no]</span></p>
</td>
</tr><tr><td valign="top">
<p>--enable-vorbis</p>
</td>
<td valign="top">
<p>启用<span style="font-family:Calibri;">Vorbis</span><span style="font-family:'宋体';">支持</span><span style="font-family:Calibri;">libvorbis</span><span style="font-family:'宋体';">（默认关闭）</span><span style="font-family:Calibri;">enable Vorbis support via libvorbis [default=no]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--enable-faad</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>启用<span style="font-family:Calibri;">faad</span><span style="font-family:'宋体';">支持</span><span style="font-family:Calibri;">libfaad</span><span style="font-family:'宋体';">（默认关闭）</span><span style="font-family:Calibri;">enable FAAD support via libfaad [default=no]</span></p>
</td>
</tr><tr><td valign="top">
<p>--enable-faadbin</p>
</td>
<td valign="top">
<p>启用<span style="font-family:Calibri;">faad</span><span style="font-family:'宋体';">运行时链接支持（默认关闭）</span><span style="font-family:Calibri;">build FAAD support with runtime linking[default=no]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--enable-faac</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>启用<span style="font-family:Calibri;">faac</span><span style="font-family:'宋体';">支持</span><span style="font-family:Calibri;">libfaac</span><span style="font-family:'宋体';">（默认关闭）</span><span style="font-family:Calibri;">enable FAAC support via libfaac [default=no]</span></p>
</td>
</tr><tr><td valign="top">
<p>--enable-libgsm</p>
</td>
<td valign="top">
<p>启用<span style="font-family:Calibri;">GSM</span><span style="font-family:'宋体';">支持</span><span style="font-family:Calibri;">libgsm</span><span style="font-family:'宋体';">（默认关闭）</span><span style="font-family:Calibri;">enable GSM support via libgsm [default=no]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--enable-xvid</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>启用<span style="font-family:Calibri;">xvid</span><span style="font-family:'宋体';">支持</span><span style="font-family:Calibri;">xvidcore</span><span style="font-family:'宋体';">（默认关闭）</span><span style="font-family:Calibri;">enable XviD support via xvidcore [default=no]</span></p>
</td>
</tr><tr><td valign="top">
<p><span style="color:rgb(255,0,0);">--enable-</span><span style="color:rgb(255,0,0);">lib</span><span style="color:rgb(255,0,0);">x264</span></p>
</td>
<td valign="top">
<p><span style="color:rgb(255,0,0);">启用<span style="font-family:Calibri;">H.264</span><span style="font-family:'宋体';">编码（默认关闭）</span><span style="font-family:Calibri;">enable H.264 encoding via x264 [default=no]</span></span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--enable-mingw32</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>启用<span style="font-family:Calibri;">MinGW</span><span style="font-family:'宋体';">本地</span><span style="font-family:Calibri;">/</span><span style="font-family:'宋体';">交叉</span><span style="font-family:Calibri;">win</span><span style="font-family:'宋体';">环境编译</span><span style="font-family:Calibri;">|enable
 MinGW native/cross Windows compile</span></p>
</td>
</tr><tr><td valign="top">
<p>--enable-mingwce</p>
</td>
<td valign="top">
<p>启用<span style="font-family:Calibri;">MinGW</span><span style="font-family:'宋体';">本地</span><span style="font-family:Calibri;">/</span><span style="font-family:'宋体';">交叉</span><span style="font-family:Calibri;">winCE</span><span style="font-family:'宋体';">环境编译</span><span style="font-family:Calibri;">enable
 MinGW native/cross WinCE compile</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--enable-a52</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>启用<span style="font-family:Calibri;">A52</span><span style="font-family:'宋体';">支持（默认关闭）</span><span style="font-family:Calibri;">enable GPLed A52 support [default=no]</span></p>
</td>
</tr><tr><td valign="top">
<p>--enable-a52bin</p>
</td>
<td valign="top">
<p>启用运行时打开<span style="font-family:Calibri;">liba52.so.0</span><span style="font-family:'宋体';">（默认关闭）</span><span style="font-family:Calibri;">open liba52.so.0 at runtime [default=no]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--enable-dts</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>启用<span style="font-family:Calibri;">DTS</span><span style="font-family:'宋体';">支持（默认关闭）</span><span style="font-family:Calibri;">enable GPLed DTS support [default=no]</span></p>
</td>
</tr><tr><td valign="top">
<p>--enable-pp</p>
</td>
<td valign="top">
<p>启用后加工支持（默认关闭）<span style="font-family:Calibri;">enable GPLed postprocessing support [default=no]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--enable-static</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>构建静态库（默认启用）<span style="font-family:Calibri;">build static libraries [default=yes]</span></p>
</td>
</tr><tr><td valign="top">
<p><span style="color:rgb(255,0,0);">--disable-static</span></p>
</td>
<td valign="top">
<p><span style="color:rgb(255,0,0);">禁止构建静态库（默认关闭）<span style="font-family:Calibri;">do not build static libraries [default=no]</span></span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p><span style="color:rgb(255,0,0);">--enable-shared</span></p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p><span style="color:rgb(255,0,0);">构建共享库（默认关闭）<span style="font-family:Calibri;">build shared libraries [default=no]</span></span></p>
</td>
</tr><tr><td valign="top">
<p>--disable-shared</p>
</td>
<td valign="top">
<p>禁止构建共享库（默认启用）<span style="font-family:Calibri;">do not build shared libraries [default=yes]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--enable-amr_nb</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>启用<span style="font-family:Calibri;">amr_nb float</span><span style="font-family:'宋体';">音频编解码器</span><span style="font-family:Calibri;">|enable amr_nb float audio codec</span></p>
</td>
</tr><tr><td valign="top">
<p>--enable-amr_nb-fixed</p>
</td>
<td valign="top">
<p>启用<span style="font-family:Calibri;">fixed amr_nb codec | use fixed point for amr-nb codec</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--enable-amr_wb</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>启用<span style="font-family:Calibri;">amr_wb float</span><span style="font-family:'宋体';">音频编解码器</span><span style="font-family:Calibri;">|enable amr_wb float audio codec</span></p>
</td>
</tr><tr><td valign="top">
<p>--enable-amr_if2</p>
</td>
<td valign="top">
<p>启用<span style="font-family:Calibri;">amr_wb IF2</span><span style="font-family:'宋体';">音频编解码器</span><span style="font-family:Calibri;">|enable amr_wb IF2 audio codec</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--enable-sunmlib</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>启用<span style="font-family:Calibri;">Sun medialib</span><span style="font-family:'宋体';">（默认关闭）</span><span style="font-family:Calibri;">| use Sun medialib [default=no]</span></p>
</td>
</tr><tr><td valign="top">
<p><span style="color:rgb(255,0,0);">--enable-pthreads</span></p>
</td>
<td valign="top">
<p><span style="color:rgb(255,0,0);">启用<span style="font-family:Calibri;">pthreads</span><span style="font-family:'宋体';">（多线程）（默认关闭）</span><span style="font-family:Calibri;">use pthreads [default=no]</span></span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--enable-dc1394</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>启用<span style="font-family:Calibri;">libdc1394</span><span style="font-family:'宋体';">、</span><span style="font-family:Calibri;">libraw1394</span><span style="font-family:'宋体';">抓取</span><span style="font-family:Calibri;">IIDC-1394</span><span style="font-family:'宋体';">（默认关闭）</span><span style="font-family:Calibri;">enable
 IIDC-1394 grabbing using libdc1394 and libraw1394 [default=no]</span></p>
</td>
</tr><tr><td valign="top">
<p>--enable-swscaler</p>
</td>
<td valign="top">
<p>启用计数器支持？（默认关闭）<span style="font-family:Calibri;">software scaler support [default=no]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--enable-avisynth</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>允许读取<span style="font-family:Calibri;">AVISynth</span><span style="font-family:'宋体';">脚本本件（默认关闭）</span><span style="font-family:Calibri;">allow reading AVISynth script files [default=no]</span></p>
</td>
</tr><tr><td valign="top">
<p>--enable-gpl</p>
</td>
<td valign="top">
<p>允许使用<span style="font-family:Calibri;">GPL</span><span style="font-family:'宋体';">（默认关闭）</span><span style="font-family:Calibri;">allow use of GPL code, the resulting libav* and ffmpeg will be under GPL [default=no]</span></p>
</td>
</tr></tbody></table>

高级参数选项：  
<table><tbody><tr><td valign="top">
<p><strong><span style="color:rgb(49,132,155);">命令</span></strong></p>
</td>
<td valign="top">
<p><strong><span style="color:rgb(49,132,155);">帮助信息</span></strong></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--source-path=PATH</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>源码的路径（当前为<span style="font-family:Calibri;">/root/flv/ffmpeg</span><span style="font-family:'宋体';">）</span><span style="font-family:Calibri;">| path to source code [/root/flv/ffmpeg]</span></p>
</td>
</tr><tr><td valign="top">
<p>--cross-prefix=PREFIX</p>
</td>
<td valign="top">
<p>为编译工具指定路径 <span style="font-family:Calibri;">| use PREFIX for compilation tools []</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--cross-compile</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>假定使用了交叉编译 <span style="font-family:Calibri;">| assume a cross-compiler is used</span></p>
</td>
</tr><tr><td valign="top">
<p>--cc=CC</p>
</td>
<td valign="top">
<p>指定使用何种<span style="font-family:Calibri;">C</span><span style="font-family:'宋体';">编译器（默认</span><span style="font-family:Calibri;">gcc</span><span style="font-family:'宋体';">）</span><span style="font-family:Calibri;">use C compiler CC [gcc]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--make=MAKE</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>使用特定的<span style="font-family:Calibri;">make | use specified make [make]</span></p>
</td>
</tr><tr><td valign="top">
<p>--extra-cflags=ECFLAGS</p>
</td>
<td valign="top">
<p>添加<span style="font-family:Calibri;">ECFLAGS</span><span style="font-family:'宋体';">到</span><span style="font-family:Calibri;">CFLAGS | add ECFLAGS to CFLAGS []</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--extra-ldflags=ELDFLAGS</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>添加<span style="font-family:Calibri;">ELDFLAGS</span><span style="font-family:'宋体';">到</span><span style="font-family:Calibri;">LDFLAGS</span><span style="font-family:'宋体';">（默认</span><span style="font-family:Calibri;">-Wl</span><span style="font-family:'宋体';">，</span><span style="font-family:Calibri;">--as-needed</span><span style="font-family:'宋体';">）</span><span style="font-family:Calibri;">|
 add ELDFLAGS to LDFLAGS [ -Wl,--as-needed]</span></p>
</td>
</tr><tr><td valign="top">
<p>--extra-libs=ELIBS</p>
</td>
<td valign="top">
<p>添加<span style="font-family:Calibri;">ELIBS | add ELIBS []</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--build-suffix=SUFFIX</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>为专用程序添加后缀 <span style="font-family:Calibri;">| suffix for application specific build []</span></p>
</td>
</tr><tr><td valign="top">
<p>--arch=ARCH</p>
</td>
<td valign="top">
<p>选择机器架构（默认<span style="font-family:Calibri;">x86</span><span style="font-family:'宋体';">）</span><span style="font-family:Calibri;">select architecture [x86]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--cpu=CPU</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>选用最低的<span style="font-family:Calibri;">cpu</span><span style="font-family:'宋体';">（影响指令的选择，可以在老</span><span style="font-family:Calibri;">CPU</span><span style="font-family:'宋体';">上出错）</span><span style="font-family:Calibri;">| selects the minimum cpu required (affects
 instruction selection, may crash on older CPUs)</span></p>
</td>
</tr><tr><td valign="top">
<p>--powerpc-perf-enable</p>
</td>
<td valign="top">
<p>启用<span style="font-family:Calibri;">PPC</span><span style="font-family:'宋体';">上面的性能报告（需要启用</span><span style="font-family:Calibri;">PMC</span><span style="font-family:'宋体';">）</span><span style="font-family:Calibri;">enable performance report on PPC</span>(requires
 enabling PMC)</p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--disable-mmx</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>禁用<span style="font-family:Calibri;">MMX | disable MMX usage</span></p>
</td>
</tr><tr><td valign="top">
<p>--disable-armv5te</p>
</td>
<td valign="top">
<p>禁用<span style="font-family:Calibri;">armv5te | disable armv5te usage</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--disable-iwmmxt</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>禁用<span style="font-family:Calibri;">iwmmxt | disable iwmmxt usage</span></p>
</td>
</tr><tr><td valign="top">
<p>--disable-altivec</p>
</td>
<td valign="top">
<p>禁用<span style="font-family:Calibri;">AltiVec | disable AltiVec usage</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--disable-audio-oss</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>禁用<span style="font-family:Calibri;">OSS</span><span style="font-family:'宋体';">音频支持（默认启用）</span><span style="font-family:Calibri;">disable OSS audio support [default=no]</span></p>
</td>
</tr><tr><td valign="top">
<p>--disable-audio-beos</p>
</td>
<td valign="top">
<p>禁用<span style="font-family:Calibri;">BeOS</span><span style="font-family:'宋体';">音频支持（默认启用）</span><span style="font-family:Calibri;">disable BeOS audio support [default=no]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--disable-v4l</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>禁用<span style="font-family:Calibri;">video4linux</span><span style="font-family:'宋体';">提取（默认启用）</span><span style="font-family:Calibri;">disable video4linux grabbing [default=no]</span></p>
</td>
</tr><tr><td valign="top">
<p>--disable-v4l2</p>
</td>
<td valign="top">
<p>禁用<span style="font-family:Calibri;">video4linux2</span><span style="font-family:'宋体';">提取（默认启用）</span><span style="font-family:Calibri;">disable video4linux2 grabbing [default=no]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--disable-bktr</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>禁用<span style="font-family:Calibri;">bktr</span><span style="font-family:'宋体';">视频提取（默认启用）</span><span style="font-family:Calibri;">disable bktr video grabbing [default=no]</span></p>
</td>
</tr><tr><td valign="top">
<p>--disable-dv1394</p>
</td>
<td valign="top">
<p>禁用<span style="font-family:Calibri;">DV1394</span><span style="font-family:'宋体';">提取（默认启用）</span><span style="font-family:Calibri;">disable DV1394 grabbing [default=no]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--disable-network</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>禁用网络支持（默认支持）<span style="font-family:Calibri;">disable network support [default=no]</span></p>
</td>
</tr><tr><td valign="top">
<p>--disable-ipv6</p>
</td>
<td valign="top">
<p>禁用<span style="font-family:Calibri;">ipv6</span><span style="font-family:'宋体';">支持（默认支持）</span><span style="font-family:Calibri;">disable ipv6 support [default=no]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--disable-zlib</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>禁用<span style="font-family:Calibri;">zlib</span><span style="font-family:'宋体';">（默认支持）</span><span style="font-family:Calibri;">disable zlib [default=no]</span></p>
</td>
</tr><tr><td valign="top">
<p>--disable-simple_idct</p>
</td>
<td valign="top">
<p>禁用<span style="font-family:Calibri;">simple IDCT</span><span style="font-family:'宋体';">例程（默认启用）</span><span style="font-family:Calibri;">disable simple IDCT routines [default=no]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--disable-vhook</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>禁用<span style="font-family:Calibri;">video hooking</span><span style="font-family:'宋体';">支持</span><span style="font-family:Calibri;">| disable video hooking support</span></p>
</td>
</tr><tr><td valign="top">
<p>--enable-gprof</p>
</td>
<td valign="top">
<p>启用profiling with gprof [no]</p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--disable-debug</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>禁用调试符号 <span style="font-family:Calibri;">| disable debugging symbols</span></p>
</td>
</tr><tr><td valign="top">
<p>--disable-opts</p>
</td>
<td valign="top">
<p>禁用编译器最优化 <span style="font-family:Calibri;">| disable compiler optimizations</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--disable-mpegaudio-hp</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>启用更快的解码<span style="font-family:Calibri;">MPEG</span><span style="font-family:'宋体';">音频（但精确度较低）（默认禁用）</span><span style="font-family:Calibri;">faster (but less accurate) MPEG audio decoding [default=no]</span></p>
</td>
</tr><tr><td valign="top">
<p>--disable-protocols</p>
</td>
<td valign="top">
<p>禁用 <span style="font-family:Calibri;">I/O </span><span style="font-family:'宋体';">协议支持（默认启用）</span><span style="font-family:Calibri;">disable I/O protocols support [default=no]</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--disable-ffserver</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>禁用生成<span style="font-family:Calibri;">ffserver | disable ffserver build</span></p>
</td>
</tr><tr><td valign="top">
<p>--disable-ffplay</p>
</td>
<td valign="top">
<p>禁用生成<span style="font-family:Calibri;">ffplay | disable ffplay build</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--enable-small</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>启用优化文件尺寸大小（牺牲速度）<span style="font-family:Calibri;">optimize for size instead of speed</span></p>
</td>
</tr><tr><td valign="top">
<p><span style="color:rgb(255,0,0);">--enable-memalign-hack</span></p>
</td>
<td valign="top">
<p><span style="color:rgb(255,0,0);">启用模拟内存排列，由内存调试器干涉？ <span style="font-family:Calibri;">
| emulate memalign, interferes with memory debuggers</span></span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--disable-strip</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>禁用剥离可执行程序和共享库 <span style="font-family:Calibri;">| disable stripping of executables and shared libraries</span></p>
</td>
</tr><tr><td valign="top">
<p>--disable-encoder=NAME</p>
</td>
<td valign="top">
<p>禁用<span style="font-family:Calibri;">XX</span><span style="font-family:'宋体';">编码器 </span>
<span style="font-family:Calibri;">| disables encoder NAME</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--enable-encoder=NAME</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>启用<span style="font-family:Calibri;">XX</span><span style="font-family:'宋体';">编码器 </span>
<span style="font-family:Calibri;">| enables encoder NAME</span></p>
</td>
</tr><tr><td valign="top">
<p>--disable-decoder=NAME</p>
</td>
<td valign="top">
<p>禁用<span style="font-family:Calibri;">XX</span><span style="font-family:'宋体';">解码器 </span>
<span style="font-family:Calibri;">| disables decoder NAME</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--enable-decoder=NAME</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>启用<span style="font-family:Calibri;">XX</span><span style="font-family:'宋体';">解码器 </span>
<span style="font-family:Calibri;">| enables decoder NAME</span></p>
</td>
</tr><tr><td valign="top">
<p>--disable-encoders</p>
</td>
<td valign="top">
<p>禁用所有编码器 <span style="font-family:Calibri;">| disables all encoders</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--disable-decoders</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>禁用所有解码器 <span style="font-family:Calibri;">| disables all decoders</span></p>
</td>
</tr><tr><td valign="top">
<p>--disable-muxer=NAME</p>
</td>
<td valign="top">
<p>禁用<span style="font-family:Calibri;">XX</span><span style="font-family:'宋体';">混音器 </span>
<span style="font-family:Calibri;">| disables muxer NAME</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--enable-muxer=NAME</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>启用<span style="font-family:Calibri;">XX</span><span style="font-family:'宋体';">混音器 </span>
<span style="font-family:Calibri;">| enables muxer NAME</span></p>
</td>
</tr><tr><td valign="top">
<p>--disable-muxers</p>
</td>
<td valign="top">
<p>禁用所有混音器 <span style="font-family:Calibri;">| disables all muxers</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--disable-demuxer=NAME</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>禁用<span style="font-family:Calibri;">XX</span><span style="font-family:'宋体';">解轨器 </span>
<span style="font-family:Calibri;">| disables demuxer NAME</span></p>
</td>
</tr><tr><td valign="top">
<p>--enable-demuxer=NAME</p>
</td>
<td valign="top">
<p>启用<span style="font-family:Calibri;">XX</span><span style="font-family:'宋体';">解轨器 </span>
<span style="font-family:Calibri;">| enables demuxer NAME</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--disable-demuxers</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>禁用所有解轨器 <span style="font-family:Calibri;">| disables all demuxers</span></p>
</td>
</tr><tr><td valign="top">
<p>--enable-parser=NAME</p>
</td>
<td valign="top">
<p>启用<span style="font-family:Calibri;">XX</span><span style="font-family:'宋体';">剖析器 </span>
<span style="font-family:Calibri;">| enables parser NAME</span></p>
</td>
</tr><tr><td valign="top" style="background:rgb(210,234,240);">
<p>--disable-parser=NAME</p>
</td>
<td valign="top" style="background:rgb(210,234,240);">
<p>禁用<span style="font-family:Calibri;">XX</span><span style="font-family:'宋体';">剖析器 </span>
<span style="font-family:Calibri;">| disables parser NAME</span></p>
</td>
</tr><tr><td valign="top">
<p>--disable-parsers</p>
</td>
<td valign="top">
<p>禁用所有剖析器 <span style="font-family:Calibri;">| disables all parsers</span></p>
</td>
</tr></tbody></table>


### Windows安装配置ffmpeg
参考链接：  
https://blog.csdn.net/chy466071353/article/details/54949221  

首先下载ffmpeg的windows版本https://ffmpeg.zeranoe.com/builds/

解压下载的压缩包，进入bin目录并获取路径。在此电脑界面下右击选择属性，选择高级系统设置。选择环境变量，在用户环境变量双击path，选择新建（注意不要更改其他环境变量），将刚才的bin路径粘贴进去，记得点下方的确定，再关闭当前窗口再点确定以保存。到这里，ffmpeg的配置就差不多了，调用命令行（windows+R输入cmd）输入“ffmpeg –version”，如果出现如下说明配置成功。


## <a id="6.2">FFmpeg命令行</a>
- [1.文件封装格式转换](#6.2.1)  
- [2.拉流存储](#6.2.2)
- [3.推流](#6.2.3)  
- [4.FFmpeg DASH/hls切片](#6.2.4)  
- [5.设置超时](#6.2.5)  
- [6.拉rtmp转rtsp](#6.2.6)  
- [7.修改比特率码率（-b）、帧率（-r）和文件大小（-fs）相关操作](#6.2.7)  
- [8.filter滤镜的使用](#6.2.8)  


参考链接：  
1.ffmpeg 常用命令总结  
https://blog.csdn.net/weixin_42081389/article/details/100543007  

2.ffmpeg文档  
https://ffmpeg.org/ffmpeg-formats.html  

3.ffmpeg常用命令总结  
https://blog.csdn.net/langzijing/article/details/85256846  

```cpp
FFmpeg命令行说明  
命令行基本格式如下：  
ffmpeg [global_options] {[input_file_options] -i input_url} ... {[output_file_options] output_url} ...  

常用参数说明：
主要参数：
-i 设定输入流 
-f 设定输出格式 
-ss 开始时间 
-re 按原始帧率推送
-f 输出流格式类型(flv,mp4等等)

视频参数：
-b 设定视频流量(码率)，默认为200Kbit/s 
-r 设定帧速率，默认为25 
-s 设定画面的宽与高 
-aspect 设定画面的比例 
-vn 不处理视频 
-vcodec 设定视频编解码器，未设定时则使用与输入流相同的编解码器

音频参数：
ar 设定采样率
ac 设定声音的Channel数
acodec 设定声音编解码器，未设定时则使用与输入流相同的编解码器an 不处理音频

```
FFmpeg DASH/hls切片
### <a id="6.2.1">1.文件封装格式转换</a>
参考链接：   
1.ffmpeg常见命令   
https://www.cnblogs.com/HongyunL/p/8120146.html   


#### 1.将mp4文件转换为flv文件
```js
ffmpeg  -i test.mp4  -c  copy  -f  flv  test.flv
-i 读取任意数量的输入文件(这些文件可以是正常文件、管道、网络流，抓取设备等)，写入任意数量的被声明为一个简单的输出文件名的文 件。在命令行的任何不能被解释的选项都被作为输出文件
-i test.mp4 输入文件是当前目录下的test.mp4文件
-c 编码方式，一般视频文件以h264编码、音频文件以aac编码居多
-c copy 拷贝输入文件的音视频编码方式到输出文件
-f flv 指定输出文件的封装方式为flv
```

#### 2.将文件转换为mp4文件
```js
ffmpeg  -i test.flv  -c  copy  -f  mp4  test.mp4

ffmpeg -i ingput.rmvb -vcodec mpeg4 -b:v 200k -r 15 -s 640x480 -an -f mp4 -movflags faststart -y output.mp4
-vcodec : 强制指定视频编码格式，-acodec 强制指定音频编码格式 （ffmpeg -codecs可以查看支持的codecs， 如果指定copy则就不转码，直接copy原来的stream到新文件）
-b:v 指定视频的码率，b:a 指定音频码率 -b 指定视频和音频加起来的码率。
-r : frame rate 帧率
-an ： disable audio 这样output.mp4里面没有音频。
-f force format，指定文件格式（ffmpeg -formats可以列举支持的format）
-movflags faststart : 如果不指定faststart, 则ffmpeg默认会将moov box放在mdat box的后面，如果加上这个flag，moov box会放在前面。
-s ：分辨率，还可以利用-vf scale=iw*0.9:ih*0.9来实现缩放，vf就是指定video filter，这里利用scale filter来实现。注意这两种方法设置分辨率都是scale，不是crop，所以如果不是等比例scale，会造成图像变形。
-y : 覆盖输出文件，就是如果output.mp4文件已经存在，则不提示，直接覆盖，-n是不覆盖

ffmpeg -i input.flv -c copy -f mp4 -movflags dash output.mp4
关于movflags的选项，可以通过ffmpeg -h muxer=mp4查看.生成dash mp4文件
```
#### 3.将avi文件转换为mp4文件
```js
ffmpeg  -i test.avi  -c  copy  -f  mp4  test.mp4
```
#### 4.从mp4文件中提取h264
```js
ffmpeg -i test.mp4 -vcodec copy -an output.h264
-vcodec copy 拷贝原始视频的编码方式

ffmpeg -i test.mp4 -codec copy -bsf: h264_mp4toannexb -f h264 tmp.264  
-codec copy： 从mp4中拷贝  
-bsf: h264_mp4toannexb： 从mp4拷贝到annexB封装  
-an 取消音频
```
#### 5.将h264文件封装成mp4文件
```js
ffmpeg -i output.h264 -vcodec copy -f mp4 test.mp4
ffmpeg -i input_file -vcodec copy -an output_file_video　　//分离视频流
ffmpeg -i input_file -acodec copy -vn output_file_audio　　//分离音频流
```
#### 6.YUV序列转AVI
ffmpeg –s w*h –pix_fmt yuv420p –i input.yuv –vcodec mpeg4 output.avi  

#### 7.视频解复用
```js
ffmpeg –i test.mp4 –vcodec copy –an –f m4v test.264 

//解复用demuxer
ffmpeg -i input.mp4 -c:v copy –an output.264 
ffmpeg -i input.avi -c:v copy –an output.264
//封装muxer
ffmpeg -i input.264 -i <audio_file> -c:v copy c:a copy output.mp4
```


#### 8.视频转码
1.mpeg视频格式转换、视频抓图、给视频加水印
https://blog.csdn.net/king1425/article/details/70348374

```js
ffmpeg –i test.mp4 –vcodec h264 –s 352*278 –an –f m4v test.264 //转码为码流原始文件
ffmpeg –i test.mp4 –vcodec h264 –bf 0 –g 25 –s 352*278 –an –f m4v test.264 //转码为码流原始文件
ffmpeg –i test.avi -vcodec mpeg4 –vtag xvid –qsame test_xvid.avi //转码为封装文件
-bf B帧数目控制
-g 关键帧间隔控制
-s 分辨率控制
ffmpeg –i video_file –i audio_file –vcodec copy –acodec copy output_file

ffmpeg -i source_video.avi input -acodec aac -ab 128kb -vcodec mpeg4 -b 1200kb -mbd 2 -flags +4mv+trell -aic 2 -cmp 2 -subcmp 2 -s 320x180 -title X final_video.mp4
* 源视频：source_video.avi
* 音频编码：aac
* 音频位率：128kb/s
* 视频编码：mpeg4
* 视频位率：1200kb/s
* 视频尺寸：320 X 180
* 生成的视频：final_video.mp4

ffmpeg -i input.mp4 -c:v libx264 -g 50 -sc_threshold 0 -t 60 output.mp4
-g : gop 长度
-src_threshold : 这里等于0是为了场景切换时不自动插入I帧。如果只是设置-g，则碰到场景切换时，会自动插入I帧，所以不是严格的50帧为一个gop。P126

ffmpeg -i video_1920.mp4 -vf scale=640:360 video_640.mp4
-vf是调用video filter，这里用了scale filter来改变分辨率进行转码。
ffmpeg -i input.mp3 -acodec libmp3lame -b:a 64k output.mp3 使用libmp3lame库来转码mp3
ffmpeg -i input.wav -c:a aac -q:a 2 output.aac -q:a参数可以分不同等级，对应不同码率P142

ffmpeg -i xx.mp4 -vcodec copy -acodec copy -f mp4 -movflags frag_keyframe+empty_moov output.mp4

//yuv
ffmpeg -i input.mp4 -ss 00:00:00 -to 00:00:10  -pix_fmt yuv420p output.yuv
ffmpeg -s w*h -pix_fmt yuv420p -r 30 -i input.yuv -c:v libx264 output.mp4
ffmpeg -s 1280x720 -pix_fmt yuv420p -r 30 -i input.yuv -c:v libx264 output.mp4

//libx264
ffmpeg -i input.mp4 -c:v libx264 -x264-param "level=30:bframes=0:cabac=0:ref=1:vbv-maxrate=768:vbv-bufsize=2000" -g 30 -b:v 1000k output.mp4 
ffmpeg -i input.mp4 -c:v libx264 -g 30 -crf 23 output.mp4
//libx265
ffmpeg -i input.mp4 -c:v libx265 -x265-param <param> -g 30 -b:v 1000k output.mp4
//硬解硬编（解码hevc_cuvid、h264_cuvid，编码hevc_nvenc、h264_nvenc）
ffmpeg -hwaccel cuvid -c:v hevc_cuvid -i input.mp4 -c:v hevc_nvenc -b:v 1000k -r 30 output.mp4
//硬解硬编、加filter（scale_cuda、resize、自定义cuda filter）
ffmpeg -hwaccel cuvid -c:v hevc_cuvid -i input.mp4 -vf scale_cuda=1280:720 -c:v hevc_nvenc output.mp4
//camera、v4l2、rtmp等
ffmpeg -re -i input.mp4 -f flv rtmp://myserver/live/mystream //-re 实时

```

#### 9.视频剪切
```js
ffmpeg –i test.avi –r 1 –f image2 image-%3d.jpeg //提取图片
ffmpeg -ss 0:1:30 -t 0:0:20 -i input.avi -vcodec copy -acodec copy output.avi //剪切视频
-r 提取图像的频率
-ss 开始时间
-t 持续时间

//时间上截取 （-ss <start> -to <end>）
ffmpeg -i input.mp4 -r 1 -f image2 image-%3d.jpeg  //提取图片,-r 提取图像的频率
ffmpeg -ss 00:01:30 -t 00:00:20 -i input.mp4 -c:v copy -c:a copy output.mp4    //剪切视频，-ss 开始时间，-t 持续时间
ffmpeg -ss 00:1:30 -to 00:01:50 -i input.mp4 -c:v copy -c:a copy output.mp4    //剪切视频，-ss 开始时间，-to 截止时间
//备注：如果开始时间和结束时间不在I帧上，直接copy可能会出现有一段时间没视频数据，画面变成黑色
ffmpeg -ss 00:1:30 -to 00:01:50 -i input.mp4 -c:v libx265 -c:a copy output.mp4
//空间上裁剪 -vf crop=out_w:out_h:start_point_w:start_point_h      
ffmpeg -i input.mp4 -vf crop=640:480:0:0 -c:v libx265  -crf 23 -c:a copy output640x480.mp4
//cropdetect:自动检测裁剪尺寸(自动去除边缘的黑部)
```

#### 10.YUV序列播放
ffplay -f rawvideo -video_size 1920x1080 input.yuv  

#### 11.分离视频音频流
ffmpeg -i input.mp4 -c:v copy -an output_video.mp4　　//-an 屏蔽audio 只提取出视频流   
ffmpeg -i input.mp4 -c:v copy -vn output_audio.mp4　　//-vn 屏蔽video 只提取出音频流 

#### 12.旋转、翻转
```js
//水平翻转输入视频(hflip、vflip)
ffmpeg -i in.avi -vf "hflip" out.avi
ffmpeg -i in.avi -vf "vflip" out.avi

//transpos
ffmpeg -i in.mov -vf "transpose=1" out.mov //Rotate 90 clockwise
ffmpeg -i in.mov -vf "transpose=2,transpose=2" out.mov //Rotate 180 degrees
0 = 90CounterCLockwise and Vertical Flip (default)
1 = 90Clockwise
2 = 90CounterClockwise
3 = 90Clockwise and Vertical Flip

//任意旋转视频角度（以弧度值表示）
//With 90 degrees equating with PI/2, you can do it like so:
ffmpeg -i in.mp4 -vf "rotate=PI/2" out.mp4
//for counter-clockwise the angle must be negative
ffmpeg -i in.mp4 -vf "rotate=-PI/2" out.mp4
//顺时针旋转PI/6:rotate=PI/6
```

#### 13.视频拼接
```js
//时间上拼接,视频文件切片合并 concat    
ffmpeg -i "concat:input1.mpg|input2.mpg|input3.mpg" -c copy output.mpg 
//或者 file_list.txt
cat file_list.txt
    # this is a comment
    file '/path/to/file1'
    file '/path/to/file2'
    file '/path/to/file3'
ffmpeg -f concat -i file_list.txt -c copy output.mp4

//空间上拼接 pad, overlay(叠加)
//pad=dst_w:dst_h:start_point_w:start_point_h
//overlay=start_point_w:start_point_h (默认为0)
//2 video to 2x1 video                                       
ffmpeg -i input_640x480_1.mp4 -i input_640x480_2.mp4 -filter_complex "[0:0]pad=iw*2:ih:0:0[a];[a][1:0]overlay=w" -libx265 output_1280x480.mp4 //iw,ih 输入视频的宽高，[0:0]->第一视频的第一个流，[1:0]->第二个视频的第一路流
ffmpeg -i input_640x480_1.mp4 -i input_640x480_2.mp4 -filter_complex "[0:0]pad=1280:480:0:0[a];[a][1:0]overlay=640" -libx265 output_1280x480.mp4
//4 video to 2x2 video
ffmpeg -i 1_640x480.mp4 -i 2_640x480.mp4 -i 3_640x480.mp4 -i 4_640x480.mp4 -filter_complex "[0:0]pad=iw*2:ih*2:0:0[a];[a][1:0]overlay=640:0[over1];[over1][2:0]overlay=0:480[over2];[over2][3:0]overlay=640:480" -c:v libx265 output_2x2_1280x960.mp4
```

#### 14.截屏
```js
可以通过ffmpeg直接抓取X11显示内容：
ffmpeg -f x11grab -video_size cif -framerate 25 -i :0.0+10,20 /tmp/out.mpg
//0.0 是X11服务的显示屏幕号(display.screen)， 定义于 DISPLAY 环境变量。 10是水平偏移， 20是垂直偏移
ffmpeg -f gdigrab -show_region 1 -framerate 6 -video_size cif -offset_x 10 -offset_y 20 -i desktop out.mpg

制作gif
ffmpeg -i capx.mp4 -t 10 -s 320x240 -pix_fmt rgb24 jidu1.gif
```

#### 15.two pass
ffmpeg -y -i input.mp4 -c:v libx264 -b:v 2600k -pass 1 -an -f mp4 /dev/null && \   
ffmpeg -i input.mp4 -c:v libx264 -b:v 2600k -pass 2 -c:a aac -b:a 128k output.mp4  

#### 16.插帧、升帧
ffmpeg -i input-30fps.mp4 -vf fps=fps=60 output-60fps.mp4   

#### 17.轨道分离、合并 (map)
```js
//音视频轨道分离，假设input.mp4有两个视频轨，1个音频轨
ffmpeg -i input.mp4 -map 0:0 -c:v copy output_stream1.mp4 -map 0:1 -c:v copy output_stream2.mp4 -map 0:3 -c:a copy output_audio.mp4 //0:0表示第一个输入视频的第一个视频流

//多个视频合成为一个具有多个轨道的视频
ffmpeg -i input_video_1.mp4 -i input_video_2.mp4 -i input_audio.mp4 -map 0:0:0 -c:v copy -map 1:0:0 -c:v copy -map 2:0:0 -c:a copy output_3track.mp4
```


### <a id="6.2.2">2.拉流存储</a>
#### 1.按照时间间隔存储
```js
ffmpeg -rtsp_transport tcp -i rtsp://42.101.40.177:10236/stream1 -r 10 -f image2 /root/images%09d.png -c copy -map 0 -f segment -segment_time 60 -segment_format mp4 "/root/out%03d.mp4" -rtsp_transport tcp 用tcp方式读取rtsp视频流
-r 10  10帧/秒
-f image2 输出文件的格式image2
-map 0 所有输入流
-f segment 分段持续时间
-segment_format mp4 输出文件的格式为mp4
```

#### 2.标准拉流存储
```js
ffmpeg -rtmp_app live -i rtmp://192.168.1.23:1935/live1 -c copy -f mp4 out.mp4
-rtmp_app live 指定输入的rtmp地址为直播流的形式
```

### <a id="6.2.3">3.推流</a>
####  参考链接
1.FFmpeg 推流总结  
https://www.jianshu.com/p/37ef34258608  

2.使用FFmpeg命令行进行UDP、RTP推流  
https://blog.csdn.net/zhoubotong2012/article/details/86711097  

#### 1.推送rtsp流到rtsp服务器
ffmpeg -i rtsp://192.168.1.23:554/stream/ -c copy -f rtsp rtsp://192.168.1.24:554/stream/  

#### 2.推送视频文件到rtsp服务器
ffmpeg -re -i test.mp4 -vcodec copy -codec copy -f rtsp rtsp://192.168.1.24:554/stream/  

#### 3.推送rtmp流到rtmp服务器
```c++
ffmpeg -re -i rtmp://192.168.1.23:1935/live1 -vcodec copy -codec copy -f flv rtmp://192.168.1.24:1935/live1  
  
ffmpeg -re -stream_loop -1 -i wKgBBV7MtiaAbHhFAzl39Emwz1I432.mp4 -vf scale=1440:-2 -f flv -vcodec h264 -max_delay 1000 -g 2 rtmp://192.168.1.5/live//test    
-re : 原始帧率
-stream_loop: 循环次数 ,-1 无限循环
-vcodec h264: 修改编码
-max_delay 1000: 最大延迟
-g 2: gop_cache 单位
ffmpg控制码率有3种选择，-minrate -b:v -maxrate 。-b:v主要是控制平均码率。设置b:v时，同时加上 -bufsize 。-bufsize 用于设置码率控制缓冲器的大小，设置的好处是，让整体的码率更趋近于希望的值，减少波动。
```

#### 4.推送视频文件到rtmp服务器
ffmpeg -re -i test.mp4 -c copy -f flv rtmp://192.168.1.23:1935/live1  

#### 5.推送rtsp到rtmp服务器
```c++
ffmpeg -i rtsp://192.168.1.23:554/live1 -c copy -f flv rtmp://192.168.1.23:1935/live1  

ffmpeg -r 30  -f dshow -i video="1.3M HD WebCam" -vcodec h264 -max_delay 100 -f flv -g 5 -b 700000 rtmp://219.216.87.170/live/test1

为了实现首屏秒开使用-g设置gop大小，同时使用-b降低网络负载，保证流畅度
```

#### 6.推送rtmp流到rtsp服务器
ffmpeg -i rtmp://192.168.1.23:1935/live1-c copy -f rtsp rtsp://192.168.1.23:554/live1  

#### 7.ffmpeg+dss拉流分发
`环境 139.196.137.60 安装dss软件+ffmpeg软件`
* ffmpeg推流本地服务器mp4文件到dss软件  
在139.196.137.60服务器运行ffmpeg推流mp4到139.196.137.60上dss软件上，在/usr/local/movie下生成4.sdp文件
```shell
ffmpeg -re -i 20190329203822.mp4 -vcodec copy -codec copy -f rtsp rtsp://127.0.0.1:554/4.sdp 
``` 
播放rtsp://139.196.137.60:554/4.sdp进行播放    


* ffmpeg拉流rtsp流后推流到本地服务器到dss软件  
在139.196.137.60服务器运行ffmpeg拉流rtsp后推流到139.196.137.60上dss软件上，在/usr/local/movie下生成5.sdp文件 
```shell 
ffmpeg -rtsp_transport tcp -i rtsp://42.101.40.177:10236/stream1 -c copy -f rtsp rtsp://127.0.0.1:554/5.sdp  
```
播放rtsp://139.196.137.60:554/5.sdp进行播放   

>注意  
1.ffmpeg推流报错403可能是需要注意后rtsp地址不加引号。  
2.推流地址可能要写127.0.0.1

#### 8.FFmpeg命令UDP、RTP推流   
* 使用RTP发送H264    
ffmpeg -re -i d:\videos\1080P.264 -vcodec copy -f rtp rtp://127.0.0.1:1234       
我们可以让FFmpeg在执行推流输出时导出一个SDP，命令行如下：    
ffmpeg -re -i d:\videos\1080P.264 -vcodec copy -f rtp rtp://127.0.0.1:1234>test_rtp_h264.sdp   


* 使用UDP发送TS流     
FFmpeg的推流命令：      
ffmpeg -re -i d:\videos\1080P.264 -vcodec copy -f mpegts udp://127.0.0.1:1234     


* 使用RTP发送TS流   
很多人以为这种情况跟上面差不多，使用如下的推流命令（错误）：   
ffmpeg -re -i d:\videos\1080P.264 -vcodec copy -f mpegts rtp://127.0.0.1:1234    
但是实际上不行，我开始也使用该条命令，试过好多次老是失败，失败的原因是：发送的包缺少RTP头，输出的协议有错误。实际上，正确的命令是：  
ffmpeg -re -i d:\videos\1080P.264 -vcodec copy -f rtp_mpegts rtp://127.0.0.1:1234   


* 使用RTP分别发送音频流和视频流    
FFmpeg命令：   
ffmpeg  -re -i <media_file> -an -vcodec copy -f rtp rtp://<IP>:5004 -vn -acodec copy -f rtp rtp://<IP>:5005 > t    

* FFmpeg从视频文件中抽离视频并以udp+rtp+h264推流的命令   
ffmpeg -re -i /mnt/test/cuc_ieschool.flv -f h264 udp://192.168.110.243:12345  

* 发送H.264裸流至组播地址    
注：组播地址指的范围是224.0.0.0—239.255.255.255    
ffmpeg -re -i chunwan.h264 -vcodec copy -f h264 udp://233.233.233.223:6666    
注1：-re一定要加，代表按照帧率发送，否则ffmpeg会一股脑地按最高的效率发送数据。    
注2：-vcodec copy要加，否则ffmpeg会重新编码输入的H.264裸流。    

* 发送MPEG2裸流至组播地址    
ffmpeg -re -i chunwan.h264 -vcodec mpeg2video -f mpeg2video udp://233.233.233.223:6666   


#### 9.拉流   
```c++
1.利用系统的管道：(注意“-”的使用)
ffmpeg -i input.mp4 -acodec aac -vcodec libx264 -f flv - | ffmpeg -f flv -i - -c copy -f flv out1.flv -c copy -f mp4 out2.mp4 -c copy -f flv rtmp://publish.chinaffmpeg.com/live/stream1
2.利用tee协议：
ffmpeg -re -i input.mp4 -vcodec libx264 -acodec aac -map 0 -f tee "[f=flv]rtmp://publish.chinaffmpeg .com/live/stream1 | [f=flv]rtmp://publish.chinaffmpeg.com/live/stream2”

dash流的生成
ffmpeg -re -i input.mp4 -c:v copy -acodec copy -f dash -window_size 4 index.mpd
```

### <a id="6.2.4">4.FFmpeg DASH/hls切片</a>
#### 1.hls切片
```cpp
命令：ffmpeg -i /home/videos/xxx.ts -c copy -f segment -segment_list_size 3 -segment_list /usr/local/nginx/html/xxx.m3u8 -segment_time 10 -active_window_size 10 -y /usr/local/nginx/html/xxx%3d.ts

说明：
-i 输入文件 -c copy 不编码
-f 文件格式，表示切片segment
-segment_list_size 切片列表中含有几个片，默认为所有
-segment_time 10 每个片的时间，单位为秒
-y 过程中产生同样文件直接覆盖
/usr/local/nginx/html/xxx.m3u8 生成的m3u8文件
/usr/local/nginx/html/xxx%3d.ts 切片文件存放路径

生成实时的直播流：
ffmpeg -re -i input.mp4 -c copy -f hls -bsf:v h264_mp4toannexb -start_number 300 -hls_time 10 output.m3u8
-re :就是根据文件真实的帧率来读取文件，生成实时刷新的直播流，而不是一下子按照电脑的最大性能把文件一下子读进去。
-bsf:v h264_mp4toannexb ：就是把mp4格式的nalu转换为annexb格式的P97有解释，对265来说为hevc_mp4toannexb
-start_number :生成ts文件的名称编号的开始数字
-hls_time ： 文件切片长度。更多参数可以参考P96
-c 和-codec后面都跟codec name，就是指定codec

生成segment切片
ffmpeg -re -i input.mp4 -c copy -f segment -segment_format mp4 test_output-%d.mp4
segment切片比hls要灵活一些，因为hls只支持ts文件，segment可以切为mp4,ts，flv等各种文件格式，并且也可以生成像m3u8这种文件list。参数可以参考ffmpeg -h muxer=segment
ffmpeg -re -i input.mp4 -c copy -f segment -segment_format mp4 test_output-%d.mp4
```

#### 2.dash切片
```cpp
命令：ffmpeg -i udp://10.0.224.28:1234 -c copy -f dash -window_size 5 -min_seg_duration 3000000 -single_file 0 -init_seg_name initavatar-stream\$RepresentationID\$.m4s -media_seg_name chunkavatar-stream\$RepresentationID\$-\$Number%05d\$.m4s -use_template 0 -bsf:a aac_adtstoasc ftp://user:password@192.168.32.188/liang/liangc.mpd
说明：

-i 输入文件 -c copy 不编码
-f 文件格式，表示切片dash
--window_size 5 实时更新只保留5个切片
-min_seg_duration 3000000 每个片的时间，单位为us
-single_file 0 是否是单个文件切片
-init_seg_name initavatar-stream$RepresentationID$.m4s 初始化切片文件，只第一次生成
-media_seg_name chunkavatar-stream$RepresentationID$-$Number%05d$.m4s切片文件，实时更新
-use_template 0 -bsf:a aac_adtstoasc 根据需求是否加上
ftp://user:password@192.168.32.188/liang/liangc.mpd 生成切片列表文件

window_size 默认值：int:0 — 取值范围：[0, int_max]
--清单中保留的最大段数

extra_window_size 默认值：int:5 — 取值范围：[0, int_max]
--从磁盘中删除之前保留在清单外部的最大段数

min_seg_duration 默认值：int64:5000000 取值范围：[0, int_max]
--最小段持续时间（以微秒为单位）
--默认值:5s

remove_at_exit默认值：bool:0 – 取值范围：[0, 1]
--完成后删除所有段

use_template默认值：bool:1 – 取值范围：[0, 1]
--使用SegmentTemplate而不是SegmentList
--如single_file:1 会被置0

use_timeline 默认值：bool:1 – 取值范围：[0, 1]
--在SegmentTemplate中使用SegmentTimeline
--manifest中startNumber、timescale与之相关

single_file默认值：bool:0 – 取值范围：[0, 1]
--将所有段存储在一个文件中，使用字节范围访问
--以单一文件存储：以byte range表示

single_file_name
--用于baseURL的DASH模板名称。意味着将single_file设置为“1”。

init_seg_name
--用于初始化段的DASH模板名称。默认为“init-stream $ RepresentationID $ .m4s”

media_seg_name
--用于媒体段的DASH模板名称。默认为“chunk-stream $ RepresentationID $ - $ Number％05d $ .m4s”

streaming streaming
--块输出模式的输出。在块流模式中，每个帧将是形成块的moof片段。

adaptation_sets
--将流分配给AdaptationSets。 语法是“id = x，streams = a，b，c id = y，streams = d，e”，其中x和y是适应集的ID，a，b，c，d和e是指数的索引。 映射流。 为了将所有视频（或音频）流映射到AdaptationSet，“v”（或“a”）可以用作流标识符而不是ID。 如果未定义任何分配，则默认为每个流的AdaptationSet。

timeout
--设置套接字I / O操作的超时。仅适用于HTTP输出。
```

#### 3.搭建dash直播
参考链接：  
1.搭建dash直播  
https://www.jianshu.com/p/512cb67e35d1  

搭建dash直播和hls直播差不多, 都是用nginx和nginx-rtmp-module
```cpp
rtmp {
    server {
        listen 1935;
        chunk_size 4096;
        
        application dash {
            live on;
            dash on;
            dash_path /home/movie/rtmp/dash;
        }
    }
}

ffmpeg是通过libxml2去解析mpd文件的, 所以在configure之前需要先安装libxml2
sudo apt-get install libxml2 
sudo apt-get install libxml2-dev
安装之后, 在configure的时候, 加上--enable-libxml2,在configure完成之后,查看config.h文件, 检查
CONFIG_DASH_DEMUXER宏是否为1.
然后make & make install.
最后用编译完成的ffplay测试下上面搭建的dash直播

```

### <a id="6.2.5">5.设置超时</a>
#### 1.设置rtsp超时
```cpp
AVDictionary* opts = NULL;
av_dict_set(&opts, "rtsp_transport", m_bTcp ? "tcp" : "udp", 0); //设置tcp or udp，默认一般优先tcp再尝试udp
av_dict_set(&opts, "stimeout", "3000000", 0);//设置超时3秒
int ret = avformat_open_input(&ctx, url, NULL, &opts);
```

#### 2.设置udp,http超时
```cpp
AVDictionary* opts = NULL;
av_dict_set(&opts, "timeout", "3000000", 0);//设置超时3秒
int ret = avformat_open_input(&ctx, url, NULL, &opts);
```

#### 3.设置av_read_frame 超时
```cpp
auto ctx = avformat_alloc_context();
ctx->interrupt_callback.callback = CheckInterrupt;//超时回调
ctx->interrupt_callback.opaque = this;
//
m_tStart = time(NULL);
av_read_frame(ctx, &pkt);
//超时回调函数
static int CheckInterrupt(void* ctx)
{
　　auto p = (xxx*)ctx;
　　return time(NULL) -  p->m_tStart >= 3 ? 1 : 0;//3秒超时
}
```



### <a id="6.2.6">6.拉rtmp转rtsp</a>
#模拟无人机推流文件到rtmp  无人机环境下（验证可以）   
ffmpeg -re -i /usr/movie/test.mp4 -f flv rtmp://221.229.219.225:18081/uav5/MP4   

推流rtmp到dss服务器 无人机环境（404有问题）   
ffmpeg -f flv -i rtmp://221.229.219.225:18081/uav5/MP4 -c copy -f rtsp rtsp://139.196.137.60:554/5.sdp    

推流文件到mediaserver  无人机环境（验证可以）   
ffmpeg -re -i /usr/movie/test.mp4 -c copy -f rtsp  -rtsp_transport tcp rtsp://139.196.137.60:554/live/test    

拉流无人机下rtmp推流到阿里云mediaserver（验证可以）   
ffmpeg -i rtmp://221.229.219.225:18081/uav5/MP4 -vcodec mpeg4 -s 1280x720 -f rtsp -rtsp_transport tcp rtsp://139.196.137.60:554/live/test     

PS：
1.推流rtmp出差由于没有设置宽高。
打开编码器输出流＃0.0时出错 - 可能是不正确的参数，如bit_rate，速率，宽度或高度(Error while opening encoder for output stream #0.0 - maybe incorrect parameters such as bit_rate, rate, width or height)。 

ffmpeg -i '/home/public_html/files/video_1355440448.m4v' -s '640x360' -sameq -ab '64k' -ar '44100' -f 'flv' -y /home/public_html/files/video_1355440448.flv     

ffmpeg -i C:\files\input\test.avi -y -ab 448k -ar 48000 -vcodec mpeg4 -s 640x480 -f flv C:\files\output\test.flv     


### <a id="6.2.7">7.修改比特率码率、帧率、分辨率和文件大小相关操作</a>
帧率：帧率（Frame rate）也叫帧频率，帧率是视频文件中每一秒的帧数，肉眼想看到连续移动图像至少需要15帧。

码率：比特率（Bit rate）(也叫码率，数据率)是一个确定整体视频/音频质量的参数，秒为单位处理的字节数，码率和视频质量成正比，在视频文件中中比特率用bps来表达。

#### 1.帧率
```cpp
1、用 -r 参数设置帧率
	ffmpeg –i input –r 25 output
2、用fps的filter设置帧率
	ffmpeg -i 1.mp4 -vf fps=fps=25 11.mp4

例如设置帧率为29.97fps，下面三种方式具有相同的结果：
	ffmpeg -i input.avi -r 29.97 output.mp4
	ffmpeg -i input.avi -r 30000/1001 output.mp4
	ffmpeg -i input.avi -r netsc output.mp4
```


#### 2.码率
```cpp
码率控制

码率控制对于在线视频比较重要。因为在线视频需要考虑其能提供的带宽。

那么，什么是码率？很简单： 
bitrate = file size / duration 
比如一个文件20.8M，时长1分钟，那么，码率就是： 
biterate = 20.8M bit/60s = 20.8*1024*1024*8 bit/60s= 2831Kbps 
一般音频的码率只有固定几种，比如是128Kbps， 
那么，video的就是 
video biterate = 2831Kbps -128Kbps = 2703Kbps。

设置参数-b
	ffmpeg -i 1.mp4 -b 1.5M 2.mp4
	
音频：-b:a     视频： - b:v
	设置视频码率为1500kbps
	ffmpeg -i input.avi -b:v 1500k output.mp4

```

#### 3.控制输出文件大小
```cpp
-fs (file size首字母缩写) 
	ffmpeg -i input.avi -fs 1024K output.mp4

计算输出文件大小
	(视频码率+音频码率) * 时长 /8 = 文件大小K
```

#### 4.分辨率
```cpp
ffmpeg -i video_1920.mp4 -vf scale=640:360 video_640.mp4 -hide_banner

ffmpeg -i video_1920.mp4 -vf scale=640:480,setdar=4:3 video_640x480.mp4 -hide_banner
```


### <a id="6.2.8">8.filter滤镜的使用</a>
```c++
命令格式为：[输入流或标记名] 滤镜参数 [临时标记名];[输入流或标记名] 滤镜参数 [临时标记名] …
例子1：将一个图片logo作为overlay打在一段视频上，
a.使用filter_complex
ffmpeg -i input.mp4 -i logo.png -filter_complex "[1:v]scale=176:144[logo];[0:v][logo]overlay=x=0:y=0" output.mp4
b.使用-vf vf就是video filter
ffmpeg -i input.mp4 -vf "movie=log.png[wn]; [in][wn]overlay=30:10[out]" output.mp4
上面这两个命令就解释了filter_complex和vf (filter_simple)的区别，当有多个输入时，使用filter_complex,比如a中就是两个-i输入。b中只有一个输入-i，就使用-vf，这两个命令都实现相同的功能，但是filter的组合方式不一样。（暂时可以这么理解）

1.文字水印 利用drawtext filter （如果显示汉字，需要用的字体库支持汉字，本机支持的字库可以使用fc-list命令来查看）
ffmpeg -i input.mp4 -vf "drawtext=fontsize=100:fontfile=FreeSerif.ttf:text='hello world':fontcolor=green:x=20:y=20" output.mp4

2.时间水印：enable的作用是每3秒显示一次，如果要一直显示，去掉enable选项。
ffmpeg -i input.mp4 -vf "drawtext=fontsize=100:fontfile=FreeSerif.ttf:text='%{localtime\:%Y-%m-%d %H-%M-%S}’:fontcolor=green:box=1:boxcolor=yellow:enable=lt(mod(t\,3)\,1)" output.mp4 (这里注意转义字符，:和,都需要转义,因为ffmpeg中,和：都是分隔符，是有特殊意义的，所以某些命令中需要的这些符号，都需要转义，告诉ffmpeg这不是ffmpeg的分隔符。比如这里的:就是localtime命令里面的，所以就需要转移，后面的逗号也是一样)

3.图片水印：movie 滤镜
ffmpeg -i input.mp4 -vf "movie=logo.png[wm];[in][wm]overlay=30:10[out]" output.mp4
半透明水印：
有些水印图片是有背景色的，比如很多logo图片是白底的，这时overlay到视频上会非常突兀，可以利用colorkey来做成半透明效果，colorkey有三个参数(ffmpeg -h filter=colorkey) ：color,similarity, blend. color是指定要透明的颜色，similarity指定相似度，blend指定叠加的参数。下面的例子logo图片是白色底的，我们这里color就设置为white(也可以设置颜色色值，比如黑色就是全0x0000,也可以写black)，similarity设置相似度，因为我们要把白色去掉，可以设置的小一些0.2，blend设置为0.5
ffmpeg -i input.mp4 -vf "movie=logo.png, colorkey=white:0.2:0.5 [wm]; [in][wm] overlay=30:10 [out]" output.mp4

4.画中画：
ffmpeg -re -i input.mp4 -vf "movie=sub.mp4,scale=480x320[test]; [in][test] overlay=x=main_w-480:y=main_h-320 [out]" -vcodec libx264 output.flv

5跑马灯，就是子视频从左向右游动
ffmpeg -re -i input.mp4 -vf "movie=sub.mp4,scale=480x320[test]; [in][test] overlay=x='if(gte(t,2), -w+(t-2)*20,NAN)':y=0 [out]" -vcodec libx264 output.mp4

6.九宫格 shortest参数是overlay的参数，表明终止最短的视频时全部终止
ffmpeg -re -i input1.mp4 -re -i input2.mp4 -re -i input3.mp4 -re -i input4.mp4 -filter_complex "nullsrc=size=640x480 [base]; [0:v] setpts=PTS-STARTPTS, scale=320x240 [upperleft];[1:v] setpts=PTS-STARTPTS, scale=320x240 [upperright];[2:v] setpts=PTS-STARTPTS, scale=320x240 [lowerleft];[3:v] setpts=PTS-STARTPTS, scale=320x240 [lowerright]; [base][upperleft] overlay=shortest=1 [tmp1]; [tmp1][upperright] overlay=shortest=1:x=320 [tmp2]; [tmp2][lowerleft] overlay=shortest=1:y=240 [tmp3];[tmp3][lowerright] overlay=shortest=1:x=320:y=240" -c:v libx264 output.mp4

7.将字幕写入视频流 ass滤镜，这个滤镜是将字幕overlay到视频流上。
ffmpeg -i input.mp4 -vf ass=t1.ass -f mp4 output.mp4

8.将字幕流封装到文件中
ffmpeg -i input.mp4 -i t1.ass -acodec copy -vcodec copy -scodec copy output.mkv
如果input.mp4中已经有了字幕，想用t1.ass来替换原有的字幕，需要用map来指定(map n:m n指第几个输入，m指该输入的第几个流，从0开始计数。如-map 0:1 就是第0个输入的第1个流。)
ffmpeg -i input.mp4 -i t1.ass -map 0:0 -map 0:1 -map 1:0 -acodec copy -vcodec copy -scodec copy output.mkv

9.绿幕视频抠图(影视中的绿幕)
ffmpeg -i input.mp4 -i input_green.mp4 -filter_complex "[1:v]chromakey=Green:0.1:0.2[ckout];[0:v][ckout] overlay[out]" -map "[out]" output.mp4

10.获取视频截图
ffmpeg -i input.mp4 -ss 00:00:7.435 -vframes 1 out.png
定时获取截图
ffmpeg -i input.mp4 -vf fps=1 img%d.jpeg 一秒钟一张截图
ffmpeg -i input.mp4 -vf fps=1/60 img%03d.jpg 这样就是一分钟获取一帧截图

11音视频的倍速播放 atempo（音频）与setpts（视频）两个滤镜
atempo滤镜有一个参数tempo 范围为从0.5到2
ffmpeg -i input.wav -filter_complex "atempo=tempo=0.5" -acodec aac output.aac
setpts对视频倍速
ffmpeg -re -i input.mp4                                                                -filter_complex "setpts=PTS*2" output.mp4 (半速)
```


## <a id="6.3">FFmpeg函数</a>
- [0.函数概述](#6.3.0)
- [1.av_dump_format](#6.3.1)
- [2.avio_alloc_context读取内存数据](#6.3.2)
- [3.sws_scale()视频像素格式和分辨率的转换](#6.3.3)
- [4.ffmpeg解码相关函数解析](#6.3.4)
- [5.ffmpeg中AVUtil函数](#6.3.5)
- [6.FFmpeg源代码结构图-编解码](#6.3.6)
- [7.FFRMPEG4.1源码分析之IO层结构体AVIOContext&&APIs](#6.3.7)



参考连接：   
1.实战小项目之ffmpeg推流yolo视频实时检测   
https://www.cnblogs.com/tla001/p/7040344.html      

### <a id="6.3.0">6.3.0.函数概述</a>
参考链接：   
1.FFMPEG中最关键的结构体之间的关系   
http://www.voidcn.com/article/p-zrluutjq-bdq.html   

一、函数概述
```c++
FFMPEG中结构体很多。最关键的结构体可以分成以下几类：

a)解协议（http,rtsp,rtmp,mms）
AVIOContext，URLProtocol，URLContext主要存储视音频使用的协议的类型以及状态。URLProtocol存储输入视音频使用的
封装格式。每种协议都对应一个URLProtocol结构。（注意：FFMPEG中文件也被当做一种协议“file”）

b)解封装（flv,avi,rmvb,mp4）
AVFormatContext主要存储视音频封装格式中包含的信息；AVInputFormat存储输入视音频使用的封装格式。每种视音频封装
格式都对应一个AVInputFormat 结构。

c)解码（h264,mpeg2,aac,mp3）
每个AVStream存储一个视频/音频流的相关数据；每个AVStream对应一个AVCodecContext，存储该视频/音频流使用解码方式
的相关数据；每个AVCodecContext中对应一个AVCodec，包含该视频/音频对应的解码器。每种解码器都对应一个AVCodec结构。

d)存数据
视频的话，每个结构一般是存一帧；音频可能有好几帧
解码前数据：AVPacket
解码后数据：AVFrame
```

<img src="./image/6-17.png" style="zoom:200%" />


二、高级
```c++
1.在libavformat/options_table.h option参数    
static const AVOption avformat_options[] = {
{"avioflags", NULL, OFFSET(avio_flags), AV_OPT_TYPE_FLAGS, {.i64 = DEFAULT }, INT_MIN, INT_MAX, D|E, "avioflags"},
{"direct", "reduce buffering", 0, AV_OPT_TYPE_CONST, {.i64 = AVIO_FLAG_DIRECT }, INT_MIN, INT_MAX, D|E, "avioflags"},
{"probesize", "set probing size", OFFSET(probesize), AV_OPT_TYPE_INT64, {.i64 = 5000000 }, 32, INT64_MAX, D},
{"formatprobesize", "number of bytes to probe file format", OFFSET(format_probesize), AV_OPT_TYPE_INT, {.i64 = PROBE_BUF_MAX}, 0, INT_MAX-1, D},
{"packetsize", "set packet size", OFFSET(packet_size), AV_OPT_TYPE_INT, {.i64 = DEFAULT }, 0, INT_MAX, E},
{"fflags", NULL, OFFSET(flags), AV_OPT_TYPE_FLAGS, {.i64 = AVFMT_FLAG_AUTO_BSF }, INT_MIN, INT_MAX, D|E, "fflags"},
{"flush_packets", "reduce the latency by flushing out packets immediately", 0, AV_OPT_TYPE_CONST, {.i64 = AVFMT_FLAG_FLUSH_PACKETS }, INT_MIN, INT_MAX, E, "fflags"},
{"ignidx", "ignore index", 0, AV_OPT_TYPE_CONST, {.i64 = AVFMT_FLAG_IGNIDX }, INT_MIN, INT_MAX, D, "fflags"},
{"genpts", "generate pts", 0, AV_OPT_TYPE_CONST, {.i64 = AVFMT_FLAG_GENPTS }, INT_MIN, INT_MAX, D, "fflags"},
{"nofillin", "do not fill in missing values that can be exactly calculated", 0, AV_OPT_TYPE_CONST, {.i64 = AVFMT_FLAG_NOFILLIN }, INT_MIN, INT_MAX, D, "fflags"},
{"noparse", "disable AVParsers, this needs nofillin too", 0, AV_OPT_TYPE_CONST, {.i64 = AVFMT_FLAG_NOPARSE }, INT_MIN, INT_MAX, D, "fflags"},
{"igndts", "ignore dts", 0, AV_OPT_TYPE_CONST, {.i64 = AVFMT_FLAG_IGNDTS }, INT_MIN, INT_MAX, D, "fflags"},
{"discardcorrupt", "discard corrupted frames", 0, AV_OPT_TYPE_CONST, {.i64 = AVFMT_FLAG_DISCARD_CORRUPT }, INT_MIN, INT_MAX, D, "fflags"},
{"sortdts", "try to interleave outputted packets by dts", 0, AV_OPT_TYPE_CONST, {.i64 = AVFMT_FLAG_SORT_DTS }, INT_MIN, INT_MAX, D, "fflags"},
#if FF_API_LAVF_KEEPSIDE_FLAG
{"keepside", "deprecated, does nothing", 0, AV_OPT_TYPE_CONST, {.i64 = AVFMT_FLAG_KEEP_SIDE_DATA }, INT_MIN, INT_MAX, D, "fflags"},
#endif
{"fastseek", "fast but inaccurate seeks", 0, AV_OPT_TYPE_CONST, {.i64 = AVFMT_FLAG_FAST_SEEK }, INT_MIN, INT_MAX, D, "fflags"},
#if FF_API_LAVF_MP4A_LATM
{"latm", "deprecated, does nothing", 0, AV_OPT_TYPE_CONST, {.i64 = AVFMT_FLAG_MP4A_LATM }, INT_MIN, INT_MAX, E, "fflags"},
#endif
{"nobuffer", "reduce the latency introduced by optional buffering", 0, AV_OPT_TYPE_CONST, {.i64 = AVFMT_FLAG_NOBUFFER }, 0, INT_MAX, D, "fflags"},
{"bitexact", "do not write random/volatile data", 0, AV_OPT_TYPE_CONST, { .i64 = AVFMT_FLAG_BITEXACT }, 0, 0, E, "fflags" },
{"shortest", "stop muxing with the shortest stream", 0, AV_OPT_TYPE_CONST, { .i64 = AVFMT_FLAG_SHORTEST }, 0, 0, E, "fflags" },
{"autobsf", "add needed bsfs automatically", 0, AV_OPT_TYPE_CONST, { .i64 = AVFMT_FLAG_AUTO_BSF }, 0, 0, E, "fflags" },
{"seek2any", "allow seeking to non-keyframes on demuxer level when supported", OFFSET(seek2any), AV_OPT_TYPE_BOOL, {.i64 = 0 }, 0, 1, D},
{"analyzeduration", "specify how many microseconds are analyzed to probe the input", OFFSET(max_analyze_duration), AV_OPT_TYPE_INT64, {.i64 = 0 }, 0, INT64_MAX, D},
{"cryptokey", "decryption key", OFFSET(key), AV_OPT_TYPE_BINARY, {.dbl = 0}, 0, 0, D},
{"indexmem", "max memory used for timestamp index (per stream)", OFFSET(max_index_size), AV_OPT_TYPE_INT, {.i64 = 1<<20 }, 0, INT_MAX, D},
{"rtbufsize", "max memory used for buffering real-time frames", OFFSET(max_picture_buffer), AV_OPT_TYPE_INT, {.i64 = 3041280 }, 0, INT_MAX, D}, /* defaults to 1s of 15fps 352x288 YUYV422 video */
{"fdebug", "print specific debug info", OFFSET(debug), AV_OPT_TYPE_FLAGS, {.i64 = DEFAULT }, 0, INT_MAX, E|D, "fdebug"},
{"ts", NULL, 0, AV_OPT_TYPE_CONST, {.i64 = FF_FDEBUG_TS }, INT_MIN, INT_MAX, E|D, "fdebug"},
{"max_delay", "maximum muxing or demuxing delay in microseconds", OFFSET(max_delay), AV_OPT_TYPE_INT, {.i64 = -1 }, -1, INT_MAX, E|D},
{"start_time_realtime", "wall-clock time when stream begins (PTS==0)", OFFSET(start_time_realtime), AV_OPT_TYPE_INT64, {.i64 = AV_NOPTS_VALUE}, INT64_MIN, INT64_MAX, E},
{"fpsprobesize", "number of frames used to probe fps", OFFSET(fps_probe_size), AV_OPT_TYPE_INT, {.i64 = -1}, -1, INT_MAX-1, D},
{"audio_preload", "microseconds by which audio packets should be interleaved earlier", OFFSET(audio_preload), AV_OPT_TYPE_INT, {.i64 = 0}, 0, INT_MAX-1, E},
{"chunk_duration", "microseconds for each chunk", OFFSET(max_chunk_duration), AV_OPT_TYPE_INT, {.i64 = 0}, 0, INT_MAX-1, E},
{"chunk_size", "size in bytes for each chunk", OFFSET(max_chunk_size), AV_OPT_TYPE_INT, {.i64 = 0}, 0, INT_MAX-1, E},
/* this is a crutch for avconv, since it cannot deal with identically named options in different contexts.
 * to be removed when avconv is fixed */
{"f_err_detect", "set error detection flags (deprecated; use err_detect, save via avconv)", OFFSET(error_recognition), AV_OPT_TYPE_FLAGS, {.i64 = AV_EF_CRCCHECK }, INT_MIN, INT_MAX, D, "err_detect"},
{"err_detect", "set error detection flags", OFFSET(error_recognition), AV_OPT_TYPE_FLAGS, {.i64 = AV_EF_CRCCHECK }, INT_MIN, INT_MAX, D, "err_detect"},
{"crccheck", "verify embedded CRCs", 0, AV_OPT_TYPE_CONST, {.i64 = AV_EF_CRCCHECK }, INT_MIN, INT_MAX, D, "err_detect"},
{"bitstream", "detect bitstream specification deviations", 0, AV_OPT_TYPE_CONST, {.i64 = AV_EF_BITSTREAM }, INT_MIN, INT_MAX, D, "err_detect"},
{"buffer", "detect improper bitstream length", 0, AV_OPT_TYPE_CONST, {.i64 = AV_EF_BUFFER }, INT_MIN, INT_MAX, D, "err_detect"},
{"explode", "abort decoding on minor error detection", 0, AV_OPT_TYPE_CONST, {.i64 = AV_EF_EXPLODE }, INT_MIN, INT_MAX, D, "err_detect"},
{"ignore_err", "ignore errors", 0, AV_OPT_TYPE_CONST, {.i64 = AV_EF_IGNORE_ERR }, INT_MIN, INT_MAX, D, "err_detect"},
{"careful",    "consider things that violate the spec, are fast to check and have not been seen in the wild as errors", 0, AV_OPT_TYPE_CONST, {.i64 = AV_EF_CAREFUL }, INT_MIN, INT_MAX, D, "err_detect"},
{"compliant",  "consider all spec non compliancies as errors", 0, AV_OPT_TYPE_CONST, {.i64 = AV_EF_COMPLIANT }, INT_MIN, INT_MAX, D, "err_detect"},
{"aggressive", "consider things that a sane encoder shouldn't do as an error", 0, AV_OPT_TYPE_CONST, {.i64 = AV_EF_AGGRESSIVE }, INT_MIN, INT_MAX, D, "err_detect"},
{"use_wallclock_as_timestamps", "use wallclock as timestamps", OFFSET(use_wallclock_as_timestamps), AV_OPT_TYPE_BOOL, {.i64 = 0}, 0, 1, D},
{"skip_initial_bytes", "set number of bytes to skip before reading header and frames", OFFSET(skip_initial_bytes), AV_OPT_TYPE_INT64, {.i64 = 0}, 0, INT64_MAX-1, D},
{"correct_ts_overflow", "correct single timestamp overflows", OFFSET(correct_ts_overflow), AV_OPT_TYPE_BOOL, {.i64 = 1}, 0, 1, D},
{"flush_packets", "enable flushing of the I/O context after each packet", OFFSET(flush_packets), AV_OPT_TYPE_INT, {.i64 = -1}, -1, 1, E},
{"metadata_header_padding", "set number of bytes to be written as padding in a metadata header", OFFSET(metadata_header_padding), AV_OPT_TYPE_INT, {.i64 = -1}, -1, INT_MAX, E},
{"output_ts_offset", "set output timestamp offset", OFFSET(output_ts_offset), AV_OPT_TYPE_DURATION, {.i64 = 0}, -INT64_MAX, INT64_MAX, E},
{"max_interleave_delta", "maximum buffering duration for interleaving", OFFSET(max_interleave_delta), AV_OPT_TYPE_INT64, { .i64 = 10000000 }, 0, INT64_MAX, E },
{"f_strict", "how strictly to follow the standards (deprecated; use strict, save via avconv)", OFFSET(strict_std_compliance), AV_OPT_TYPE_INT, {.i64 = DEFAULT }, INT_MIN, INT_MAX, D|E, "strict"},
{"strict", "how strictly to follow the standards", OFFSET(strict_std_compliance), AV_OPT_TYPE_INT, {.i64 = DEFAULT }, INT_MIN, INT_MAX, D|E, "strict"},
{"very", "strictly conform to a older more strict version of the spec or reference software", 0, AV_OPT_TYPE_CONST, {.i64 = FF_COMPLIANCE_VERY_STRICT }, INT_MIN, INT_MAX, D|E, "strict"},
{"strict", "strictly conform to all the things in the spec no matter what the consequences", 0, AV_OPT_TYPE_CONST, {.i64 = FF_COMPLIANCE_STRICT }, INT_MIN, INT_MAX, D|E, "strict"},
{"normal", NULL, 0, AV_OPT_TYPE_CONST, {.i64 = FF_COMPLIANCE_NORMAL }, INT_MIN, INT_MAX, D|E, "strict"},
{"unofficial", "allow unofficial extensions", 0, AV_OPT_TYPE_CONST, {.i64 = FF_COMPLIANCE_UNOFFICIAL }, INT_MIN, INT_MAX, D|E, "strict"},
{"experimental", "allow non-standardized experimental variants", 0, AV_OPT_TYPE_CONST, {.i64 = FF_COMPLIANCE_EXPERIMENTAL }, INT_MIN, INT_MAX, D|E, "strict"},
{"max_ts_probe", "maximum number of packets to read while waiting for the first timestamp", OFFSET(max_ts_probe), AV_OPT_TYPE_INT, { .i64 = 50 }, 0, INT_MAX, D },
{"avoid_negative_ts", "shift timestamps so they start at 0", OFFSET(avoid_negative_ts), AV_OPT_TYPE_INT, {.i64 = -1}, -1, 2, E, "avoid_negative_ts"},
{"auto",              "enabled when required by target format",    0, AV_OPT_TYPE_CONST, {.i64 = AVFMT_AVOID_NEG_TS_AUTO },              INT_MIN, INT_MAX, E, "avoid_negative_ts"},
{"disabled",          "do not change timestamps",                  0, AV_OPT_TYPE_CONST, {.i64 = 0 },                                    INT_MIN, INT_MAX, E, "avoid_negative_ts"},
{"make_non_negative", "shift timestamps so they are non negative", 0, AV_OPT_TYPE_CONST, {.i64 = AVFMT_AVOID_NEG_TS_MAKE_NON_NEGATIVE }, INT_MIN, INT_MAX, E, "avoid_negative_ts"},
{"make_zero",         "shift timestamps so they start at 0",       0, AV_OPT_TYPE_CONST, {.i64 = AVFMT_AVOID_NEG_TS_MAKE_ZERO },         INT_MIN, INT_MAX, E, "avoid_negative_ts"},
{"dump_separator", "set information dump field separator", OFFSET(dump_separator), AV_OPT_TYPE_STRING, {.str = ", "}, CHAR_MIN, CHAR_MAX, D|E},
{"codec_whitelist", "List of decoders that are allowed to be used", OFFSET(codec_whitelist), AV_OPT_TYPE_STRING, { .str = NULL },  CHAR_MIN, CHAR_MAX, D },
{"format_whitelist", "List of demuxers that are allowed to be used", OFFSET(format_whitelist), AV_OPT_TYPE_STRING, { .str = NULL },  CHAR_MIN, CHAR_MAX, D },
{"protocol_whitelist", "List of protocols that are allowed to be used", OFFSET(protocol_whitelist), AV_OPT_TYPE_STRING, { .str = NULL },  CHAR_MIN, CHAR_MAX, D },
{"protocol_blacklist", "List of protocols that are not allowed to be used", OFFSET(protocol_blacklist), AV_OPT_TYPE_STRING, { .str = NULL },  CHAR_MIN, CHAR_MAX, D },
{"max_streams", "maximum number of streams", OFFSET(max_streams), AV_OPT_TYPE_INT, { .i64 = 1000 }, 0, INT_MAX, D },
{"skip_estimate_duration_from_pts", "skip duration calculation in estimate_timings_from_pts", OFFSET(skip_estimate_duration_from_pts), AV_OPT_TYPE_BOOL, {.i64 = 0}, 0, 1, D},
{NULL},
};

{"genpts", "generate pts", 0, AV_OPT_TYPE_CONST, {.i64 = AVFMT_FLAG_GENPTS }, INT_MIN, INT_MAX, D, "fflags"},{"fastseek", "fast but inaccurate seeks", 0, AV_OPT_TYPE_CONST, {.i64 = AVFMT_FLAG_FAST_SEEK }, INT_MIN, INT_MAX, D, "fflags"},

ffmpeg -fflags +genpts -i input.ts -vcodec copy -an out.mkv


2.函数总结
avio_open打开filename
avformat_alloc_context打开封装
avcodec_find_encoder找到编码器
avformat_new_stream根据code找到stream

a.avformat_open_input(&pFormatCtx, filepath, NULL, &options)打开rtmp流或者文件，保存到formatctx中。
b.根据pFormatCtx->streams[i]->codec->codec_type == AVMEDIA_TYPE_VIDEO，找到videoindex。
c.pCodecCtx = pFormatCtx->streams[videoindex]->codec;找到codecctx，根据codecx找到codec。
d.pCodec = avcodec_find_decoder(pCodecCtx->codec_id);(avcodec_open2(pCodecCtx, pCodec, NULL)打开codec。
e.使用av_read_frame(pFormatCtx, packet)获取packet
f.使用ret = avcodec_decode_video2(pCodecCtx, pFrame, &got_picture,packet);获取frame。
g.根据got_picture确定获取，使用sws_scale用来做视频像素格式和分辨率的转换。与其类似的函数还有： 
sws_getCachedContext ，区别在于： sws_getContext 可以用于多路码流转换，为每个不同的码流都指定一个不同的转换
上下文，而 sws_getCachedContext 只能用于一路码流转换。 
img_convert_ctx = sws_getContext(pCodecCtx->width, pCodecCtx->height,pCodecCtx->pix_fmt, pCodecCtx->width, 
pCodecCtx->height,AV_PIX_FMT_BGR24, SWS_BICUBIC, NULL, NULL, NULL);
sws_scale(img_convert_ctx,(const uint8_t* const *) pFrame->data, pFrame->linesize,0, pCodecCtx->height,
 pFrameRGB->data,pFrameRGB->linesize);

//打开编码器
avcodec_open2(c, video_codec, NULL);
/*写入数据流的头部*/
avformat_write_header(fmtctx, NULL);
//打印SDP信息, 该信息可用于Rtp流接收解码
char sdp[2048];
int tmp = av_sdp_create(&fmtctx,1, sdp, sizeof(sdp));

视频读取   
av_read_frame(pFormatCtx_decode, packet)
视频解码
avcodec_decode_video2(pCodecCtx_decode, pFrame, &got_picture, packet)
格式转换
sws_scale(img_convert_ctx,pFrame->data, pFrame->linesize, 0, pCodecCtx_decode->height

AVCodecParameters *in_codecpar = in_stream->codecpar;获取codec编码参数

AVRational typedef struct AVRational{
    int num; ///< Numerator
    int den; ///< Denominator
} AVRational;
rational number就是可以表示成p/q两个整数相除的数。其实就是传说中的有理数。numberator: 分子 denominator: 分母

time_base：流的时间基准，是一个实数，该流中媒体数据的pts和dts都将以这个时间基准为粒度。
如（1,25），那么时间刻度就是1/25
    （1,9000），那么时间刻度就是1/90000
那么，在刻度为1/25的体系下的time=5，转换成在刻度为1/90000体系下的时间time为(5*1/25)/(1/90000) = 3600*5=18000 
ffmpeg中做pts计算时，存在大量这种转换。

out_stream = avformat_new_stream(ofmt_ctx, NULL);
将一个新流(out_stream)添加到输出文件(ofmt_ctx)

将当前输入流中的参数拷贝到输出流中
ret = avcodec_parameters_copy(out_stream->codecpar, in_codecpar);
```

### <a id="6.3.1">6.3.1.av_dump_format</a>
```cpp
void av_dump_format(AVFormatContext *ic,
                    int index,
                    const char *url,
                    int is_output);

参数：
* Print detailed information about the input or output format, such as
 * duration, bitrate, streams, container, programs, metadata, side data,
 * codec and time base.
 *
 * @param ic        the context to analyze
 * @param index     index of the stream to dump information about
 * @param url       the URL to print, such as source or destination file
 * @param is_output Select whether the specified context is an input(0) or output(1)

此函数的作用：av_dump_format()是一个手工调试的函数，能使我们看到pFormatCtx->streams里面有什么内容。
一般接下来我们使用av_find_stream_info()函数，它的作用是为pFormatCtx->streams填充上正确的信息。
调用这个函数就可以看见流信息。
```


### <a id="6.3.2">6.3.2.avio_alloc_context读取内存数据</a>
```c++
设置回调读取内存中数据

pb = avio_alloc_context(
		pIObuffer,
		4096,
		0,
		param,
		fill_iobuffer,  //读取内存中数据
		NULL,
		NULL);

pFormatContext = avformat_alloc_context();
pFormatContext->pb = pb;

pFormatContext->max_analyze_duration = 2000000;
pFormatContext->fps_probe_size = 5;

m_result = avformat_open_input(&pFormatContext, "", piFmt, NULL);
if (m_result < 0)
{
    return -1;
}
```

### <a id="6.3.0">6.3.3.sws_scale()视频像素格式和分辨率的转换</a>
FFmpeg中的 sws_scale() 函数主要是用来做视频像素格式和分辨率的转换，其优势在于：可以在同一个函数里实现：1.图像色彩空间转换， 2:分辨率缩放，3:前后图像滤波处理。不足之处在于：效率相对较低，不如libyuv或shader，其关联的函数主要有：
```c++
1.sws_getContext():
struct SwsContext *sws_getContext(
            int srcW, /* 输入图像的宽度 */
            int srcH, /* 输入图像的宽度 */
            enum AVPixelFormat srcFormat, /* 输入图像的像素格式 */
            int dstW, /* 输出图像的宽度 */
            int dstH, /* 输出图像的高度 */
            enum AVPixelFormat dstFormat, /* 输出图像的像素格式 */
            int flags,/* 选择缩放算法(只有当输入输出图像大小不同时有效),一般选择SWS_FAST_BILINEAR */
            SwsFilter *srcFilter, /* 输入图像的滤波器信息, 若不需要传NULL */
            SwsFilter *dstFilter, /* 输出图像的滤波器信息, 若不需要传NULL */
            const double *param /* 特定缩放算法需要的参数(?)，默认为NULL */
            );

与其类似的函数还有： sws_getCachedContext ，区别在于： sws_getContext 可以用于多路码流转换，为每个不同的码流都指定一个不同的转换上下文，而 sws_getCachedContext 只能用于一路码流转换。

2.sws_freeContext
// 释放sws_scale
void sws_freeContext(struct SwsContext *swsContext);
真正用来做转换的函数则是： sws_scale() ，其函数定义如下：

int sws_scale(struct SwsContext *c, const uint8_t *const srcSlice[],
              const int srcStride[], int srcSliceY, int srcSliceH,
              uint8_t *const dst[], const int dstStride[]);

下面对其函数参数进行详细说明：
1.参数 SwsContext *c， 转换格式的上下文。也就是 sws_getContext 函数返回的结果。

2.参数 const uint8_t *const srcSlice[], 输入图像的每个颜色通道的数据指针。其实就是解码后的AVFrame中的data[]数组。因为不同像素的存储格式不同，所以srcSlice[]维数
也有可能不同。
以YUV420P为例，它是planar格式，它的内存中的排布如下：
YYYYYYYY UUUU VVVV
使用FFmpeg解码后存储在AVFrame的data[]数组中时：
data[0]——-Y分量, Y1, Y2, Y3, Y4, Y5, Y6, Y7, Y8……
data[1]——-U分量, U1, U2, U3, U4……
data[2]——-V分量, V1, V2, V3, V4……
linesize[]数组中保存的是对应通道的数据宽度 ，
linesize[0]——-Y分量的宽度
linesize[1]——-U分量的宽度
linesize[2]——-V分量的宽度
而RGB24，它是packed格式，它在data[]数组中则只有一维，它在存储方式如下：
data[0]: R1, G1, B1, R2, G2, B2, R3, G3, B3, R4, G4, B4……
这里要特别注意，linesize[0]的值并不一定等于图片的宽度，有时候为了对齐各解码器的CPU，实际尺寸会大于图片的宽度，这点在我们编程时（比如OpengGL硬件转换/渲染）要特别注意，否则解码出来的图像会异常。

3.参数const int srcStride[]，输入图像的每个颜色通道的跨度。.也就是每个通道的行字节数，对应的是解码后的AVFrame中的linesize[]数组。根据它可以确立下一行的起始位置，不过stride和width不一定相同，这是因为：
a.由于数据帧存储的对齐，有可能会向每行后面增加一些填充字节这样 stride = width + N；
b.packet色彩空间下，每个像素几个通道数据混合在一起，例如RGB24，每个像素3字节连续存放，因此下一行的位置需要跳过3*width字节。

4.参数int srcSliceY, int srcSliceH,定义在输入图像上处理区域，srcSliceY是起始位置，srcSliceH是处理多少行。如果srcSliceY=0，srcSliceH=height，表示一次性处理完整个图像。这种设置是为了多线程并行，例如可以创建两个线程，第一个线程处理 [0, h/2-1]行，第二个线程处理 [h/2, h-1]行。并行处理加快速度。

5.参数uint8_t *const dst[], const int dstStride[]定义输出图像信息（输出的每个颜色通道数据指针，每个颜色通道行字节数）

代码示例：将解码后的数据转换成1280*720的RGBA8888 格式
1. 定义转换格式的上下文
vctx = sws_getCachedContext(
                        vctx,
                        frame->width, // 源图像的宽度
                        frame->height, //  源图像的高度
                        (AVPixelFormat)frame->format,
                        outWidth,
                        outHeight,
                        AV_PIX_FMT_RGBA,
                        SWS_FAST_BILINEAR,
                        0, 0, 0
                        );

2. 开始转换
int outWidth = 1280;
int outHeight = 720;
char *rgb = new char[1920*1080*4];
uint8_t *data[AV_NUM_DATA_POINTERS] = {0};

data[0] = (uint8_t *)rgb;
int lines[AV_NUM_DATA_POINTERS] = {0};
lines[0] = outWidth * 4;
int h = sws_scale(vctx,
            (const uint8_t **)frame->data,
            frame->linesize,
            0,
            frame->height,
            data,
            lines
        ); 
```


### <a id="6.3.4">6.3.4.ffmpeg解码相关函数解析</a>
参考链接：   
1.ffmpeg总体调用    
https://blog.csdn.net/evsqiezi/article/details/51564657   

本文主要以转码为例子，流程主要为解析命令，转码，转码又可以细分为：打开编解码器，开始转码，关闭编解码器。

<img src="./image/6-17.jfif" style="zoom:200%" />

#### 1.avformat_find_stream_info
参考链接：  
1.javacpp-FFmpeg系列补充：FFmpeg解决avformat_find_stream_info检索时间过长问题     
https://blog.csdn.net/eguid_1/article/details/82754352     

avformat_find_stream_info()函数可以读取一部分视音频数据并且获得一些相关的信息。函数正常执行后返回值大于等于0。该函数上下文部分是调节起播延迟效果最明显的地方。函数分析流的时间主要由传入的AVFormatContext的probesize 和 max_analyze_duration两个属性决定，probesize是探测读入大小，默认值为32K；max_analyze_duration默认值为5S。在网络状况比较好的情况下可分别设置4K和1S，也可根据具体情况在起播画面效果、起播延迟、分析成功率等因素间取舍。这两个参数的设置在avformat_open_input()函数对结构体AVFormatContext处理完成后进行设置。

```c++
一、解决办法
（1）rtsp解决办法
rtsp可以通过设置rtsp_transport为tcp的方式来降低检索时长，相对于默认的udp的很多限制，tcp会更可靠

（2）rtsp/rtmp通用解决办法
设置probesize，这个值可以限制最大的读取缓存，比如500*1024就是500k，500k可以保证高清视频也可以读取到一帧视频帧，如果太小就会导致读取不到完整视频帧的问题，所以这个大小需要通过实际情况进行控制。
设置max_analyze_duration，比如设置为5 * AV_TIME_BASE，这个值可以设置avformat_find_stream_info这个函数的持续时长，超过这个时间不结束也会结束，我们设置为5秒

二、其他解决办法
1、跳过avformat_find_stream_info函数
如果已知源流媒体源的信息，可以选择跳过这个函数，直接设置给AVFormatContext设置参数；
2、尝试手动读取视频帧和音频帧进行解析
当然也可以尝试读取一帧视频帧或音频帧，可以读取到一些基本信息
```



<img src="./image/6-18.png" style="zoom:100%" />

在ffmpeg_parse_options中，会调用libavfornat/utils.c里的avformat_find_stream_info，其中会会调用try_decode_frame（avcodec_open2，avcodec_decode_xxx，），avcodec_close；重要步骤为：   
1.查找解码器：find_decoder()  
2.打开解码器：avcodec_open2()  
3.读取完整的一帧压缩编码的数据：read_frame_internal()  
4.解码一些压缩编码数据：try_decode_frame()   

#### 2.choose_output()
用于选择一个OutputStream。比如有一个audio，一个video，那要根据pts策略，比如谁的pts比较小，就挑哪个OutputStream先干活。
transcode_frome_filter()函数用于选个一个InputStream，用于下一步的process_input()。
process_input()函数主要是解码，并把解码的buffer送往filter处理，送入滤镜。

#### 3.update_stream_timings
start_time1 = av_rescale_q(st->start_time, st->time_base,AV_TIME_BASE_Q);
start_time = FFMIN(start_time, start_time1);
ic->start_time = start_time;  //90000基转成1000000基，修改了开始时间。如133200变成1480000。

#### 4.try_decode_frame
从try_decode_frame()的定义可以看出，该函数首先判断视音频流的解码器是否已经打开，如果没有打开的话，先打开相应的解码器。接下来根据视音频流类型的不同，调用不同的解码函数进行解码：视频流调用avcodec_decode_video2()，音频流调用avcodec_decode_audio4()，字幕流调用avcodec_decode_subtitle2()。解码的循环会一直持续下去直到满足了while()的所有条件。while()语句的条件中有一个has_codec_parameters()函数，用于判断AVStream中的成员变量是否都已经设置完毕。该函数在avformat_find_stream_info()中的多个地方被使用过。


### <a id="6.3.5">6.3.5.ffmpeg中AVUtil函数</a>
```c++
void test_log(){
	av_register_all();
	AVFormatContext *obj=NULL;
	obj=avformat_alloc_context();
	printf("====================================\n");
	av_log(obj,AV_LOG_PANIC,"Panic: Something went really wrong and we will crash now.\n");
	av_log(obj,AV_LOG_FATAL,"Fatal: Something went wrong and recovery is not possible.\n");
	av_log(obj,AV_LOG_ERROR,"Error: Something went wrong and cannot losslessly be recovered.\n");
	av_log(obj,AV_LOG_WARNING,"Warning: This may or may not lead to problems.\n");
	av_log(obj,AV_LOG_INFO,"Info: Standard information.\n");
	av_log(obj,AV_LOG_VERBOSE,"Verbose: Detailed information.\n");
	av_log(obj,AV_LOG_DEBUG,"Debug: Stuff which is only useful for libav* developers.\n");
	printf("====================================\n");
	avformat_free_context(obj);
}

#if TEST_OPT
	av_opt_set(pCodecCtx,"b","400000",0);		//bitrate
	//Another method
	//av_opt_set_int(pCodecCtx,"b",400000,0);	//bitrate
 
	av_opt_set(pCodecCtx,"time_base","1/25",0);	//time_base
	av_opt_set(pCodecCtx,"bf","5",0);			//max b frame
	av_opt_set(pCodecCtx,"g","25",0);			//gop
	av_opt_set(pCodecCtx,"qmin","10",0);		//qmin/qmax
	av_opt_set(pCodecCtx,"qmax","51",0);
#else
	pCodecCtx->time_base.num = 1;  
	pCodecCtx->time_base.den = 25;  
	pCodecCtx->max_b_frames=5;
	pCodecCtx->bit_rate = 400000;  
	pCodecCtx->gop_size=25;
	pCodecCtx->qmin = 10;
	pCodecCtx->qmax = 51;
#endif

av_opt_set(pCodecCtx->priv_data,"preset","slow",0);
//tune: film, animation, grain, stillimage, psnr, 
//ssim, fastdecode, zerolatency
av_opt_set(pCodecCtx->priv_data,"tune","zerolatency",0);
//profile: baseline, main, high, high10, high422, high444
av_opt_set(pCodecCtx->priv_data,"profile","main",0);

//print
av_opt_get(pCodecCtx->priv_data,"preset",0,(uint8_t **)&val_str);
printf("preset val: %s\n",val_str);
av_opt_get(pCodecCtx->priv_data,"tune",0,(uint8_t **)&val_str);
printf("tune val: %s\n",val_str);
av_opt_get(pCodecCtx->priv_data,"profile",0,(uint8_t **)&val_str);
printf("profile val: %s\n",val_str);


void test_avdictionary(){
 
	AVDictionary *d = NULL;
	AVDictionaryEntry *t = NULL;
 
	av_dict_set(&d, "name", "lei xiaohua", 0);
	av_dict_set(&d, "email", "leixiaohua1020@126.com", 0);
	av_dict_set(&d, "school", "cuc", 0);
	av_dict_set(&d, "gender", "man", 0);
	av_dict_set(&d, "website", "http://blog.csdn.net/leixiaohua1020", 0);
	//av_strdup()
	char *k = av_strdup("location");
	char *v = av_strdup("Beijing-China");
	av_dict_set(&d, k, v, AV_DICT_DONT_STRDUP_KEY | AV_DICT_DONT_STRDUP_VAL);
	printf("====================================\n");
	int dict_cnt= av_dict_count(d);
	printf("dict_count:%d\n",dict_cnt);
	printf("dict_element:\n");
	while (t = av_dict_get(d, "", t, AV_DICT_IGNORE_SUFFIX)) {
		printf("key:%10s  |  value:%s\n",t->key,t->value);
	}
 
	t = av_dict_get(d, "email", t, AV_DICT_IGNORE_SUFFIX);
	printf("email is %s\n",t->value);
	printf("====================================\n");
	av_dict_free(&d);
}


void test_parseutil(){
	char input_str[100]={0};
	printf("========= Parse Video Size =========\n");
	int output_w=0;
	int output_h=0;
	strcpy(input_str,"1920x1080");
	av_parse_video_size(&output_w,&output_h,input_str);
	printf("w:%4d | h:%4d\n",output_w,output_h);
	strcpy(input_str,"vga");
	//strcpy(input_str,"hd1080");
	//strcpy(input_str,"ntsc");
	av_parse_video_size(&output_w,&output_h,input_str);
	printf("w:%4d | h:%4d\n",output_w,output_h);
	printf("========= Parse Frame Rate =========\n");
	AVRational output_rational={0,0};
	strcpy(input_str,"15/1");
	av_parse_video_rate(&output_rational,input_str);
	printf("framerate:%d/%d\n",output_rational.num,output_rational.den);
	strcpy(input_str,"pal");
	av_parse_video_rate(&output_rational,input_str);
	printf("framerate:%d/%d\n",output_rational.num,output_rational.den);
	printf("=========== Parse Time =============\n");
	int64_t output_timeval;
	strcpy(input_str,"00:01:01");
	av_parse_time(&output_timeval,input_str,1);
	printf("microseconds:%lld\n",output_timeval);
	printf("====================================\n");
}
```

### <a id="6.3.6">6.3.6.FFmpeg源代码结构图</a>

<img src="./image/6-19.jpg" style="zoom:100%" />

```c++
函数在图中以方框的形式表现出来。不同的背景色标志了该函数不同的作用：
粉红色背景函数：FFmpeg的API函数。
白色背景的函数：FFmpeg的内部函数。
黄色背景的函数：URLProtocol结构体中的函数，包含处理协议（Protocol）的功能。
绿色背景的函数：AVInputFormat结构体中的函数，包含处理封装格式（Format）的功能。
蓝色背景的函数：AVCodec结构体中的函数，包含了编解码器（Codec）的功能。

整个架构图可以分为以下几个区域：
左边区域——架构函数区域：这些函数并不针对某一特定的视频格式。
右上方黄色区域——协议处理函数区域：不同的协议（RTP，RTMP，FILE）会调用不同的协议处理函数。
右边中间绿色区域——封装格式处理函数区域：不同的封装格式（MKV，FLV，MPEGTS，AVI）会调用不同的封装格式处理函数。
右边下方蓝色区域——编解码函数区域：不同的编码标准（HEVC，H.264，MPEG2）会调用不同的编解码函数。

左边区域（FFmpeg架构函数）
1.  av_register_all()【函数简单分析】
	1)  avcodec_register_all()
		(a) REGISTER_HWACCEL()
		(b) REGISTER_ENCODER()
		(c) REGISTER_DECODER()
		(d) REGISTER_PARSER()
		(e) REGISTER_BSF()
	2)  REGISTER_MUXER()
	3)  REGISTER_DEMUXER()
	4)  REGISTER_PROTOCOL()

2.  avformat_alloc_context()【函数简单分析】
	1) av_malloc(sizeof(AVFormatContext))
	2) avformat_get_context_defaults()
	(a) av_opt_set_defaults()

3.  avformat_open_input()【函数简单分析】
	1) init_input()
	(a) avio_open2()【函数简单分析】
		a) ffurl_open()
		i. ffurl_alloc()
			l  url_find_protocol()
			l  url_alloc_for_protocol()
		ii. ffurl_connect()
			URLProtocol->url_open()
		b) ffio_fdopen()
		i. av_malloc(buffer_size)
		ii. avio_alloc_context()
		l  av_mallocz(sizeof(AVIOContext))
		l  ffio_init_context()
 	(b) av_probe_input_buffer2()
		a) avio_read()
			i.  AVInputFormat->read_packet()
		b) av_probe_input_format2()
		c) av_probe_input_format3()
			i. av_iformat_next()
			ii. av_match_name()
			iii. av_match_ext()
			iv. AVInputFormat->read_probe()
	2) AVInputFormat->read_header()

4. avformat_find_stream_info()【函数简单分析】
	1) find_decoder()
		(a) avcodec_find_decoder()
	2) avcodec_open2()
	3) read_frame_internal()
	4) try_decode_frame()
		(a) avcodec_decode_video2()
	5) avcodec_close()
	6) estimate_timings()
		(a)  estimate_timings_from_pts()
		(b)  estimate_timings_from_bit_rate()
		(c)  update_stream_timings()

5. avcodec_find_decoder()【函数简单分析】
 1) find_encdec()

6. avcodec_open2()【函数简单分析】
 1) AVCodec->init()

7. av_read_frame()【函数简单分析】
 1) read_from_packet_buffer()
 2) read_frame_internal()
	(a) ff_read_packet()
 		a) AVInputFormat->read_packet()
 	(b) parse_packet()
 		a) av_parser_parse2()

8. avcodec_decode_video2()【函数简单分析】
 1) av_packet_split_side_data()
 2) AVCodec-> decode()
 3) av_frame_set_pkt_pos()
 4) av_frame_set_best_effort_timestamp()

9. avcodec_close()【函数简单分析】
 1) AVCodec->close()

10. avformat_close_input()【函数简单分析】
	1) AVInputFormat->read_close()
	2) avformat_free_context()
		(a) ff_free_stream()
	3) avio_close()
		(a) avio_flush()
			a) flush_buffer()
		(b) ffurl_close()
			a) ffurl_closep()
			URLProtocol->url_close()



右上区域（URLProtocol协议处理函数）
URLProtocol结构体包含如下协议处理函数指针：
url_open()：打开
url_read()：读取
url_write()：写入
url_seek()：调整进度
url_close()：关闭


【例子】不同的协议对应着上述接口有不同的实现函数，举几个例子：
File协议（即文件）对应的URLProtocol结构体ff_file_protocol：
url_open() -> file_open() -> open()
url_read() -> file_read() -> read()
url_write() -> file_write() -> write()
url_seek() -> file_seek() -> lseek()
url_close() -> file_close() -> close()

RTMP协议（libRTMP）对应的URLProtocol结构体ff_librtmp_protocol：
url_open() -> rtmp_open() -> RTMP_Init(), RTMP_SetupURL(), RTMP_Connect(), RTMP_ConnectStream()
url_read() -> rtmp_read() -> RTMP_Read()
url_write() -> rtmp_write() -> RTMP_Write()
url_seek() -> rtmp_read_seek() -> RTMP_SendSeek()
url_close() -> rtmp_close() -> RTMP_Close()

UDP协议对应的URLProtocol结构体ff_udp_protocol：
url_open() -> udp_open()
url_read() -> udp_read()
url_write() -> udp_write()
url_seek() -> udp_close()
url_close() -> udp_close()

右中区域（AVInputFormat封装格式处理函数）
AVInputFormat包含如下封装格式处理函数指针：
read_probe()：检查格式
read_header()：读取文件头
read_packet()：读取一帧数据
read_seek()：调整进度
read_close()：关闭


【例子】不同的封装格式对应着上述接口有不同的实现函数，举几个例子：
FLV封装格式对应的AVInputFormat结构体ff_flv_demuxer：
read_probe() -> flv_probe() –> probe()
read_header() -> flv_read_header() -> create_stream() -> avformat_new_stream()
read_packet() -> flv_read_packet()
read_seek() -> flv_read_seek()
read_close() -> flv_read_close()

MKV封装格式对应的AVInputFormat结构体ff_matroska_demuxer：
read_probe() -> matroska_probe()
read_header() -> matroska_read_header()
read_packet() -> matroska_read_packet()
read_seek() -> matroska_read_seek()
read_close() -> matroska_read_close()

MPEG2TS封装格式对应的AVInputFormat结构体ff_mpegts_demuxer：
read_probe() -> mpegts_probe()
read_header() -> mpegts_read_header()
read_packet() -> mpegts_read_packet() 
read_close() -> mpegts_read_close()

AVI封装格式对应的AVInputFormat结构体ff_avi_demuxer：
read_probe() -> avi_probe()
read_header() -> avi_read_header()
read_packet() -> avi_read_packet()
read_seek() -> avi_read_seek()
read_close() -> avi_read_close()

右下区域（AVCodec编解码函数）
AVCodec包含如下编解码函数指针：
init()：初始化
decode()：解码一帧数据
close()：关闭

【例子】不同的编解码器对应着上述接口有不同的实现函数，举几个例子：
HEVC解码对应的AVCodec结构体ff_hevc_decoder：
init() -> hevc_decode_init()
decode() -> hevc_decode_frame() -> decode_nal_units()
close() -> hevc_decode_free()

H.264解码对应的AVCodec结构体ff_h264_decoder：
init() -> ff_h264_decode_init()
decode() -> h264_decode_frame() -> decode_nal_units()
close() -> h264_decode_end()

VP8解码（libVPX）对应的AVCodec结构体ff_libvpx_vp8_decoder：
init() -> vpx_init() -> vpx_codec_dec_init()
decode() -> vp8_decode() -> vpx_codec_decode(), vpx_codec_get_frame()
close() -> vp8_free() -> vpx_codec_destroy()

MPEG2解码对应的AVCodec结构体ff_mpeg2video_decoder：
init() -> mpeg_decode_init()
decode() -> mpeg_decode_frame()
close() -> mpeg_decode_end()
```

### <a id="6.3.7">6.3.7.FFRMPEG4.1源码分析之IO层结构体AVIOContext&&APIs</a>
参考链接：   
1.FFRMPEG4.1源码分析之IO层结构体AVIOContext&&APIs    
https://blog.csdn.net/ice_ly000/article/details/93032911   

```c++
AVIOContext结构体处于FFMPEG音视频处理的IO层（也有称为协议层，不过个人认为叫做IO层更合适）.FFMPEG中的I/O
层定义在avformat库中，头文件为avio.h，该头文件包含了IO相关的结构体定义以及IO层的Public APIs，大体上分为目
录相关结构体以及APIs，文件相关结构体以及APIs。AVIOContext结构体通过持有URLContext，URLContext持有
URLProtocol，而URLProtocol持有访问特定协议的所有IO操作，从而AVIOContext获取了根据URL识别并访问资源的能力。

AVIOContext同时在内部提供了一个缓冲区，提供了一个对资源的带缓冲区的访问方式；同时，AVIOContext提供了direc
t字段用于表示是否使用该缓冲区，direct设置为1时，表示不使用AVIOContext缓冲区，而直接调用底层的协议访问函数，
从而提供对资源不带缓冲的访问方式，此时，后续将要介绍的IO层 Public APIs的avio_read()读，avio_write()写函数
将不使用AVIOContext缓冲，而avio_seek()跳转函数直接调用更底层的seek函数。

总之：FFMPEG IO层的目标就是提供一个统一的，类似于C语言 "标准IO库" 的接口来访问音视频资源。结构体AVIOContext
可以类比 标准IO库中的 文件指针对象FILE。而操作AVIOContext的函数avio_open()，avio_read()，avio_write()，
avio_seek()，avio_tell()，avio_flush()，avio_close()等在概念上与标准IO库中的fopen()，fread()，fwrite()，
fseek()，ftell()，fflush()，fclose()等是一一对应的。
```

<img src="./image/6-18.png" style="zoom:100%" />

```c++
1 目录相关APIs
打开目录：
int avio_open_dir(AVIODirContext **s, const char *url, AVDictionary **options);

读取目录：
int avio_read_dir(AVIODirContext *s, AVIODirEntry **next);

关闭目录：
int avio_close_dir(AVIODirContext **s);

2 文件相关APIs
打开文件：
int avio_open(AVIOContext **s, const char *url, int flags);
int avio_open2(AVIOContext **s, const char *url, int flags,const AVIOInterruptCB *int_cb, AVDictionary **options);

读文件：
1）按数据块读：
int avio_read(AVIOContext *s, unsigned char *buf, int size);
int avio_read_partial(AVIOContext *s, unsigned char *buf, int size);
2）按特定数据类型读：
int          avio_r8  (AVIOContext *s);
int avio_get_str16be(AVIOContext *pb, int maxlen, char *buf, int buflen);

写文件：
1）按数据块写：
void avio_write(AVIOContext *s, const unsigned char *buf, int size);
2）按特定数据类型写：
void avio_w8(AVIOContext *s, int b);
int avio_put_str16be(AVIOContext *s, const char *str);
void avio_write_marker(AVIOContext *s, int64_t time, enum AVIODataMarkerType type);

文件尾：
int avio_feof(AVIOContext *s);

冲刷缓冲区：
void avio_flush(AVIOContext *s);

seek相关：
int64_t avio_seek(AVIOContext *s, int64_t offset, int whence);
int64_t avio_skip(AVIOContext *s, int64_t offset);
static av_always_inline int64_t avio_tell(AVIOContext *s)；
int64_t avio_size(AVIOContext *s);

关闭文件：
int avio_close(AVIOContext *s);
int avio_closep(AVIOContext **s);

上述APIs族提供了一套类似于C语言标准I/O库的函数接口，区别在于标准I/O库操作的表征文件的结构体对象是FILE，而FFMPEG中表征文件的结构体对象是AVIOContext，因此，可以将二者类比理解。

avio_open2()声明：相比avio_open()多了一个协议层的终端回调函数AVIOInterruptCB和协议层私有选项，当使用avio_open()时，这两个参数传入为空。
```

## <a id="6.4">FFmpeg使用注意点</a>
- [1.ffmpeg open函数打开rtsp地址失败](#6.4.1)
- [2.ffmpeg推流失败](#6.4.2)
- [3.ffmpeg降低延时](#6.4.3)
- [4.ffmpeg打开视频流卡住问题](#6.4.4)
- [5.ffmpeg丢包](#6.4.5)
- [6.源本身音视频不同步且无法播放](#6.4.6)
- [7.源本身的时间戳问题，但源可正常播放](#6.4.7)
- [8.一段时间内，只丢失一个流的数据](#6.4.8)
- [9.源的部分视频帧未设置时间戳](#6.4.9)
- [10."dts < pcr"](#6.4.10)
- [11.循环源与非单调时间戳](#6.4.11)
- [12.长时间转码后出现轻微音视频不同步](#6.4.12)
- [13.ffmpeg处理MP4](#6.4.13)



### <a id="6.4.1">1.ffmpeg open函数打开rtsp地址失败</a>
ffmpeg open函数打开rtsp地址失败，导致获取视频信息失败。可能是采用了rtsp tcp方式需要采用tcp方式。

### <a id="6.4.2">2.ffmpeg推流失败</a>
1.ffmpeg推流报错403可能是需要注意后rtsp地址不加引号。  
2.推流地址可能要写127.0.0.1   

### <a id="6.4.3">3.ffmpeg降低延时</a>
```c++
1.ffmpeg推流降低延迟的优化
AVCodecContext *vc = NULL;
// AV_CODEC_FLAG_GLOBAL_HEADER  -- 将全局头文件放在引渡文件中，而不是每个关键帧中。
//AV_CODEC_FLAG_LOW_DELAY  --较低延迟
vc->flags |= AV_CODEC_FLAG_GLOBAL_HEADER | AV_CODEC_FLAG_LOW_DELAY;

//实时推流，零延迟
av_opt_set(vc->priv_data, "tune", "zerolatency", 0);

//一组图片的数量
vc->gop_size = 2;

//去掉B帧只留下 I帧和P帧
vc->max_b_frames = 0;

2.ffmpeg直播流的转码延时
设置zerolatency和ultrafast参数
./ffmpeg -i rtmp://192.168.1.12/live/src -tune zerolatency -vcodec libx264 -preset ultrafast -b:v 400k -s 720x576 -r 25 -acodec libfaac -b:a 64k -f flv rtmp://192.168.1.12/live/dst

--preset 的参数主要调节编码速度和质量的平衡，有ultrafast、superfast、veryfast、faster、fast、medium、slow、slower、veryslow、placebo这10个选项，从快到慢。 

--tune 的参数主要配合视频类型和视觉优化的参数，或特别的情况。如果视频的内容符合其中一个可用的调整值又或者有其中需要，则可以使用此选项，否则建议不使用（如tune grain是为高比特率的编码而设计的）。
tune的值有： 
film：  电影、真人类型； 
animation：  动画； 
grain：      需要保留大量的grain时用； 
stillimage：  静态图像编码时使用； 
psnr：      为提高psnr做了优化的参数； 
ssim：      为提高ssim做了优化的参数； 
fastdecode： 可以快速解码的参数； 
zerolatency：零延迟，用在需要非常低的延迟的情况下，比如电视电话会议的编码。

ffmpeg设置转码延时的参数和步骤如下:
. 关闭sync-lookahead
. 降低rc-lookahead，但别小于10,默认是-1
. 降低threads(比如从12降到6)
. 禁用rc-lookahead
. 禁用b-frames
. 缩小GOP， 
. 开启x264的 -preset fast/faster/verfast/superfast/ultrafast参数
. 使用-tune zerolatency 参数

3.配置参数
pFormatCtx->probesize = 20000000;
pFormatCtx->max_analyze_duration = 2000;
//    pFormatCtx->interrupt_callback.callback = timout_callback;
//    pFormatCtx->interrupt_callback.opaque = pFormatCtx;
//    pFormatCtx->flags |= AVFMT_FLAG_NONBLOCK;

 av_dict_set(&options, "fflags", "nobuffer", 0);//降低延时
//    av_dict_set(&options, "max_delay", "100000", 0);//最大延时
//    av_dict_set(&options, "rtmp_transport", "tcp", 0);
//    av_dict_set(&options, "stimeout", "6", 0);

CvSize imagesize;
imagesize.width = pCodecCtx->width;
imagesize.height = pCodecCtx->height;
IplImage *image = cvCreateImageHeader(imagesize, IPL_DEPTH_8U, 3);
cvSetData(image, out_buffer, imagesize.width * 3);

4.推音视频导致延时
在使用FFmpeg进行推流时，单独推视频和单独推音频都没有延迟，但是音视频一起推时，不发音频流数据会延迟10秒。经查问题原因为AVFormatContext的max_interleave_delta参数默认为10秒，在ff_interleave_packet_per_dts方法中会检查所有流是否都有数据，如果没有数据会默认等待10秒，因此造成推流延迟。ff_interleave_packet_per_dts 方法 源码在libavformat/mux.c  

删除视频中不需要的音频流
ffmpeg  -i gf.mkv  -map 0:0 -map 0:2  -vcodec copy -acodec copy out.mkv
注： -map指定哪些流做为输入， 0:0 表示第0个输入文件的第0个流
```


### <a id="6.4.4">4.ffmpeg打开视频流卡住问题</a>
```c++
1.打开rtsp可以尝试需要设置tcp
AVDictionary* options = NULL;
av_dict_set(&options, "rtsp_transport", "tcp", 0);
ret = avformat_open_input(&formatCtx, url, NULL, &options);
```


### <a id="6.4.5">5.ffmpeg丢包</a>
如果长时间不调用av_read_frame(例如在读取文件时，如果自己建立的队列A已满， 则不再调用av_read_frame，而是使本线程休眠若干毫秒，从队列A取出数据包进行解码），那么不断到来的 UDP数据包就会将缓冲区B写爆，之后再调用av_read_frame就会返回-5，代表IO错误。

因为UDP组播一般是与实时源（Live Source）相关联，因此过时的数据包是没有用的，因此当队列A写满时，采取从队列A弹出若干个数据包，之后再进行av_read_frame操作。这 样的代价是在播放过程中会产生丢包（视频中出现马赛克、花屏），但是延时会控制在一个合理的范围内；也可以直接丢弃av_read_frame得到的数据 包，这样视频中的马赛克较少，但是这样会产生无法估计的延时。

另外，在同时读取多路组播数据包时，也会遇到与缓冲区B相关的问题：av_find_stream_info会在视频流中寻找并解析PPS、SPS，是非 常耗时的。在做好第一路的初始化之后，就应该立即新建一个线程用于调用av_read_frame函数获取数据（否则，第一路的缓冲区B会被写爆）；与此 同时，第二路的初始化工作也将开始；由于此时有一个读取第一路UDP数据包的线程来争夺CPU时间，因此第二路的av_find_stream_info 将会花费更多的时间，这将导致第二路的缓冲区B被写爆。

上述问题的解决思路有两个，第一个是同时初始化两路UDP连接，但是在我的程序里由于有一些共享资源的限制，线程同步比较麻烦，所以后来放弃了；第二个是 尽量减少av_find_stream_info的时间或者加大缓冲区B的大小。看了AVFormatContext的属性，经过试验发现，减小 probesize和max_analyze_duration可以减少av_find_stream_info的时间，上面两个属性也不可以减少的太多，而要依据视频源的SPS和PPS的特点来确定，否则会找不到stream_info，也就没有分辨率等信息。增大缓冲区B的办法在udp://127.0.0.1：8080后面加?overrun_nonfatal=1&fifo_size=50000000即，ffmpeg在接收网络协议流时可以在链接上通过fifo_size进行缓冲大小的设置


### <a id="6.4.6">6.源本身音视频不同步且无法播放</a>
这种情况极为罕见，在A客户东方卫视频道转码时遇到过。表现为输出的音视频严重不匹配，录制下来的视频源无法播放，要么只有声音没有图像，要么只有图像没有声音，且图像播放卡顿。

解决方案：转码器无法修复这种异常源，需要反馈给源提供方修复。

### <a id="6.4.7">7.源本身的时间戳问题，但源可正常播放</a>
这种情况较少见，在M客户湖南卫视直播源时出现过。表现为转码出来的视频图像播放正常，但是没有声音。录制下来的视频源能正常播放。

分析方法：打印出输入AVPacket包的时间戳，发现音频包的PTS正常，而DTS长时间不变。

解决方法：读出音频数据包后，如果判断DTS != PTS，则强制将DTS设为PTS的值。

### <a id="6.4.8">8.一段时间内，只丢失一个流的数据</a>
在M客户的中国气象频道转码过程中遇到。

表现：输入源有数据持续输入的情况下，转码有一段时间没有输出，且恢复输出后播放没有声音。

分析方法：录制出问题时间段的视频源，分析其中的音视频数据包及时间戳。分析发现在一段时间之内，大概几秒到十几秒，输入的音频流丢失而视频流正常，导致在输出交织时一直等不到音频流而不断地buffer数据，直到音频流恢复后，或者等待超时后才有数据输出，而输入音频流恢复后，由于FFMPEG中执行了如下代码调整了原本正常的音频时间戳，使得音频时间戳错乱，导致播放时只有画面而没有声音。

<img src="./image/6-11.png" style="zoom:150%" />

解决方法：本段代码在已知场景中用处不大，可以注销掉；也可以将dts_delta_threshold阈值调至一个较大值(默认为10秒，可以调至100秒等等)，使之不满足阈值而不做时间戳调整。另外，为了防止长时间缺失一个流时没有输出，以及防止buffer溢出，参数max_interleave_delta值不能设置过大。



### <a id="6.4.9">9.源的部分视频帧未设置时间戳</a>
在一些客户的直播转码过程中，输入输出帧率相同的情况下，会遇到log中大量打印“Past duration XXX too large”且伴有frame drop相关信息的打印。

分析方法：将输入视频包的时间戳信息打印出来，发现一些视频帧未设置DTS/PTS，这些视频帧携带的时间戳都是默认值为AV_NOPTS_VALUE，代码中对该值的定义如下：

#define AV_NOPTS_VALUE         ((int64_t)UINT64_C(0x8000000000000000))

而这些帧在编码输出之前会被Drop掉，为了满足设定的帧率，后面的有效时间戳的帧会提前输出，导致视频提前而音频滞后。

解决方法：由于我们为这些未设置时间戳的输入帧填充有效时间戳不是一件很容易的事(需要结合输入视频是否采用B帧等)，因此在输入输出帧率相同时，我们可以将这些携带无效时间戳的帧正常送至编码器编码并输出，而不是丢弃。

### <a id="6.4.10">10."dts < pcr"</a>
在线CBR转码时常遇到"dts < pcr, TS is invalid"的打印，时间久了也可能引起输出码流的音视频失步。先我们先看看代码中PCR的计算，

<img src="./image/6-12.png" style="zoom:150%" />

可见PCR的值与muxrate和写入到AVIOContext中的数据量有关系。

在保证封装码率稳定的前提下，我们需要关注写入到AVIOContext中的音视频数据的速度。一般音频编码的速率较为稳定，而视频编码速度受B帧策略的影响较大，当视频编码使用较多的B帧时，由于是双向预测，导致编码速度慢于时间戳相近的音频帧，而当视频帧编码完成并输出时，AVIOContext中已经填充了较多的音频帧，使得PCR值超过了当前输出的视频帧的DTS，即出现了“dts < pcr”的打印。

解决方法: 在线转码时，在对视频质量没有特别要求的前提下，优先保证编码速度，就x264编码器来说，可以使用zerolatency策略，禁掉B帧的使用并尽量降低编码延迟。

### <a id="6.4.11">11.循环源与非单调时间戳</a>
在M客户电竞直播与H客户云转码项目中都遇到了这种循环源与非单调时间戳问题。FFMPEG默认只支持单调时间戳，一旦遇到非单调时间戳，FFMPEG会强制修改输出时间戳，代码如下，

<img src="./image/6-13.png" style="zoom:120" />

但修改后的时间戳是不正常的，导致输出视频播放异常。具体表现为，若为音视频透传，则输出的音视频均异常；若转码，输出的视频正常，而音频异常，这是因为视频编码器会根据帧率和输出的帧数重新计算时间戳，而音频时间戳则是继承了输入时间戳，从而输出的视频时间戳为单调递增的正常值，而音频时间戳为FFMPEG强制修改后的值，导致播放没有声音。

解决方法：

包括三个方面，

1.  将以上截图部分对时间戳的修改注销；

2. 取消由于非单调时间戳导致的转码退出：

<img src="./image/6-14.png" style="zoom:120" />

3. 需要转码时，由于视频转码输出时间戳为单调递增，为了使音频时间戳与视频时间戳同步，需要将音频时间戳按照单调递增的顺序来说出，算法设计时需要考虑时间戳跳变、丢帧等各种情况的处理。

### <a id="6.4.12">12.长时间转码后出现轻微音视频不同步</a>
这种音视频不同步暂时没有找到根本原因，多见于网络不稳定时，对音视频丢帧数量统计不精确导致网络恢复后时间戳出现轻微不对齐，且随着转码时间的延长或者网络波动情况的增多，不同步程度会加重。由于暂时无法确定具体原因，且一般在转码几天之后才出现，因此针对该情况的处理方案是，设计一套音视频不同步检测算法来评估音视频不同步的程度，当达到一定的阈值后，通知前端重启转码器以达到重新同步。


### <a id="6.4.13">13.ffmpeg处理MP4flv</a>
参考链接：   
https://blog.csdn.net/Lyman_Ye/article/details/80261312   

```c++
moov在madt写完以后再写入，所以它出现在容器的最后面。通过faststart可以把moov移到mdat的前面：注意要从一个flv文件转换为mp4文件让它重新生成Box数据
由于我只准备了一个mp4文件，所以先通过下面的命令把我的mp4文件转换为flv文件再来验证这个参数：
ffmpeg -i ~/Movies/i_am_you.mp4 -vcodec libx264 -preset fast -profile baseline -x264opts bitrate=300 -s 320x240 -r 12 -g 12 -f flv ~/Movies/output.flv

下面可以验证我们的faststart参数的效果了
ffmpeg -i output.flv -c copy -f mp4 -movflags faststart output.mp4

使用DASH格式的时候，生成的MP4文件的Box信息和之前的不同，它主要包含的容器为sidx,moof和mdat这三个Box
FFmpeg命令生成DASH格式的MP4文件：
ffmpeg -i input.flv -c copy -f mp4 -movflags dash output.mp4

在网络视频点播文件为FLV格式时，人们常用yamdi工具对FLV文件进行一次转换，主要是将FLV文件中的关键帧建立一个索引，并将索引写入Metadata中，使用FFmpeg也可以实现
ffmpeg -i input.mp4 -c copy -f flv -flvflags add_keyframe_index output.flv

FFmpeg中自带HLS的封装参数，使用HLS格式即可进行HLS的封装，但是生成HLS的时候有很多参数可以参考具体的参数使用的时候查看文档
转换文件命令：
ffmpeg -re -i input.mp4 -c copy -f hls -bsf:v h264_mp4toannexb output.m3u8

执行完这个命令之后 看到目录下面有一大片ts切片文件我上面的命令默认的每一个切片的时间是5秒，这个时间可以通过 -hls_time这个参数来设置
ffmpeg -re -i input.mp4 -c copy -f hls -bsf:v h264_mp4toannexb -hls_time 40 output.m3u8

我们可以把一个mp4文件来切成多个mp4文件，可以使用segment 方式来进行切片，也可以使用ss加上t参数进行切片。这里指定一个切片命令的参数-segment_format
ffmpeg -re -i input.mp4 -c copy -f segment -segment_format mp4 test_output-%d.mp4

ss进行视频文件的seek定位，ss传递的参数为时间值，t传递的参数也是时间值,使用ss指定剪切开头部分
ffmpeg -ss 10 -i input.mp4 -c copy output.ts

命令执行后生成的ouput.ts会比input.mp4视频少大概8秒，因为output.ts是从input.mp4的第八秒开始截取的。使用如下命令看duration信息
ffprobe -v quiet -show_format input.mp4 |grep duration;ffmpeg -v quiet -show_format output.ts |grep duration

使用output_ts_offset指定输出start_time,FFmpeg支持ss与t参数一同使用以达到切割视频的某一段的效果，但其不能指定输出文件的start_time，而且也不希望时间戳归0，可以使用output_ts_offset来达到指定输出文件的start_time的目的
ffmpeg -i input.mp4 -c copy -t 10 -output_ts_offset 120 output.mp4
```


## <a id="6.5">Window集成ffmpeg环境搭建</a>
### 1.编译准备  
#### 1.1安装MinGW	    
MinGW(Minimalist GNU on Windows)：一个可自由使用和自由发布的Windows特定头文件和使用GNU工具集导入库的集合，允许你生成本地的Windows程序而不需要第三方C运行时库。  
安装过程如下：   
- 1.下载MinGW，下载地：http://sourceforge.net/projects/mingw/    
- 2.下载完mingw-get-setup.exe。点击运行exe程序，安装后，运行MinGW installer，安装如下内容：   
<img src="./image/6-3.png" style="zoom:100%" />

- 3.在Installation菜单下点击Apply Changes，完成。点击mingw-w64\msys\1.0\msys.bat，即可在弹出的窗口中进行linux命令行操作，cd到ffmpeg的源码目录.  
- 4.然后执行ffmpeg下configure命令。在弹出的窗口下gcc -v可以看到gcc版本，如果无效，可以尝试配置环境变量解决，将msys.bat配置到环境变量中。  

#### 1.2 安装YASM
- 1.下载YASM源码，下载地址：http://yasm.tortall.net/Download.html
- 2.解压yasm源码包，用mingw进入到yams目录下，运行./configure --prefix=/c/window_ffmpeg/yasm-1.3.0/build，进行配置结果到yasm/build目录中
- 3.然后运行make和make install两个命令，编译完成。
- 4.配置bin目录到系统环境下，可以在mingw窗口下如下配置：export PATH="$PATH:/c/window_ffmpeg/yasm-1.3.0/build/bin"。即可配置到系统路径下。配置完，可用yasm --verion验证是否安装成功。

#### 1.3 pkg-config
- 1.下载pkg-config源码，下载地址：https://www.freedesktop.org/wiki/Software/pkg-config/
- 2.解压pkg-config源码包，用mingw进入到pkg-config目录下，运行./configure --prefix=/c/window_ffmpeg/pkg-config-0.29/build，进行配置结果到pkg-config-0.29/build目录中
- 3.然后运行make和make install两个命令，编译完成。
- 4.配置bin目录到系统环境下，可以在mingw窗口下如下配置：export PATH="$PATH:/c/window_ffmpeg/pkg-config-0.29/build/bin"。即可配置到系统路径下。

### 2.编译ffmpeg
- 1.下载ffmpeg源码
下载地址：http://ffmpeg.org/download.html。
解压源码包。
- 2.在mingw窗口下，cd到ffmpeg目录下，运行./configure --prefix=/c/window_ffmpeg/ffmpeg/build --enable-shared  --enable-static --enable-debug。enable-debug开启ffmpeg的debug。
- 3.运行make&make install，完成编译
- 4.编译结果在build目录下。

### 3.Window集成使用ffmpeg
- 3.1安装VS IDE开发环境
	安装Visual Studio 2017 IDE环境，下载地址：https://visualstudio.microsoft.com/zh-hans/downloads/。下载完进行安装。
- 3.2 VS环境配置
 - 1.VS环境配置include和lib目录，alt+f7进入到项目属性，在VC++目录->包含目录中设置ffmpeg的include目录。在VC++目录->库目录中设置ffmpeg的lib目录，配置lib/include路径。
 - 2.vs工程下debug放入ffmpeg的dll文件，完成ffmpeg集成。
- 3.3 ffmpeg程序运行
写完ffmpeg测试程序，F5运行，如果“出现函数被声明为否决”，需要做如下设置可以解决：
<img src="./image/6-4.png" style="zoom:100%" />

如果无问题，即可在输出窗口看到运行结果和ffmpeg的日志信息。Ffmpeg运行结果和调试信息如下：
<img src="./image/6-5.png" style="zoom:100%" />

>以上方式为自己编译可以直接安装



## <a id="6.6">FFmpeg应用</a>
- [1.优秀链接](#6.6.1)
- [2.雷神优秀链接更新](#6.6.2)
- [3.最简单的基于FFMPEG的封装格式转换器（无编解码）](#6.6.3)
- [4.最简单的基于FFmpeg的封装格式处理：视音频复用器（muxer）](#6.6.4)
- [5.最简单的基于FFmpeg的封装格式处理：视音频分离器（demuxer）](#6.6.5)
- [6.ffmpeg将YUV原始数据编码封装为mp4格式](#6.6.6)
- [7.FFmpeg流媒体处理-收流与推流](#6.6.7)
- [8.基于ffmpeg的内存数据UDP直播推流](#6.6.8)
- [9.FFmpeg内存H264流发布rtmp](#6.6.9)




### <a id="6.6.1">1.优秀链接</a>
1.使用FFmpeg进行RTP接受解析（H264、TS）    
https://blog.csdn.net/zhoubotong2012/article/details/86711097   

2.使用FFMPEG+H264实现RTP传输数据      
https://blog.csdn.net/czc1009/article/details/12913759    

### <a id="6.6.2">2.雷神优秀链接更新</a>
```c++
1.最简单的基于FFmpeg的内存读写的例子：内存播放器    
https://blog.csdn.net/leixiaohua1020/article/details/39759163    

2.最简单的基于FFMPEG的封装格式转换器（无编解码）     
https://blog.csdn.net/leixiaohua1020/article/details/25422685   

3.最简单的基于FFmpeg的封装格式处理：视音频复用器（muxer）     
https://blog.csdn.net/leixiaohua1020/article/details/39802913 

4.最简单的基于FFmpeg的封装格式处理：视音频分离器（demuxer）
https://blog.csdn.net/leixiaohua1020/article/details/39802819

5.最简单的基于FFmpeg的AVUtil例子 (AVLog, AVOption等)   
https://blog.csdn.net/leixiaohua1020/article/details/46890739

6.FFmpeg源代码结构图 - 解码
https://blog.csdn.net/leixiaohua1020/article/details/44220151

7.FFmpeg源代码结构图 - 编码
https://blog.csdn.net/leixiaohua1020/article/details/44226355

8.live555源代码分析
https://blog.csdn.net/leixiaohua1020/article/details/12235615
```


### <a id="6.6.3">3.最简单的基于FFMPEG的封装格式转换器（无编解码）</a>
 
<img src="./image/6-14.jfif" style="zoom:100%" />

```c++
输入文件操作：
avformat_open_input()：打开输入文件，初始化输入视频码流的AVFormatContext。
av_read_frame()：从输入文件中读取一个AVPacket。
av_dump_format():打印关于输入或输出格式的详细信息,例如持续时间等

输出文件操作：
avformat_alloc_output_context2()：初始化输出视频码流的AVFormatContext。
avformat_new_stream()：创建输出码流的AVStream。
avcodec_copy_context()：拷贝输入视频码流的AVCodecContex的数值t到输出视频的AVCodecContext。
avio_open()：打开输出文件。
avformat_write_header()：写文件头（对于某些没有文件头的封装格式，不需要此函数。比如说MPEG2TS）。
av_interleaved_write_frame()：将AVPacket（存储视频压缩码流数据）写入文件。
av_write_trailer()：写文件尾（对于某些没有文件头的封装格式，不需要此函数。比如说MPEG2TS）。
avformat_new_stream()：创建一个流，可以根据输入的codec创建流不涉及转换的话。
av_compare_ts()：比较时间戳，决定写入视频还是写入音频。这个函数相对要少见一些。


/*
 *最简单的基于FFmpeg的封装格式转换器
 *Simplest FFmpeg Remuxer
 *
 *雷霄骅 Lei Xiaohua
 *leixiaohua1020@126.com
 *中国传媒大学/数字电视技术
 *Communication University of China / Digital TV Technology
 *http://blog.csdn.net/leixiaohua1020
 *
 *本程序实现了视频封装格式之间的转换。
 *需要注意的是本程序并不改变视音频的编码格式。
 *
 * This software converts a media file from one container format
 * to another container format without encoding/decoding video files.
 */
#include "stdafx.h"
 
extern "C"
{
#include "libavformat/avformat.h"
};
 
int _tmain(int argc, _TCHAR* argv[])
{
    AVOutputFormat *ofmt = NULL;
    //输入对应一个AVFormatContext，输出对应一个AVFormatContext
    //（Input AVFormatContext and Output AVFormatContext）
    AVFormatContext *ifmt_ctx = NULL, *ofmt_ctx = NULL;
    AVPacket pkt;
    const char *in_filename, *out_filename;
    int ret, i;
    if (argc < 3) {
        printf("usage: %s input output\n"
            "Remux a media file with libavformat and libavcodec.\n"
            "The output format is guessed according to the file extension.\n"
            "Modified by Lei Xiaohua, leixiaohua1020@126.com\n"
            "Communication University of China / Digital TV Technology\n"
            "http://blog.csdn.net/leixiaohua1020", argv[0]);
        return 1;
    }
    in_filename  = argv[1];//输入文件名（Input file URL）
    out_filename = argv[2];//输出文件名（Output file URL）
    av_register_all();
    //输入（Input）
    if ((ret = avformat_open_input(&ifmt_ctx, in_filename, 0, 0)) < 0) {
        printf( "Could not open input file.");
        goto end;
    }
    if ((ret = avformat_find_stream_info(ifmt_ctx, 0)) < 0) {
        printf( "Failed to retrieve input stream information");
        goto end;
    }
    av_dump_format(ifmt_ctx, 0, in_filename, 0);
    //输出（Output）
    avformat_alloc_output_context2(&ofmt_ctx, NULL, NULL, out_filename);
    if (!ofmt_ctx) {
        printf( "Could not create output context\n");
        ret = AVERROR_UNKNOWN;
        goto end;
    }
    ofmt = ofmt_ctx->oformat;
    for (i = 0; i < ifmt_ctx->nb_streams; i++) {
        //根据输入流创建输出流（Create output AVStream according to input AVStream）
        AVStream *in_stream = ifmt_ctx->streams[i];
        AVStream *out_stream = avformat_new_stream(ofmt_ctx, in_stream->codec->codec);
        if (!out_stream) {
            printf( "Failed allocating output stream\n");
            ret = AVERROR_UNKNOWN;
            goto end;
        }
        //复制AVCodecContext的设置（Copy the settings of AVCodecContext）
        ret = avcodec_copy_context(out_stream->codec, in_stream->codec);
        if (ret < 0) {
            printf( "Failed to copy context from input to output stream codec context\n");
            goto end;
        }
        out_stream->codec->codec_tag = 0;
        if (ofmt_ctx->oformat->flags & AVFMT_GLOBALHEADER)
            out_stream->codec->flags |= CODEC_FLAG_GLOBAL_HEADER;
    }
    //输出一下格式------------------
    av_dump_format(ofmt_ctx, 0, out_filename, 1);
    //打开输出文件（Open output file）
    if (!(ofmt->flags & AVFMT_NOFILE)) {
        ret = avio_open(&ofmt_ctx->pb, out_filename, AVIO_FLAG_WRITE);
        if (ret < 0) {
            printf( "Could not open output file '%s'", out_filename);
            goto end;
        }
    }
    //写文件头（Write file header）
    ret = avformat_write_header(ofmt_ctx, NULL);
    if (ret < 0) {
        printf( "Error occurred when opening output file\n");
        goto end;
    }
    int frame_index=0;
    while (1) {
        AVStream *in_stream, *out_stream;
        //获取一个AVPacket（Get an AVPacket）
        ret = av_read_frame(ifmt_ctx, &pkt);
        if (ret < 0)
            break;
        in_stream  = ifmt_ctx->streams[pkt.stream_index];
        out_stream = ofmt_ctx->streams[pkt.stream_index];
        /* copy packet */
        //转换PTS/DTS（Convert PTS/DTS）
        pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream->time_base, out_stream->time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX));
        pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream->time_base, out_stream->time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX));
        pkt.duration = av_rescale_q(pkt.duration, in_stream->time_base, out_stream->time_base);
        pkt.pos = -1;
        //写入（Write）
        ret = av_interleaved_write_frame(ofmt_ctx, &pkt);
        if (ret < 0) {
            printf( "Error muxing packet\n");
            break;
        }
        printf("Write %8d frames to output file\n",frame_index);
        av_free_packet(&pkt);
        frame_index++;
    }
    //写文件尾（Write file trailer）
    av_write_trailer(ofmt_ctx);
end:
    avformat_close_input(&ifmt_ctx);
    /* close output */
    if (ofmt_ctx && !(ofmt->flags & AVFMT_NOFILE))
        avio_close(ofmt_ctx->pb);
    avformat_free_context(ofmt_ctx);
    if (ret < 0 && ret != AVERROR_EOF) {
        printf( "Error occurred.\n");
        return -1;
    }
    return 0;
}
```

### <a id="6.6.4">4.最简单的基于FFmpeg的封装格式处理：视音频复用器（muxer）</a>

<img src="./image/6-15.jfif" style="zoom:100%" />

```c++
/**
 * 最简单的基于FFmpeg的视音频复用器
 * Simplest FFmpeg Muxer
 *
 * 雷霄骅 Lei Xiaohua
 * leixiaohua1020@126.com
 * 中国传媒大学/数字电视技术
 * Communication University of China / Digital TV Technology
 * http://blog.csdn.net/leixiaohua1020
 *
 * 本程序可以将视频码流和音频码流打包到一种封装格式中。
 * 程序中将AAC编码的音频码流和H.264编码的视频码流打包成
 * MPEG2TS封装格式的文件。
 * 需要注意的是本程序并不改变视音频的编码格式。
 *
 * This software mux a video bitstream and a audio bitstream 
 * together into a file.
 * In this example, it mux a H.264 bitstream (in MPEG2TS) and 
 * a AAC bitstream file together into MP4 format file.
 *
 */
 
#include <stdio.h>
 
#define __STDC_CONSTANT_MACROS
 
#ifdef _WIN32
//Windows
extern "C"
{
#include "libavformat/avformat.h"
};
#else
//Linux...
#ifdef __cplusplus
extern "C"
{
#endif
#include <libavformat/avformat.h>
#ifdef __cplusplus
};
#endif
#endif
 
/*
FIX: H.264 in some container format (FLV, MP4, MKV etc.) need 
"h264_mp4toannexb" bitstream filter (BSF)
  *Add SPS,PPS in front of IDR frame
  *Add start code ("0,0,0,1") in front of NALU
H.264 in some container (MPEG2TS) don't need this BSF.
*/
//'1': Use H.264 Bitstream Filter 
#define USE_H264BSF 0
 
/*
FIX:AAC in some container format (FLV, MP4, MKV etc.) need 
"aac_adtstoasc" bitstream filter (BSF)
*/
//'1': Use AAC Bitstream Filter 
#define USE_AACBSF 0
 
 
 
int main(int argc, char* argv[])
{
	AVOutputFormat *ofmt = NULL;
	//Input AVFormatContext and Output AVFormatContext
	AVFormatContext *ifmt_ctx_v = NULL, *ifmt_ctx_a = NULL,*ofmt_ctx = NULL;
	AVPacket pkt;
	int ret, i;
	int videoindex_v=-1,videoindex_out=-1;
	int audioindex_a=-1,audioindex_out=-1;
	int frame_index=0;
	int64_t cur_pts_v=0,cur_pts_a=0;
 
	//const char *in_filename_v = "cuc_ieschool.ts";//Input file URL
	const char *in_filename_v = "cuc_ieschool.h264";
	//const char *in_filename_a = "cuc_ieschool.mp3";
	//const char *in_filename_a = "gowest.m4a";
	//const char *in_filename_a = "gowest.aac";
	const char *in_filename_a = "huoyuanjia.mp3";
 
	const char *out_filename = "cuc_ieschool.mp4";//Output file URL
	av_register_all();
	//Input
	if ((ret = avformat_open_input(&ifmt_ctx_v, in_filename_v, 0, 0)) < 0) {
		printf( "Could not open input file.");
		goto end;
	}
	if ((ret = avformat_find_stream_info(ifmt_ctx_v, 0)) < 0) {
		printf( "Failed to retrieve input stream information");
		goto end;
	}
 
	if ((ret = avformat_open_input(&ifmt_ctx_a, in_filename_a, 0, 0)) < 0) {
		printf( "Could not open input file.");
		goto end;
	}
	if ((ret = avformat_find_stream_info(ifmt_ctx_a, 0)) < 0) {
		printf( "Failed to retrieve input stream information");
		goto end;
	}
	printf("===========Input Information==========\n");
	av_dump_format(ifmt_ctx_v, 0, in_filename_v, 0);
	av_dump_format(ifmt_ctx_a, 0, in_filename_a, 0);
	printf("======================================\n");
	//Output
	avformat_alloc_output_context2(&ofmt_ctx, NULL, NULL, out_filename);
	if (!ofmt_ctx) {
		printf( "Could not create output context\n");
		ret = AVERROR_UNKNOWN;
		goto end;
	}
	ofmt = ofmt_ctx->oformat;
 
	for (i = 0; i < ifmt_ctx_v->nb_streams; i++) {
		//Create output AVStream according to input AVStream
		if(ifmt_ctx_v->streams[i]->codec->codec_type==AVMEDIA_TYPE_VIDEO){
		AVStream *in_stream = ifmt_ctx_v->streams[i];
		AVStream *out_stream = avformat_new_stream(ofmt_ctx, in_stream->codec->codec);
		videoindex_v=i;
		if (!out_stream) {
			printf( "Failed allocating output stream\n");
			ret = AVERROR_UNKNOWN;
			goto end;
		}
		videoindex_out=out_stream->index;
		//Copy the settings of AVCodecContext
		if (avcodec_copy_context(out_stream->codec, in_stream->codec) < 0) {
			printf( "Failed to copy context from input to output stream codec context\n");
			goto end;
		}
		out_stream->codec->codec_tag = 0;
		if (ofmt_ctx->oformat->flags & AVFMT_GLOBALHEADER)
			out_stream->codec->flags |= CODEC_FLAG_GLOBAL_HEADER;
		break;
		}
	}
 
	for (i = 0; i < ifmt_ctx_a->nb_streams; i++) {
		//Create output AVStream according to input AVStream
		if(ifmt_ctx_a->streams[i]->codec->codec_type==AVMEDIA_TYPE_AUDIO){
			AVStream *in_stream = ifmt_ctx_a->streams[i];
			AVStream *out_stream = avformat_new_stream(ofmt_ctx, in_stream->codec->codec);
			audioindex_a=i;
			if (!out_stream) {
				printf( "Failed allocating output stream\n");
				ret = AVERROR_UNKNOWN;
				goto end;
			}
			audioindex_out=out_stream->index;
			//Copy the settings of AVCodecContext
			if (avcodec_copy_context(out_stream->codec, in_stream->codec) < 0) {
				printf( "Failed to copy context from input to output stream codec context\n");
				goto end;
			}
			out_stream->codec->codec_tag = 0;
			if (ofmt_ctx->oformat->flags & AVFMT_GLOBALHEADER)
				out_stream->codec->flags |= CODEC_FLAG_GLOBAL_HEADER;
 
			break;
		}
	}
 
	printf("==========Output Information==========\n");
	av_dump_format(ofmt_ctx, 0, out_filename, 1);
	printf("======================================\n");
	//Open output file
	if (!(ofmt->flags & AVFMT_NOFILE)) {
		if (avio_open(&ofmt_ctx->pb, out_filename, AVIO_FLAG_WRITE) < 0) {
			printf( "Could not open output file '%s'", out_filename);
			goto end;
		}
	}
	//Write file header
	if (avformat_write_header(ofmt_ctx, NULL) < 0) {
		printf( "Error occurred when opening output file\n");
		goto end;
	}
 
 
	//FIX
#if USE_H264BSF
	AVBitStreamFilterContext* h264bsfc =  av_bitstream_filter_init("h264_mp4toannexb"); 
#endif
#if USE_AACBSF
	AVBitStreamFilterContext* aacbsfc =  av_bitstream_filter_init("aac_adtstoasc"); 
#endif
 
	while (1) {
		AVFormatContext *ifmt_ctx;
		int stream_index=0;
		AVStream *in_stream, *out_stream;
 
		//Get an AVPacket
		if(av_compare_ts(cur_pts_v,ifmt_ctx_v->streams[videoindex_v]->time_base,cur_pts_a,ifmt_ctx_a->streams[audioindex_a]->time_base) <= 0){
			ifmt_ctx=ifmt_ctx_v;
			stream_index=videoindex_out;
 
			if(av_read_frame(ifmt_ctx, &pkt) >= 0){
				do{
					in_stream  = ifmt_ctx->streams[pkt.stream_index];
					out_stream = ofmt_ctx->streams[stream_index];
 
					if(pkt.stream_index==videoindex_v){
						//FIX：No PTS (Example: Raw H.264)
						//Simple Write PTS
						if(pkt.pts==AV_NOPTS_VALUE){
							//Write PTS
							AVRational time_base1=in_stream->time_base;
							//Duration between 2 frames (us)
							int64_t calc_duration=(double)AV_TIME_BASE/av_q2d(in_stream->r_frame_rate);
							//Parameters
							pkt.pts=(double)(frame_index*calc_duration)/(double)(av_q2d(time_base1)*AV_TIME_BASE);
							pkt.dts=pkt.pts;
							pkt.duration=(double)calc_duration/(double)(av_q2d(time_base1)*AV_TIME_BASE);
							frame_index++;
						}
 
						cur_pts_v=pkt.pts;
						break;
					}
				}while(av_read_frame(ifmt_ctx, &pkt) >= 0);
			}else{
				break;
			}
		}else{
			ifmt_ctx=ifmt_ctx_a;
			stream_index=audioindex_out;
			if(av_read_frame(ifmt_ctx, &pkt) >= 0){
				do{
					in_stream  = ifmt_ctx->streams[pkt.stream_index];
					out_stream = ofmt_ctx->streams[stream_index];
 
					if(pkt.stream_index==audioindex_a){
 
						//FIX：No PTS
						//Simple Write PTS
						if(pkt.pts==AV_NOPTS_VALUE){
							//Write PTS
							AVRational time_base1=in_stream->time_base;
							//Duration between 2 frames (us)
							int64_t calc_duration=(double)AV_TIME_BASE/av_q2d(in_stream->r_frame_rate);
							//Parameters
							pkt.pts=(double)(frame_index*calc_duration)/(double)(av_q2d(time_base1)*AV_TIME_BASE);
							pkt.dts=pkt.pts;
							pkt.duration=(double)calc_duration/(double)(av_q2d(time_base1)*AV_TIME_BASE);
							frame_index++;
						}
						cur_pts_a=pkt.pts;
 
						break;
					}
				}while(av_read_frame(ifmt_ctx, &pkt) >= 0);
			}else{
				break;
			}
 
		}
 
		//FIX:Bitstream Filter
#if USE_H264BSF
		av_bitstream_filter_filter(h264bsfc, in_stream->codec, NULL, &pkt.data, &pkt.size, pkt.data, pkt.size, 0);
#endif
#if USE_AACBSF
		av_bitstream_filter_filter(aacbsfc, out_stream->codec, NULL, &pkt.data, &pkt.size, pkt.data, pkt.size, 0);
#endif
 
 
		//Convert PTS/DTS
		pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream->time_base, out_stream->time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX));
		pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream->time_base, out_stream->time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX));
		pkt.duration = av_rescale_q(pkt.duration, in_stream->time_base, out_stream->time_base);
		pkt.pos = -1;
		pkt.stream_index=stream_index;
 
		printf("Write 1 Packet. size:%5d\tpts:%lld\n",pkt.size,pkt.pts);
		//Write
		if (av_interleaved_write_frame(ofmt_ctx, &pkt) < 0) {
			printf( "Error muxing packet\n");
			break;
		}
		av_free_packet(&pkt);
 
	}
	//Write file trailer
	av_write_trailer(ofmt_ctx);
 
#if USE_H264BSF
	av_bitstream_filter_close(h264bsfc);
#endif
#if USE_AACBSF
	av_bitstream_filter_close(aacbsfc);
#endif
 
end:
	avformat_close_input(&ifmt_ctx_v);
	avformat_close_input(&ifmt_ctx_a);
	/* close output */
	if (ofmt_ctx && !(ofmt->flags & AVFMT_NOFILE))
		avio_close(ofmt_ctx->pb);
	avformat_free_context(ofmt_ctx);
	if (ret < 0 && ret != AVERROR_EOF) {
		printf( "Error occurred.\n");
		return -1;
	}
	return 0;
}
```

### <a id="6.6.5">5.最简单的基于FFmpeg的封装格式处理：视音频分离器（demuxer）</a>

<img src="./image/6-16.jfif" style="zoom:100%" />

```c++

/**
 * 最简单的基于FFmpeg的视音频分离器
 * Simplest FFmpeg Demuxer
 *
 * 雷霄骅 Lei Xiaohua
 * leixiaohua1020@126.com
 * 中国传媒大学/数字电视技术
 * Communication University of China / Digital TV Technology
 * http://blog.csdn.net/leixiaohua1020
 *
 * 本程序可以将封装格式中的视频码流数据和音频码流数据分离出来。
 * 在该例子中， 将MPEG2TS的文件分离得到H.264视频码流文件和AAC
 * 音频码流文件。
 *
 * This software split a media file (in Container such as 
 * MKV, FLV, AVI...) to video and audio bitstream.
 * In this example, it demux a MPEG2TS file to H.264 bitstream
 * and AAC bitstream.
 */
 
#include <stdio.h>
 
#define __STDC_CONSTANT_MACROS
 
#ifdef _WIN32
//Windows
extern "C"
{
#include "libavformat/avformat.h"
};
#else
//Linux...
#ifdef __cplusplus
extern "C"
{
#endif
#include <libavformat/avformat.h>
#ifdef __cplusplus
};
#endif
#endif
 
/*
FIX: H.264 in some container format (FLV, MP4, MKV etc.) need 
"h264_mp4toannexb" bitstream filter (BSF)
  *Add SPS,PPS in front of IDR frame
  *Add start code ("0,0,0,1") in front of NALU
H.264 in some container (MPEG2TS) don't need this BSF.
*/
//'1': Use H.264 Bitstream Filter 
#define USE_H264BSF 0
 
int main(int argc, char* argv[])
{
	AVOutputFormat *ofmt_a = NULL,*ofmt_v = NULL;
	//（Input AVFormatContext and Output AVFormatContext）
	AVFormatContext *ifmt_ctx = NULL, *ofmt_ctx_a = NULL, *ofmt_ctx_v = NULL;
	AVPacket pkt;
	int ret, i;
	int videoindex=-1,audioindex=-1;
	int frame_index=0;
 
	const char *in_filename  = "cuc_ieschool.ts";//Input file URL
	//char *in_filename  = "cuc_ieschool.mkv";
	const char *out_filename_v = "cuc_ieschool.h264";//Output file URL
	//char *out_filename_a = "cuc_ieschool.mp3";
	const char *out_filename_a = "cuc_ieschool.aac";
 
	av_register_all();
	//Input
	if ((ret = avformat_open_input(&ifmt_ctx, in_filename, 0, 0)) < 0) {
		printf( "Could not open input file.");
		goto end;
	}
	if ((ret = avformat_find_stream_info(ifmt_ctx, 0)) < 0) {
		printf( "Failed to retrieve input stream information");
		goto end;
	}
 
	//Output
	avformat_alloc_output_context2(&ofmt_ctx_v, NULL, NULL, out_filename_v);
	if (!ofmt_ctx_v) {
		printf( "Could not create output context\n");
		ret = AVERROR_UNKNOWN;
		goto end;
	}
	ofmt_v = ofmt_ctx_v->oformat;
 
	avformat_alloc_output_context2(&ofmt_ctx_a, NULL, NULL, out_filename_a);
	if (!ofmt_ctx_a) {
		printf( "Could not create output context\n");
		ret = AVERROR_UNKNOWN;
		goto end;
	}
	ofmt_a = ofmt_ctx_a->oformat;
 
	for (i = 0; i < ifmt_ctx->nb_streams; i++) {
			//Create output AVStream according to input AVStream
			AVFormatContext *ofmt_ctx;
			AVStream *in_stream = ifmt_ctx->streams[i];
			AVStream *out_stream = NULL;
			
			if(ifmt_ctx->streams[i]->codec->codec_type==AVMEDIA_TYPE_VIDEO){
				videoindex=i;
				out_stream=avformat_new_stream(ofmt_ctx_v, in_stream->codec->codec);
				ofmt_ctx=ofmt_ctx_v;
			}else if(ifmt_ctx->streams[i]->codec->codec_type==AVMEDIA_TYPE_AUDIO){
				audioindex=i;
				out_stream=avformat_new_stream(ofmt_ctx_a, in_stream->codec->codec);
				ofmt_ctx=ofmt_ctx_a;
			}else{
				break;
			}
			
			if (!out_stream) {
				printf( "Failed allocating output stream\n");
				ret = AVERROR_UNKNOWN;
				goto end;
			}
			//Copy the settings of AVCodecContext
			if (avcodec_copy_context(out_stream->codec, in_stream->codec) < 0) {
				printf( "Failed to copy context from input to output stream codec context\n");
				goto end;
			}
			out_stream->codec->codec_tag = 0;
 
			if (ofmt_ctx->oformat->flags & AVFMT_GLOBALHEADER)
				out_stream->codec->flags |= CODEC_FLAG_GLOBAL_HEADER;
	}
 
	//Dump Format------------------
	printf("\n==============Input Video=============\n");
	av_dump_format(ifmt_ctx, 0, in_filename, 0);
	printf("\n==============Output Video============\n");
	av_dump_format(ofmt_ctx_v, 0, out_filename_v, 1);
	printf("\n==============Output Audio============\n");
	av_dump_format(ofmt_ctx_a, 0, out_filename_a, 1);
	printf("\n======================================\n");
	//Open output file
	if (!(ofmt_v->flags & AVFMT_NOFILE)) {
		if (avio_open(&ofmt_ctx_v->pb, out_filename_v, AVIO_FLAG_WRITE) < 0) {
			printf( "Could not open output file '%s'", out_filename_v);
			goto end;
		}
	}
 
	if (!(ofmt_a->flags & AVFMT_NOFILE)) {
		if (avio_open(&ofmt_ctx_a->pb, out_filename_a, AVIO_FLAG_WRITE) < 0) {
			printf( "Could not open output file '%s'", out_filename_a);
			goto end;
		}
	}
 
	//Write file header
	if (avformat_write_header(ofmt_ctx_v, NULL) < 0) {
		printf( "Error occurred when opening video output file\n");
		goto end;
	}
	if (avformat_write_header(ofmt_ctx_a, NULL) < 0) {
		printf( "Error occurred when opening audio output file\n");
		goto end;
	}
	
#if USE_H264BSF
	AVBitStreamFilterContext* h264bsfc =  av_bitstream_filter_init("h264_mp4toannexb"); 
#endif
 
	while (1) {
		AVFormatContext *ofmt_ctx;
		AVStream *in_stream, *out_stream;
		//Get an AVPacket
		if (av_read_frame(ifmt_ctx, &pkt) < 0)
			break;
		in_stream  = ifmt_ctx->streams[pkt.stream_index];
 
		
		if(pkt.stream_index==videoindex){
			out_stream = ofmt_ctx_v->streams[0];
			ofmt_ctx=ofmt_ctx_v;
			printf("Write Video Packet. size:%d\tpts:%lld\n",pkt.size,pkt.pts);
#if USE_H264BSF
			av_bitstream_filter_filter(h264bsfc, in_stream->codec, NULL, &pkt.data, &pkt.size, pkt.data, pkt.size, 0);
#endif
		}else if(pkt.stream_index==audioindex){
			out_stream = ofmt_ctx_a->streams[0];
			ofmt_ctx=ofmt_ctx_a;
			printf("Write Audio Packet. size:%d\tpts:%lld\n",pkt.size,pkt.pts);
		}else{
			continue;
		}
 
 
		//Convert PTS/DTS
		pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream->time_base, out_stream->time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX));
		pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream->time_base, out_stream->time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX));
		pkt.duration = av_rescale_q(pkt.duration, in_stream->time_base, out_stream->time_base);
		pkt.pos = -1;
		pkt.stream_index=0;
		//Write
		if (av_interleaved_write_frame(ofmt_ctx, &pkt) < 0) {
			printf( "Error muxing packet\n");
			break;
		}
		//printf("Write %8d frames to output file\n",frame_index);
		av_free_packet(&pkt);
		frame_index++;
	}
 
#if USE_H264BSF
	av_bitstream_filter_close(h264bsfc);  
#endif
 
	//Write file trailer
	av_write_trailer(ofmt_ctx_a);
	av_write_trailer(ofmt_ctx_v);
end:
	avformat_close_input(&ifmt_ctx);
	/* close output */
	if (ofmt_ctx_a && !(ofmt_a->flags & AVFMT_NOFILE))
		avio_close(ofmt_ctx_a->pb);
 
	if (ofmt_ctx_v && !(ofmt_v->flags & AVFMT_NOFILE))
		avio_close(ofmt_ctx_v->pb);
 
	avformat_free_context(ofmt_ctx_a);
	avformat_free_context(ofmt_ctx_v);
 
 
	if (ret < 0 && ret != AVERROR_EOF) {
		printf( "Error occurred.\n");
		return -1;
	}
	return 0;
}
```

#### <a id="6.6.6">6.ffmpeg将YUV原始数据编码封装为mp4格式</a>
```c++
流程：
1、首先使用av_register_all()函数注册所有的编码器和复用器(理解为格式封装器)。该步骤必须放在所有ffmpeg代码前第一个执行
2、avformat_alloc_output_context2()：初始化包含有输出码流(AVStream)和解复用器(AVInputFormat)的AVFormatContext
3、avio_open( )打开输出文件
4、av_new_stream() 创建视频码流 该函数生成一个空AVstream 该结构存放编码后的视频码流 。视频码流被拆分为AVPacket新式保存在AVStream中。
5、设置编码器信息，该步骤主要是为AVCodecContext(从AVStream->codec 获取指针)结构体设置一些参数，包括codec_id、codec_type、width、height、pix_fmt .....  根据编码器的不同，还要额外设置一些参数(如 h264 要设置qmax、qmin、qcompress参数才能正常使用h264编码)
6、查找并打开编码器，根据前一步设置的编码器参数信息，来查找初始化一个编码其，并将其打开。用到函数为av_fine_encoder()和av_open2()。
7、写头文件  avformat_write_header()。这一步主要是将封装格式的信息写入文件头部位置。
8、编码帧。用到的函数 avcodec_encode_video2() 将AVFrame编码为AVPacket
9、在写入文件之前 还需要做一件事情就是设置AVPacket一些信息。这些信息关乎最后封装格式能否被正确读取。后面回详细讲述该部分内容
10、编码帧写入文件 av_write_frame()
11、flush_encoder()：输入的像素数据读取完成后调用此函数。用于输出编码器中剩余的AVPacket。
12、av_write_trailer()：写文件尾（对于某些没有文件头的封装格式，不需要此函数。比如说MPEG2TS）。

extern "C"
{
#include <libavcodec/avcodec.h>
#include <libavformat/avformat.h>
#include <libavutil/avutil.h>
#include <libswscale/swscale.h>
}
 
using namespace std;
 
int flush_encoder(AVFormatContext *fmt_ctx,unsigned int stream_index);
 
int main(int argc, char *argv[])
{
    AVFormatContext *pFormatCtx=nullptr;
    AVOutputFormat *fmt=nullptr;
    AVStream *video_st=nullptr;
    AVCodecContext *pCodecCtx=nullptr;
    AVCodec *pCodec=nullptr;
 
    uint8_t *picture_buf=nullptr;
    AVFrame *picture=nullptr;
    int size;
 
    //打开视频
    FILE *in_file = fopen("F://src01_480x272.yuv", "rb");
    if(!in_file)
    {
        cout<<"can not open file!"<<endl;
        return -1;
    }
 
    int in_w=480,in_h=272;
    int framenum=50;
    const char* out_file="src01.mp4";
 
    //[1] --注册所有ffmpeg组件
    avcodec_register_all();
    av_register_all();
    //[1]
 
    //[2] --初始化AVFormatContext结构体,根据文件名获取到合适的封装格式
    avformat_alloc_output_context2(&pFormatCtx,NULL,NULL,out_file);
    fmt = pFormatCtx->oformat;
    //[2]
 
    //[3] --打开文件
    if(avio_open(&pFormatCtx->pb,out_file,AVIO_FLAG_READ_WRITE))
    {
        cout<<"output file open fail!";
        goto end;
    }
    //[3]
 
    //[4] --初始化视频码流
    video_st = avformat_new_stream(pFormatCtx,0);
    if(video_st==NULL)
    { printf("failed allocating output stram\n");
        goto end;
    }
    video_st->time_base.num = 1;
    video_st->time_base.den =25;
    //[4]
 
    //[5] --编码器Context设置参数
    pCodecCtx = video_st->codec;
    pCodecCtx->codec_id = fmt->video_codec;
    pCodecCtx->codec_type = AVMEDIA_TYPE_VIDEO;
    pCodecCtx->pix_fmt = AV_PIX_FMT_YUV420P;
    pCodecCtx->width=in_w;
    pCodecCtx->height=in_h;
    pCodecCtx->time_base.num = 1;
    pCodecCtx->time_base.den = 25;
    pCodecCtx->bit_rate = 400000;
    pCodecCtx->gop_size = 12;
 
    if(pCodecCtx->codec_id == AV_CODEC_ID_H264)
    {
        pCodecCtx->qmin = 10;
        pCodecCtx->qmax = 51;
        pCodecCtx->qcompress = 0.6;
    }
    if (pCodecCtx->codec_id == AV_CODEC_ID_MPEG2VIDEO)
        pCodecCtx->max_b_frames = 2;
    if (pCodecCtx->codec_id == AV_CODEC_ID_MPEG1VIDEO)
        pCodecCtx->mb_decision = 2;
    //[5]
 
    //[6] --寻找编码器并打开编码器
    pCodec = avcodec_find_encoder(pCodecCtx->codec_id);
    if(!pCodec)
    {
        cout<<"no right encoder!"<<endl;
        goto end;
    }
    if(avcodec_open2(pCodecCtx,pCodec,NULL)<0)
    {
        cout<<"open encoder fail!"<<endl;
        goto end;
    }
    //[6]
 
    //输出格式信息
    av_dump_format(pFormatCtx,0,out_file,1);
 
    //初始化帧
    picture = av_frame_alloc();
    picture->width=pCodecCtx->width;
    picture->height=pCodecCtx->height;
    picture->format=pCodecCtx->pix_fmt;
    size = avpicture_get_size(pCodecCtx->pix_fmt,pCodecCtx->width,pCodecCtx->height);
    picture_buf = (uint8_t*)av_malloc(size);
    avpicture_fill((AVPicture*)picture,picture_buf,pCodecCtx->pix_fmt,pCodecCtx->width,pCodecCtx->height);
 
    //[7] --写头文件
    avformat_write_header(pFormatCtx,NULL);
    //[7]
 
    AVPacket pkt; //创建已编码帧
    int y_size = pCodecCtx->width*pCodecCtx->height;
    av_new_packet(&pkt,size*3);
 
    //[8] --循环编码每一帧
    for(int i=0;i<framenum;i++)
    {
        //读入YUV
        if(fread(picture_buf,1,y_size*3/2,in_file)<0)
        {
            cout<<"read file fail!"<<endl;
            goto end;
        }
        else if(feof(in_file))
            break;
 
        picture->data[0] = picture_buf; //亮度Y
        picture->data[1] = picture_buf+y_size; //U
        picture->data[2] = picture_buf+y_size*5/4; //V
        //AVFrame PTS
        picture->pts=i;
        int got_picture = 0;
 
        //编码
        int ret = avcodec_encode_video2(pCodecCtx,&pkt,picture,&got_picture);
        if(ret<0)
        {
            cout<<"encoder fail!"<<endl;
            goto end;
        }
 
        if(got_picture == 1)
        {
            cout<<"encoder success!"<<endl;
 
            // parpare packet for muxing
            pkt.stream_index = video_st->index;
            av_packet_rescale_ts(&pkt, pCodecCtx->time_base, video_st->time_base);
            pkt.pos = -1;
            ret = av_interleaved_write_frame(pFormatCtx,&pkt);
            av_free_packet(&pkt);
        }
    }
    //[8]
 
    //[9] --Flush encoder
    int ret = flush_encoder(pFormatCtx,0);
    if(ret < 0)
    {
        cout<<"flushing encoder failed!"<<endl;
        goto end;
    }
    //[9]
 
    //[10] --写文件尾
    av_write_trailer(pFormatCtx);
    //[10]
 
end:
    //释放内存
    if(video_st)
    {
        avcodec_close(video_st->codec);
        av_free(picture);
        av_free(picture_buf);
    }
    if(pFormatCtx)
    {
        avio_close(pFormatCtx->pb);
        avformat_free_context(pFormatCtx);
    }
 
    fclose(in_file);
 
    return 0;
}
 
int flush_encoder(AVFormatContext *fmt_ctx,unsigned int stream_index)
{
    int ret;
    int got_frame;
    AVPacket enc_pkt;
    if (!(fmt_ctx->streams[stream_index]->codec->codec->capabilities &
          CODEC_CAP_DELAY))
        return 0;
    while (1) {
        printf("Flushing stream #%u encoder\n", stream_index);
        enc_pkt.data = NULL;
        enc_pkt.size = 0;
        av_init_packet(&enc_pkt);
        ret = avcodec_encode_video2 (fmt_ctx->streams[stream_index]->codec, &enc_pkt,
                                     NULL, &got_frame);
        av_frame_free(NULL);
        if (ret < 0)
            break;
        if (!got_frame)
        {ret=0;break;}
        cout<<"success encoder 1 frame"<<endl;
 
        // parpare packet for muxing
        enc_pkt.stream_index = stream_index;
        av_packet_rescale_ts(&enc_pkt,
                             fmt_ctx->streams[stream_index]->codec->time_base,
                             fmt_ctx->streams[stream_index]->time_base);
        ret = av_interleaved_write_frame(fmt_ctx, &enc_pkt);
        if (ret < 0)
            break;
    }
    return ret;
}
```

### <a id="6.6.7">7.FFmpeg流媒体处理-收流与推流</a>
参考链接：   
https://www.cnblogs.com/leisure_chn/p/10623968.html   

```c++
#include <stdbool.h>
#include <libavutil/timestamp.h>
#include <libavformat/avformat.h>

// ffmpeg -re -i tnhaoxc.flv -c copy -f flv rtmp://192.168.0.104/live
// ffmpeg -i rtmp://192.168.0.104/live -c copy tnlinyrx.flv
// ./streamer tnhaoxc.flv rtmp://192.168.0.104/live
// ./streamer rtmp://192.168.0.104/live tnhaoxc.flv
int main(int argc, char **argv)
{
    AVOutputFormat *ofmt = NULL;
    AVFormatContext *ifmt_ctx = NULL, *ofmt_ctx = NULL;
    AVPacket pkt;
    const char *in_filename, *out_filename;
    int ret, i;
    int stream_index = 0;
    int *stream_mapping = NULL;
    int stream_mapping_size = 0;

    if (argc < 3) {
        printf("usage: %s input output\n"
               "API example program to remux a media file with libavformat and libavcodec.\n"
               "The output format is guessed according to the file extension.\n"
               "\n", argv[0]);
        return 1;
    }

    in_filename  = argv[1];
    out_filename = argv[2];

    // 1. 打开输入
    // 1.1 读取文件头，获取封装格式相关信息
    if ((ret = avformat_open_input(&ifmt_ctx, in_filename, 0, 0)) < 0) {
        printf("Could not open input file '%s'", in_filename);
        goto end;
    }
    
    // 1.2 解码一段数据，获取流相关信息
    if ((ret = avformat_find_stream_info(ifmt_ctx, 0)) < 0) {
        printf("Failed to retrieve input stream information");
        goto end;
    }

    av_dump_format(ifmt_ctx, 0, in_filename, 0);

    // 2. 打开输出
    // 2.1 分配输出ctx
    bool push_stream = false;
    char *ofmt_name = NULL;
    if (strstr(out_filename, "rtmp://") != NULL) {
        push_stream = true;
        ofmt_name = "flv";
    }
    else if (strstr(out_filename, "udp://") != NULL) {
        push_stream = true;
        ofmt_name = "mpegts";
    }
    else {
        push_stream = false;
        ofmt_name = NULL;
    }
    avformat_alloc_output_context2(&ofmt_ctx, NULL, ofmt_name, out_filename);
    if (!ofmt_ctx) {
        printf("Could not create output context\n");
        ret = AVERROR_UNKNOWN;
        goto end;
    }

    stream_mapping_size = ifmt_ctx->nb_streams;
    stream_mapping = av_mallocz_array(stream_mapping_size, sizeof(*stream_mapping));
    if (!stream_mapping) {
        ret = AVERROR(ENOMEM);
        goto end;
    }

    ofmt = ofmt_ctx->oformat;

    AVRational frame_rate;
    double duration;

    for (i = 0; i < ifmt_ctx->nb_streams; i++) {
        AVStream *out_stream;
        AVStream *in_stream = ifmt_ctx->streams[i];
        AVCodecParameters *in_codecpar = in_stream->codecpar;

        if (in_codecpar->codec_type != AVMEDIA_TYPE_AUDIO &&
            in_codecpar->codec_type != AVMEDIA_TYPE_VIDEO &&
            in_codecpar->codec_type != AVMEDIA_TYPE_SUBTITLE) {
            stream_mapping[i] = -1;
            continue;
        }

        if (push_stream && (in_codecpar->codec_type == AVMEDIA_TYPE_VIDEO)) {
            frame_rate = av_guess_frame_rate(ifmt_ctx, in_stream, NULL);
            duration = (frame_rate.num && frame_rate.den ? av_q2d((AVRational){frame_rate.den, frame_rate.num}) : 0);
        }

        stream_mapping[i] = stream_index++;

        // 2.2 将一个新流(out_stream)添加到输出文件(ofmt_ctx)
        out_stream = avformat_new_stream(ofmt_ctx, NULL);
        if (!out_stream) {
            printf("Failed allocating output stream\n");
            ret = AVERROR_UNKNOWN;
            goto end;
        }

        // 2.3 将当前输入流中的参数拷贝到输出流中
        ret = avcodec_parameters_copy(out_stream->codecpar, in_codecpar);
        if (ret < 0) {
            printf("Failed to copy codec parameters\n");
            goto end;
        }
        out_stream->codecpar->codec_tag = 0;
    }
    av_dump_format(ofmt_ctx, 0, out_filename, 1);

    if (!(ofmt->flags & AVFMT_NOFILE)) {    // TODO: 研究AVFMT_NOFILE标志
        // 2.4 创建并初始化一个AVIOContext，用以访问URL(out_filename)指定的资源
        ret = avio_open(&ofmt_ctx->pb, out_filename, AVIO_FLAG_WRITE);
        if (ret < 0) {
            printf("Could not open output file '%s'", out_filename);
            goto end;
        }
    }

    // 3. 数据处理
    // 3.1 写输出文件头
    ret = avformat_write_header(ofmt_ctx, NULL);
    if (ret < 0) {
        printf("Error occurred when opening output file\n");
        goto end;
    }

    while (1) {
        AVStream *in_stream, *out_stream;

        // 3.2 从输出流读取一个packet
        ret = av_read_frame(ifmt_ctx, &pkt);
        if (ret < 0) {
            break;
        }

        in_stream  = ifmt_ctx->streams[pkt.stream_index];
        if (pkt.stream_index >= stream_mapping_size ||
            stream_mapping[pkt.stream_index] < 0) {
            av_packet_unref(&pkt);
            continue;
        }

        int codec_type = in_stream->codecpar->codec_type;
        if (push_stream && (codec_type == AVMEDIA_TYPE_VIDEO)) {
            av_usleep((int64_t)(duration*AV_TIME_BASE));
        }

        pkt.stream_index = stream_mapping[pkt.stream_index];
        out_stream = ofmt_ctx->streams[pkt.stream_index];

        /* copy packet */
        // 3.3 更新packet中的pts和dts
        // 关于AVStream.time_base(容器中的time_base)的说明：
        // 输入：输入流中含有time_base，在avformat_find_stream_info()中可取到每个流中的time_base
        // 输出：avformat_write_header()会根据输出的封装格式确定每个流的time_base并写入文件中
        // AVPacket.pts和AVPacket.dts的单位是AVStream.time_base，不同的封装格式AVStream.time_base不同
        // 所以输出文件中，每个packet需要根据输出封装格式重新计算pts和dts
        av_packet_rescale_ts(&pkt, in_stream->time_base, out_stream->time_base);
        pkt.pos = -1;

        // 3.4 将packet写入输出
        ret = av_interleaved_write_frame(ofmt_ctx, &pkt);
        if (ret < 0) {
            printf("Error muxing packet\n");
            break;
        }
        av_packet_unref(&pkt);
    }

    // 3.5 写输出文件尾
    av_write_trailer(ofmt_ctx);

end:
    avformat_close_input(&ifmt_ctx);

    /* close output */
    if (ofmt_ctx && !(ofmt->flags & AVFMT_NOFILE)) {
        avio_closep(&ofmt_ctx->pb);
    }
    avformat_free_context(ofmt_ctx);

    av_freep(&stream_mapping);

    if (ret < 0 && ret != AVERROR_EOF) {
        printf("Error occurred: %s\n", av_err2str(ret));
        return 1;
    }

    return 0;
}
```

### <a id="6.6.8">8.基于ffmpeg的内存数据UDP直播推流</a>
参考链接：   
1.基于ffmpeg的内存数据UDP直播推流    
https://blog.csdn.net/chuicao4350/article/details/51554155?locationNum=8&fps=1   

2.最简单的基于FFmpeg的推流器（以推送RTMP为例）   
https://blog.csdn.net/leixiaohua1020/article/details/39803457      

```c++
unsigned char * iobuffer=(unsigned char *)av_malloc(32768);  
AVIOContext *avio =avio_alloc_context(iobuffer, 32768,0,NULL,fill_iobuffer,NULL,NULL);  
ic->pb=avio;  
err = avformat_open_input(&ic, "nothing", NULL, NULL); 
将buf_size大小的数据读到buf中，并返回buf大小，buf大小参照VLC设置为32k，自己可以设置。（读取过程中，出现一级
错误，可能FPGA寄存器和读取速度不匹配导致，或者GPMC被占用，速率不足导致，具体原因后续分析）。

RTMP采用的封装格式是FLV。因此在指定输出流媒体的时候需要指定其封装格式为“flv”。同理，其他流媒体协议也需要指定其
封装格式。例如采用UDP推送流媒体的时候，可以指定其封装格式为“mpegts”。

发送流媒体的数据的时候需要延时。不然的话，FFmpeg处理数据速度很快，瞬间就能把所有的数据发送出去，流媒体服务器
是接受不了的。因此需要按照视频实际的帧率发送数据。本文记录的推流器在视频帧与帧之间采用了av_usleep()函数休眠的
方式来延迟发送。这样就可以按照视频的帧率发送数据了
//…
int64_t start_time=av_gettime();
while (1) {
//…
	//Important:Delay
	if(pkt.stream_index==videoindex){
		AVRational time_base=ifmt_ctx->streams[videoindex]->time_base;
		AVRational time_base_q={1,AV_TIME_BASE};
		int64_t pts_time = av_rescale_q(pkt.dts, time_base, time_base_q);
		int64_t now_time = av_gettime() - start_time;
		if (pts_time > now_time)
			av_usleep(pts_time - now_time);
	}
//…
}

没有封装格式的裸流（例如H.264裸流）是不包含PTS、DTS这些参数的。在发送这种数据的时候，需要自己计算并写入
AVPacket的PTS，DTS，duration等参数。这里还没有深入研究，简单写了一点代码，如下所示。
//FIX：No PTS (Example: Raw H.264)
//Simple Write PTS
if(pkt.pts==AV_NOPTS_VALUE){
	//Write PTS
	AVRational time_base1=ifmt_ctx->streams[videoindex]->time_base;
	//Duration between 2 frames (us)
	int64_t calc_duration=(double)AV_TIME_BASE/av_q2d(ifmt_ctx->streams[videoindex]->r_frame_rate);
	//Parameters
	pkt.pts=(double)(frame_index*calc_duration)/(double)(av_q2d(time_base1)*AV_TIME_BASE);
	pkt.dts=pkt.pts;
	pkt.duration=(double)calc_duration/(double)(av_q2d(time_base1)*AV_TIME_BASE);
}

#include <stdio.h>
 
#define __STDC_CONSTANT_MACROS
 
#ifdef _WIN32
//Windows
extern "C"
{
#include "libavformat/avformat.h"
#include "libavutil/mathematics.h"
#include "libavutil/time.h"
};
#else
//Linux...
#ifdef __cplusplus
extern "C"
{
#endif
#include <libavformat/avformat.h>
#include <libavutil/mathematics.h>
#include <libavutil/time.h>
#ifdef __cplusplus
};
#endif
#endif
 
int main(int argc, char* argv[])
{
	AVOutputFormat *ofmt = NULL;
	//输入对应一个AVFormatContext，输出对应一个AVFormatContext
	//（Input AVFormatContext and Output AVFormatContext）
	AVFormatContext *ifmt_ctx = NULL, *ofmt_ctx = NULL;
	AVPacket pkt;
	const char *in_filename, *out_filename;
	int ret, i;
	int videoindex=-1;
	int frame_index=0;
	int64_t start_time=0;
	//in_filename  = "cuc_ieschool.mov";
	//in_filename  = "cuc_ieschool.mkv";
	//in_filename  = "cuc_ieschool.ts";
	//in_filename  = "cuc_ieschool.mp4";
	//in_filename  = "cuc_ieschool.h264";
	in_filename  = "cuc_ieschool.flv";//输入URL（Input file URL）
	//in_filename  = "shanghai03_p.h264";
	
	out_filename = "rtmp://localhost/publishlive/livestream";//输出 URL（Output URL）[RTMP]
	//out_filename = "rtp://233.233.233.233:6666";//输出 URL（Output URL）[UDP]
 
	av_register_all();
	//Network
	avformat_network_init();
	//输入（Input）
	if ((ret = avformat_open_input(&ifmt_ctx, in_filename, 0, 0)) < 0) {
		printf( "Could not open input file.");
		goto end;
	}
	if ((ret = avformat_find_stream_info(ifmt_ctx, 0)) < 0) {
		printf( "Failed to retrieve input stream information");
		goto end;
	}
 
	for(i=0; i<ifmt_ctx->nb_streams; i++) 
		if(ifmt_ctx->streams[i]->codec->codec_type==AVMEDIA_TYPE_VIDEO){
			videoindex=i;
			break;
		}
 
	av_dump_format(ifmt_ctx, 0, in_filename, 0);
 
	//输出（Output）
	
	avformat_alloc_output_context2(&ofmt_ctx, NULL, "flv", out_filename); //RTMP
	//avformat_alloc_output_context2(&ofmt_ctx, NULL, "mpegts", out_filename);//UDP
 
	if (!ofmt_ctx) {
		printf( "Could not create output context\n");
		ret = AVERROR_UNKNOWN;
		goto end;
	}
	ofmt = ofmt_ctx->oformat;
	for (i = 0; i < ifmt_ctx->nb_streams; i++) {
		//根据输入流创建输出流（Create output AVStream according to input AVStream）
		AVStream *in_stream = ifmt_ctx->streams[i];
		AVStream *out_stream = avformat_new_stream(ofmt_ctx, in_stream->codec->codec);
		if (!out_stream) {
			printf( "Failed allocating output stream\n");
			ret = AVERROR_UNKNOWN;
			goto end;
		}
		//复制AVCodecContext的设置（Copy the settings of AVCodecContext）
		ret = avcodec_copy_context(out_stream->codec, in_stream->codec);
		if (ret < 0) {
			printf( "Failed to copy context from input to output stream codec context\n");
			goto end;
		}
		out_stream->codec->codec_tag = 0;
		if (ofmt_ctx->oformat->flags & AVFMT_GLOBALHEADER)
			out_stream->codec->flags |= CODEC_FLAG_GLOBAL_HEADER;
	}
	//Dump Format------------------
	av_dump_format(ofmt_ctx, 0, out_filename, 1);
	//打开输出URL（Open output URL）
	if (!(ofmt->flags & AVFMT_NOFILE)) {
		ret = avio_open(&ofmt_ctx->pb, out_filename, AVIO_FLAG_WRITE);
		if (ret < 0) {
			printf( "Could not open output URL '%s'", out_filename);
			goto end;
		}
	}
	//写文件头（Write file header）
	ret = avformat_write_header(ofmt_ctx, NULL);
	if (ret < 0) {
		printf( "Error occurred when opening output URL\n");
		goto end;
	}
 
	start_time=av_gettime();
	while (1) {
		AVStream *in_stream, *out_stream;
		//获取一个AVPacket（Get an AVPacket）
		ret = av_read_frame(ifmt_ctx, &pkt);
		if (ret < 0)
			break;
		//FIX：No PTS (Example: Raw H.264)
		//Simple Write PTS
		if(pkt.pts==AV_NOPTS_VALUE){
			//Write PTS
			AVRational time_base1=ifmt_ctx->streams[videoindex]->time_base;
			//Duration between 2 frames (us)
			int64_t calc_duration=(double)AV_TIME_BASE/av_q2d(ifmt_ctx->streams[videoindex]->r_frame_rate);
			//Parameters
			pkt.pts=(double)(frame_index*calc_duration)/(double)(av_q2d(time_base1)*AV_TIME_BASE);
			pkt.dts=pkt.pts;
			pkt.duration=(double)calc_duration/(double)(av_q2d(time_base1)*AV_TIME_BASE);
		}
		//Important:Delay
		if(pkt.stream_index==videoindex){
			AVRational time_base=ifmt_ctx->streams[videoindex]->time_base;
			AVRational time_base_q={1,AV_TIME_BASE};
			int64_t pts_time = av_rescale_q(pkt.dts, time_base, time_base_q);
			int64_t now_time = av_gettime() - start_time;
			if (pts_time > now_time)
				av_usleep(pts_time - now_time);
 
		}
 
		in_stream  = ifmt_ctx->streams[pkt.stream_index];
		out_stream = ofmt_ctx->streams[pkt.stream_index];
		/* copy packet */
		//转换PTS/DTS（Convert PTS/DTS）
		pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream->time_base, out_stream->time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX));
		pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream->time_base, out_stream->time_base, (AVRounding)(AV_ROUND_NEAR_INF|AV_ROUND_PASS_MINMAX));
		pkt.duration = av_rescale_q(pkt.duration, in_stream->time_base, out_stream->time_base);
		pkt.pos = -1;
		//Print to Screen
		if(pkt.stream_index==videoindex){
			printf("Send %8d video frames to output URL\n",frame_index);
			frame_index++;
		}
		//ret = av_write_frame(ofmt_ctx, &pkt);
		ret = av_interleaved_write_frame(ofmt_ctx, &pkt);
 
		if (ret < 0) {
			printf( "Error muxing packet\n");
			break;
		}
		
		av_free_packet(&pkt);
		
	}
	//写文件尾（Write file trailer）
	av_write_trailer(ofmt_ctx);
end:
	avformat_close_input(&ifmt_ctx);
	/* close output */
	if (ofmt_ctx && !(ofmt->flags & AVFMT_NOFILE))
		avio_close(ofmt_ctx->pb);
	avformat_free_context(ofmt_ctx);
	if (ret < 0 && ret != AVERROR_EOF) {
		printf( "Error occurred.\n");
		return -1;
	}
	return 0;
}
 
在计算dts的时候，开始dts的计算简单地等于pts，由于B帧的存在，dts是小于pts的。稍作改进，下一帧的dts是上一帧的pt
s就可以了。音视频的duration不同而导致的，有说出现的包导致dts>pts。错误处理程序compute_pkt_fields2()，函数的
定义位于libavformat\mux.c，错误处理部分如下所示。从中可以看到，ffmpeg在检查dts、pts合理性时，若出现dts>pts的
情况，会报错，停止推流。简单的处理，就是规避合理性检查。从源头中解决出现这种错误，还需再做研究。
```

### <a id="6.6.9">9.FFmpeg内存H264流发布rtmp</a>
参考链接：   
1.FFmpeg内存H264流发布rtmp   
https://blog.csdn.net/qq_22633333/article/details/98207713   

```c+++
由于新版FFmpeg和旧版FFmpeg在接口上已经有发生了一些区别，所以我这边就拿最新版本的FFmpeg来做，首先用FFmpeg的
remuxing.c例程出来做测试，主要是用来读取IPC出来的rtsp流存成FLV或者发布成rtmp，进过试验，存成的flv格式是正确
的格式，rtmp也能正常在播放器上面播放，在flv.js上面播放都没问题（我的nginx是有集成nginx-http-flv-module的）。
   AVOutputFormat *ofmt = NULL;
	AVFormatContext *ifmt_ctx = NULL, *ofmt_ctx = NULL;
	AVPacket pkt;
	const char *in_filename, *out_filename;
	int ret, i;
	int stream_index = 0;
	int *stream_mapping = NULL;
	int stream_mapping_size = 0;

	/*if (argc < 3) {
		printf("usage: %s input output\n"
			"API example program to remux a media file with libavformat and libavcodec.\n"
			"The output format is guessed according to the file extension.\n"
			"\n", argv[0]);
		return 1;
	}

	in_filename = argv[1];
	out_filename = argv[2];*/

	in_filename = "rtsp://admin:admin@172.16.28.253:554/h264/ch1/main/av_stream?videoCodecType=H.264";
	out_filename = "rtmp://localhost:1985/live/mystream";
	//out_filename = "demo.flv";

	if ((ret = avformat_open_input(&ifmt_ctx, in_filename, 0, 0)) < 0) {
		fprintf(stderr, "Could not open input file '%s'", in_filename);
		goto end;
	}

	if ((ret = avformat_find_stream_info(ifmt_ctx, 0)) < 0) {
		fprintf(stderr, "Failed to retrieve input stream information");
		goto end;
	}

	av_dump_format(ifmt_ctx, 0, in_filename, 0);

	avformat_alloc_output_context2(&ofmt_ctx, NULL, "flv", out_filename);
	if (!ofmt_ctx) {
		fprintf(stderr, "Could not create output context\n");
		ret = AVERROR_UNKNOWN;
		goto end;
	}

	stream_mapping_size = ifmt_ctx->nb_streams;
	stream_mapping = (int*)av_mallocz_array(stream_mapping_size, sizeof(*stream_mapping));
	if (!stream_mapping) {
		ret = AVERROR(ENOMEM);
		goto end;
	}

	ofmt = ofmt_ctx->oformat;

	for (i = 0; i < ifmt_ctx->nb_streams; i++) {
		AVStream *out_stream;
		AVStream *in_stream = ifmt_ctx->streams[i];
		AVCodecParameters *in_codecpar = in_stream->codecpar;

		if (in_codecpar->codec_type != AVMEDIA_TYPE_AUDIO &&
			in_codecpar->codec_type != AVMEDIA_TYPE_VIDEO &&
			in_codecpar->codec_type != AVMEDIA_TYPE_SUBTITLE) {
			stream_mapping[i] = -1;
			continue;
		}

		stream_mapping[i] = stream_index++;

		out_stream = avformat_new_stream(ofmt_ctx, NULL);
		if (!out_stream) {
			fprintf(stderr, "Failed allocating output stream\n");
			ret = AVERROR_UNKNOWN;
			goto end;
		}

		ret = avcodec_parameters_copy(out_stream->codecpar, in_codecpar);
		if (ret < 0) {
			fprintf(stderr, "Failed to copy codec parameters\n");
			goto end;
		}
		out_stream->codecpar->codec_tag = 0;
	}
	av_dump_format(ofmt_ctx, 0, out_filename, 1);

	if (!(ofmt->flags & AVFMT_NOFILE)) {
		ret = avio_open(&ofmt_ctx->pb, out_filename, AVIO_FLAG_WRITE);
		if (ret < 0) {
			fprintf(stderr, "Could not open output file '%s'", out_filename);
			goto end;
		}
	}

	ret = avformat_write_header(ofmt_ctx, NULL);
	if (ret < 0) {
		fprintf(stderr, "Error occurred when opening output file\n");
		goto end;
	}

	while (1) {
		AVStream *in_stream, *out_stream;

		ret = av_read_frame(ifmt_ctx, &pkt);
		if (ret < 0)
			break;

		in_stream = ifmt_ctx->streams[pkt.stream_index];
		if (pkt.stream_index >= stream_mapping_size ||
			stream_mapping[pkt.stream_index] < 0) {
			av_packet_unref(&pkt);
			continue;
		}

		pkt.stream_index = stream_mapping[pkt.stream_index];
		out_stream = ofmt_ctx->streams[pkt.stream_index];
		//log_packet(ifmt_ctx, &pkt, "in");

		/* copy packet */
		pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream->time_base, out_stream->time_base, (AVRounding)(AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX));
		pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream->time_base, out_stream->time_base, (AVRounding)(AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX));
		pkt.duration = av_rescale_q(pkt.duration, in_stream->time_base, out_stream->time_base);
		pkt.pos = -1;
		//log_packet(ofmt_ctx, &pkt, "out");

		ret = av_interleaved_write_frame(ofmt_ctx, &pkt);
		if (ret < 0) {
			fprintf(stderr, "Error muxing packet\n");
			break;
		}
		av_packet_unref(&pkt);
	}

	av_write_trailer(ofmt_ctx);
end:

	avformat_close_input(&ifmt_ctx);

	/* close output */
	if (ofmt_ctx && !(ofmt->flags & AVFMT_NOFILE))
		avio_closep(&ofmt_ctx->pb);
	avformat_free_context(ofmt_ctx);

	av_freep(&stream_mapping);

	if (ret < 0 && ret != AVERROR_EOF) {
		fprintf(stderr, "Error occurred\n");
		return 1;
	}

	return 0;

extern "C"
{
#include <libavutil/timestamp.h>
#include <libavformat/avformat.h>
};

AVOutputFormat *ofmt = NULL;
AVFormatContext *ofmt_ctx = NULL;
//const char *out_filename = "new.flv";
const char *out_filename = "rtmp://localhost:1985/live/mystream";
int stream_index = 0;
int waitI = 0, rtmpisinit = 0;
int ptsInc = 0;

int GetSpsPpsFromH264(uint8_t* buf, int len)
{
	int i = 0;
	for (i = 0; i < len; i++) {
		if (buf[i+0] == 0x00 
			&& buf[i + 1] == 0x00
			&& buf[i + 2] == 0x00
			&& buf[i + 3] == 0x01
			&& buf[i + 4] == 0x06) {
			break;
		}
	}
	if (i == len) {
		printf("GetSpsPpsFromH264 error...");
		return 0;
	}

	printf("h264(i=%d):", i);
	for (int j = 0; j < i; j++) {
		printf("%x ", buf[j]);
	}
	return i;
}

static bool isIdrFrame2(uint8_t* buf, int len)
{
	switch (buf[0] & 0x1f) {
	case 7: // SPS
		return true;
	case 8: // PPS
		return true;
	case 5:
		return true;
	case 1:
		return false;

	default:
		return false;
		break;
	}
	return false;
}

static bool isIdrFrame1(uint8_t* buf, int size)
{
	int last = 0;
	for (int i = 2; i <= size; ++i) {
		if (i == size) {
			if (last) {
				bool ret = isIdrFrame2(buf + last, i - last);
				if (ret) {
					return true;
				}
			}
		}
		else if (buf[i - 2] == 0x00 && buf[i - 1] == 0x00 && buf[i] == 0x01) {
			if (last) {
				int size = i - last - 3;
				if (buf[i - 3]) ++size;
				bool ret = isIdrFrame2(buf + last, size);
				if (ret) {
					return true;
				}
			}
			last = i + 1;
		}
	}
	return false;
}

//初始化的时候必须把H264第一个关键帧的sps、pps数据放进来
static int RtmpInit(void* spspps_date, int spspps_datalen)
{
	int ret = 0;
	AVStream *out_stream;
	AVCodecParameters *out_codecpar;
	av_register_all();
	avformat_network_init();

	printf("rtmp init...\n");

	avformat_alloc_output_context2(&ofmt_ctx, NULL, "flv", NULL);// out_filename);
	if (!ofmt_ctx) {
		fprintf(stderr, "Could not create output context\n");
		ret = AVERROR_UNKNOWN;
		goto end;
	}

	ofmt = ofmt_ctx->oformat;

	out_stream = avformat_new_stream(ofmt_ctx, NULL);
	if (!out_stream) {
		fprintf(stderr, "Failed allocating output stream\n");
		ret = AVERROR_UNKNOWN;
		goto end;
	}
	stream_index = out_stream->index;

	//因为输入是内存读出来的一帧帧的H264数据，所以没有输入的codecpar信息，必须手动添加输出的codecpar
	out_codecpar = out_stream->codecpar;
	out_codecpar->codec_type = AVMEDIA_TYPE_VIDEO;
	out_codecpar->codec_id = AV_CODEC_ID_H264;
	out_codecpar->bit_rate = 400000;
	out_codecpar->width = 1280;
	out_codecpar->height = 720;
	out_codecpar->codec_tag = 0;
	out_codecpar->format = AV_PIX_FMT_YUV420P;

	//必须添加extradata(H264第一帧的sps和pps数据)，否则无法生成带有AVCDecoderConfigurationRecord信息的FLV
	//unsigned char sps_pps[26] = { 0x00, 0x00, 0x01, 0x67, 0x4d, 0x00, 0x1f, 0x9d, 0xa8, 0x14, 0x01, 0x6e, 0x9b, 0x80, 0x80, 0x80, 0x81, 0x00, 0x00, 0x00, 0x01, 0x68, 0xee, 0x3c, 0x80 };
	out_codecpar->extradata_size = spspps_datalen;
	out_codecpar->extradata = (uint8_t*)av_malloc(spspps_datalen + AV_INPUT_BUFFER_PADDING_SIZE);
	if (out_codecpar->extradata == NULL)
	{ 
		printf("could not av_malloc the video params extradata!\n");			
		goto end;
	}
	memcpy(out_codecpar->extradata, spspps_date, spspps_datalen);
	
	av_dump_format(ofmt_ctx, 0, out_filename, 1);

	if (!(ofmt->flags & AVFMT_NOFILE)) {
		ret = avio_open(&ofmt_ctx->pb, out_filename, AVIO_FLAG_WRITE);
		if (ret < 0) {
			fprintf(stderr, "Could not open output file '%s'", out_filename);
			goto end;
		}
	}

	AVDictionary *opts = NULL;
	av_dict_set(&opts, "flvflags", "add_keyframe_index", 0);
	ret = avformat_write_header(ofmt_ctx, &opts);
	av_dict_free(&opts);
	if (ret < 0) {
		fprintf(stderr, "Error occurred when opening output file\n");
		goto end;
	}

	waitI = 1;
	return 0;

end:
	/* close output */
	if (ofmt_ctx && !(ofmt->flags & AVFMT_NOFILE))
		avio_closep(&ofmt_ctx->pb);
	if (ofmt_ctx) {
		avformat_free_context(ofmt_ctx);
		ofmt_ctx = NULL;
	}
	return -1;
}

static void VideoWrite(void* data, int datalen)
{
	int ret = 0, isI = 0;
	AVStream *out_stream;
	AVPacket pkt;
	
	out_stream = ofmt_ctx->streams[stream_index];

	av_init_packet(&pkt);
	isI = isIdrFrame1((uint8_t*)data, datalen);
	pkt.flags |= isI ? AV_PKT_FLAG_KEY : 0;
	pkt.stream_index = out_stream->index;
	pkt.data = (uint8_t*)data;
	pkt.size = datalen;
	//wait I frame
	if (waitI) {
		if (0 == (pkt.flags & AV_PKT_FLAG_KEY))
			return;
		else
			waitI = 0;	
	}
	
	AVRational time_base;
	time_base.den = 50;
	time_base.num = 1;
	pkt.pts = av_rescale_q((ptsInc++) * 2, time_base, out_stream->time_base);
	pkt.dts = av_rescale_q_rnd(pkt.dts, out_stream->time_base, out_stream->time_base, (AVRounding)(AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX));
	pkt.duration = av_rescale_q(pkt.duration, out_stream->time_base, out_stream->time_base);
	pkt.pos = -1;

	/* copy packet (remuxing例子里面的)*/
	//pkt.pts = av_rescale_q_rnd(pkt.pts, in_stream->time_base, out_stream->time_base, AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX);
	//pkt.dts = av_rescale_q_rnd(pkt.dts, in_stream->time_base, out_stream->time_base, AV_ROUND_NEAR_INF | AV_ROUND_PASS_MINMAX);
	//pkt.duration = av_rescale_q(pkt.duration, in_stream->time_base, out_stream->time_base);
	//pkt.pos = -1;

	ret = av_interleaved_write_frame(ofmt_ctx, &pkt);
	if (ret < 0) {
		fprintf(stderr, "Error muxing packet\n");
	}

	av_packet_unref(&pkt);
}

static void RtmpUnit(void)
{
	if (ofmt_ctx)
		av_write_trailer(ofmt_ctx);
	/* close output */
	if (ofmt_ctx && !(ofmt->flags & AVFMT_NOFILE))
		avio_closep(&ofmt_ctx->pb);
	if (ofmt_ctx) {
		avformat_free_context(ofmt_ctx);
		ofmt_ctx = NULL;
	}
}

int main()
{
	//以下只是把个借口的调用方法简单罗列，具体调用要看实际情况
	
	//preturnps是输入的一帧帧264数据，iPsLength是一帧的长度
	//模拟初始化调用
	char *h264buffer = new char[iPsLength];
	memcpy(h264buffer, preturnps, iPsLength);
	printf("h264 len = %d\n", iPsLength);
	if (!rtmpisinit) {
		if (isIdrFrame1((uint8_t*)h264buffer, iPsLength)) {
				int spspps_len = GetSpsPpsFromH264((uint8_t*)h264buffer, iPsLength);
				if (spspps_len > 0) {
					char *spsbuffer = new char[spspps_len];
					memcpy(spsbuffer, h264buffer, spspps_len);
					rtmpisinit = 1;
					RtmpInit(spsbuffer, spspps_len);
					delete spsbuffer;
				}
		}
	}
	//开始推送视频数据
	if (rtmpisinit) {
		VideoWrite(h264buffer, iPsLength);
	}
	//去初始化
	RtmpUnit();
	return 0;
}
现在我们没有输入文件，只能根据已知的信息按照如下编辑的编解码器信息
AVCodecParameters *out_codecpar = out_stream->codecpar;
out_codecpar->codec_type = AVMEDIA_TYPE_VIDEO;
out_codecpar->codec_id = AV_CODEC_ID_H264;
out_codecpar->bit_rate = 400000;
out_codecpar->width = 1280;
out_codecpar->height = 720;
out_codecpar->codec_tag = 0;
out_codecpar->format = AV_PIX_FMT_YUV420P;
应该是缺失了AVCDecoderConfigurationRecord这个东东，为了验证，我把输出换成flv文件的方式和demo程序存成的flv对比（可以用flvanalyzer工具查看），确实是缺失了AVCDecoderConfigurationRecord。推送H264和AAC直播流，需要首先发送"AVC sequence header"和"AAC sequence header"，这两项数据包含的是重要的编码信息，没有它们，解码器将无法解码。

得知extradata作为一个global headers，其实主要保存的是SPS、PPS等信息；
因此手动指定SPS/PPS内容，指定AVCodecParameters结构体中extradata的值；
```


## links
  * [目录](<音视频入门到精通目录.md>)