# 4.2网页直播rtsp视频
- [1.网页直播视频总结](#1)
- [2.HTML5视频监控技术预研](#2)
- [3.node+ffmpeg实现rtsp转流](#3)


参考链接：  
1.github rtsp web   
https://github.com/littlebaozi/rtsp-web   


## <a id="1">1.网页直播视频总结</a>
参考链接：  
1.网页直播rtsp视频    
https://littlebaozi.github.io/2019/10/23/pratice-rtsp-h5/   


| 协议    | 说明                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |
| --------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| HLS       | HTTP实时流（HTTP Live Streaming），由苹果开发，基于HTTP协议。 HLS的工作原理是，把整个流划分成一个个较小的文件，客户端在建立流媒体会话后，基于HTTP协议下载流片段并播放。客户端可以从多个服务器（源）下载流。 在建立会话时，客户端需要下载extended M3U (m3u8) 播放列表文件，其中包含了MPEG-2 TS（Transport Stream）容器格式的视频的列表。在播放完列表中的文件后，需要再次下载m3u8，如此循环。 此协议在移动平台上支持较好，目前的Android、iOS版本都支持。 此协议的重要缺点是高延迟（5s以上通常），要做到低延迟会导致频繁的缓冲（下载新片段）并对服务器造成压力，不适合视频监控。 播放HLS流的HTML代码片段： undefined |
| RTMP      | 实时消息协议（Real Time Messaging Protocol），由Macromedia（Adobe）开发。此协议实时性很好，需要Flash插件才能在客户端使用，但是Adobe已经打算在不久的将来放弃对Flash的支持了。最新版Chrome已经禁用flash了。 有一个开源项目[HTML5 FLV Player](https://github.com/Bilibili/flv.js)，它支持在没有Flash插件的情况下，播放Flash的视频格式FLV。此项目依赖于MSE，支持以下特性： * 支持H.264 + AAC/MP3编码的FLV容器格式的播放 * 分段（segmented）视频播放 * 基于HTTP的FLV低延迟实时流播放 * 兼容主流浏览器 * 资源占用低，可以使用客户端的硬件加速 |
| RTSP      | 实时流协议（Real Time Streaming Protocol），由RealNetworks等公司开发。此协议负责控制通信端点（Endpoint）之间的媒体会话（media sessions） —— 例如播放、暂停、录制。通常需要结合：实时传输协议（Real-time Transport Protocol）、实时控制协议（Real-time Control Protocol）来实现视频流本身的传递。 大部分监控摄像头都是RTSP协议的。大部分浏览器没有对RTSP提供原生的支持。 RTSP 2.0版本目前正在开发中，和旧版本不兼容。                                                     |
| MPEG-DASH | 基于HTTP的动态自适应流（Dynamic Adaptive Streaming over HTTP），它类似于HLS，也是把流切分为很小的片段。DASH为支持为每个片段提供多种码率的版本，以满足不同客户带宽。 协议的客户端根据自己的可用带宽，选择尽可能高（避免卡顿、重新缓冲）的码率进行播放，并根据网络状况实时调整码率。 DASH不限制编码方式，你可以使用H.265, H.264, VP9等视频编码算法。 Chrome 24+、Firefox 32+、Chrome for Android、IE 10+支持此格式。 类似于HLS的高延迟问题也存在。      |
| WebRTC    | WebRTC是一整套API，为浏览器、移动应用提供实时通信（RealTime Communications）能力。它包含了流媒体协议的功能，但是不是以协议的方式暴露给开发者的。 WebRTC支持Chrome 23+、Firefox 22+、Chrome for Android，提供Java / Objective-C绑定。 WebRTC主要有三个职责： * 捕获客户端音视频，对应接口MediaStream（也就是getUserMedia） * 音视频传输，对应接口RTCPeerConnection * 任意数据传输，对应接口RTCDataChannel WebRTC内置了点对点的支持，也就是说流不一定需要经过服务器中转。 |

<img src="./image/2-197.png" style="zoom:100%" />

```c++
放弃的方案
付费的
如果公司有钱，可以购买别人的流媒体解决方案：easyNVR、青柿、Streamedian

RTSP转RTMP+Flash
此方案可以参考如何不花钱让html5播放rtsp视频流（第一弹）。

缺点：播放RTMP视频需要flash，较新版本的Chrome已经默认不支持flash了。
VLC插件

优点：使用VLC插件可以直接播放rtsp协议视频。
缺点：要先安装VLC播放器，较新版本Chrome和Firefox都不支持NPAPI了。

选中方案
最后采取了HTML5 播放 RTSP 视频这篇文章的方案。

前端将摄像头视频地址发送到服务端
服务端使用FFmpet将RTSP转码为flv片段。
通过websocket实时推流到前端网页上
前端使用flv.js转码播放视频。
flv.js使用的是MSE技术。flv.js直接用js转码flv流转为ISO BMFF（MP4）片断，然后把视频片段给video元素播放。

存在问题：浏览器切换标签或者最小化，视频会暂停播放导致延迟。目前解决办法是监听visibilitychange，重新显示时手动调整时间。

备选方案
如何不花钱让html5播放rtsp视频流（第二弹）这篇中同样是使用FFmpeg先转码rtsp，只不过是使用node-rtsp-stream-jsmpeg将视频推流成图片不停绘制canvas。
```


## <a id="2">2.HTML5视频监控技术预研</a>
参考链接：  
1.HTML5视频监控技术预研    
https://blog.gmem.cc/research-on-html5-video-surveillance    

2.HTML5 播放 RTSP 视频       
https://hpdell.github.io/%E7%BC%96%E7%A8%8B/html5-rtsp/index.html      


| 方案 | 协议    | 视频格式 | 延迟                     | 离线事件汇报 | 最小端口占用 | 依赖                            |
| ---- | --------- | -------- | -------------------------- | ------------ | ------------ | --------------------------------- |
| 1    | HLS       | ogg      | 网络延迟较高，可达10秒以上 | 难          | n            | VLC + video.js                    |
| 2    | RTMP      | flv      | 网络延迟较低，5秒左右 | 难          | n            | ffmpeg + nginx + flash + video.js |
| 3    | WebSocket | mpegts   | 网络延迟很低，渲染速度慢 | 易          | 1            | ffmpeg + express + jsmpeg         |
| 4    | HTTP-FLV  | flv      | 网络延迟很低，渲染速度快 | 易          | 1            | ffmpeg + express + flv.js         |

视频监控客户端主要是Native应用的形式，在Web端需要利用NPAPI、ActiveX之类的插件技术实现。

但是，IE式微，Chrome也放弃了NPAPI，另一方面，监控设备硬件厂商的视频输出格式则逐渐标准化。这让基于开放、标准化接口的Web视频监控成为可能。

本文讨论以HTML5及其衍生技术为基础的B/S架构实时视频监控解决方案。主要包括两方面的内容：  
1. 视频编码、流媒体基础知识，以及相关的库、框架的介绍  
2. 介绍可以用于视频监控的HTML5特性，例如媒体标签、MSE、WebRTC，以及相关的库、框架  

### 1.JSMpeg
视频解码工作通常由浏览器本身负责，配合video实现视频播放。

现代浏览器的JS引擎性能较好，因此出现了纯粹由JS实现的解码器JSMpeg，它能够解码视频格式MPEG1、音频格式MP2。支持通过Ajax加载静态视频文件，支持低延迟（小于50ms）的流式播放（通过WebSocket）。JSMpeg包括以下组件：  

MPEG-TS分流器（demuxer）。muxer负责把视频、音频、字幕打包成一种容器格式，demuxer则作相反的工作  
MPEG1视频解码器  
MP2音频解码器  
WebGL渲染器、Canvas2D渲染器  
WebAudio音频输出组件  
JSMpeg的优势在于兼容性好，几乎所有现代浏览器都能运行JSMpeg。  

性能  
JSMpeg不能使用硬件加速。在iPhone 5S这样的设备上，JSMpeg能够处理720p@30fps视频。  

比起现代解码器，MPEG1压缩率较低，因而需要更大的带宽。720p的视频大概占用250KB/s的带宽。  

示例
```js 
下面我们尝试利用ffmpeg编码本地摄像头视频，并通过JSMpeg播放。  

创建一个NPM项目，安装依赖：  
npm install jsmpeg --save
npm install ws --save
JSMpeg提供了一个中继器，能够把基于HTTP的MPEG-TS流转换后通过WebSocket发送给客户端。此脚本需要到Github下载。 下面的命令启动一个中继器：

node ./app/websocket-relay.js 12345 8800 8801
# Listening for incomming MPEG-TS Stream on http://127.0.0.1:8800/<secret>
# Awaiting WebSocket connections on ws://127.0.0.1:8801/
# 实际上在所有网络接口上监听，并非仅仅loopback
下面的命令捕获本地摄像头（Linux），并编码为MPEG1格式，然后发送到中继器：

       # 从摄像头/dev/video0以480的分辨率捕获原始视频流
ffmpeg -s 640x480 -f video4linux2 -i /dev/video0 \
       # 输出为原始MPEG-1视频（JSMpeg可用），帧率30fps，比特率800kbps
       -f mpegts -codec:v mpeg1video -b 800k -r 30 http://127.0.0.1:8800/12345
# 在我的机器上，上述ffmpeg私有内存占用18MB
上述命令执行后，中继器控制台上打印：

Stream Connected: ::ffff:127.0.0.1:42399

客户端代码：
var player = new JSMpeg.Player( 'ws://127.0.0.1:8801/', {
    canvas: document.getElementById( 'canvas' ),
    autoplay: true
} ); 
```

### 2.Broadway
Broadway是一个基于JavaScript的H.264解码器，其源码来自于Android的H.264解码器，利用Emscripten转译成了JavaScript，之后利用Google的Closure编译器优化，并针对WebGL进一步优化。

注意：Broadway仅仅支持Baseline这个H.264 Profile。

h264-live-player是基于Broadway实现的播放器，允许通过WebSocket来传输NAL单元（原始H.264帧），并在画布上渲染。我们运行一下它的示例应用：
```js
git clone https://github.com/131/h264-live-player.git
cd h264-live-player
npm install

因为我的机器是Linux，所以修改h264-live-player/lib/ffmpeg.js， 把ffpmeg的参数改为：

var args = [
    "-f", "video4linux2",
    "-i",  "/dev/video0" ,
    "-framerate", this.options.fps,
    "-video_size", this.options.width + 'x' + this.options.height,
    '-pix_fmt',  'yuv420p',
    '-c:v',  'libx264',
    '-b:v', '600k',
    '-bufsize', '600k',
    '-vprofile', 'baseline',
    '-tune', 'zerolatency',
    '-f' ,'rawvideo',
    '-'
];
然后运行 node server-ffmpeg，打开http://127.0.0.1:8080/，可以看到自己摄像头传来的H.264码流，效果还不错。
```

### 3.HTML5媒体标签
```js
HTML5支持 <audio>和 <video>标签（两者都对应了HTMLMediaElement的子类型）以实现视频、音频的播放。

<audio>
此标签用于在浏览器中创建一个纯音频播放器。播放静态文件的示例：
<audio controls preload="auto">
    <source src="song.mp3" type="audio/mpeg">
    <!-- 备选格式，如果浏览器不支持mp3 -->
    <source src="song.ogg" type="audio/ogg">
    <!-- 如果浏览器不支持audio标签，显示下面的连接 -->
    <a href="audiofile.mp3">download audio</a>
</audio>

<video>
此标签用于在浏览器中创建一个视频播放器。播放静态文件的示例：
<!-- poster指定预览图，autoplay自动播放，muted静音 -->
<video controls width="640" height="480" poster="movie.png" autoplay muted>
  <source src="movie.mp4" type="video/mp4">
  <!-- 备选格式，如果浏览器不支持mp4 -->
  <source src="movie.webm" type="video/webm">
  <!-- 可以附带字幕 -->
  <track src="subtitles_en.vtt" kind="subtitles" srclang="en" label="English">
  <!-- 如果浏览器不支持video标签，显示下面的连接 -->
  <a href="videofile.mp4">download video</a>
</video>

编程方式创建
音频、视频播放器标签也可以利用JavaScript编程式的创建，示例代码：
var video = document.createElement( 'video' );
if ( video.canPlayType( 'video/mp4' ) ) {
    video.setAttribute( 'src', 'movie.mp4' );
}
else if ( video.canPlayType( 'video/webm' ) ) {
    video.setAttribute( 'src', 'movie.webm' );
}
video.width = 640;
video.height = 480; 
```

### 4.MSE
媒体源扩展（Media Source Extensions，MSE）是一个W3C草案，桌面浏览器对MSE的支持较好。MSE扩展流video/audio元素的能力，允许你通过JavaScript来生成（例如从服务器抓取）媒体流供video/audio元素播放。使用MSE你可以：     
1. 通过JavaScript来构建媒体流，不管媒体是如何捕获的  
2. 处理自适应码流、广告插入、时间平移（time-shifting，回看）、视频编辑等应用场景  
3. 最小化JavaScript中处理媒体解析的代码  

MSE定义支持的（你生成的）媒体格式，只有符合要求的容器格式、编码格式才能被MSE处理。通常容器格式是ISO BMFF（MP4），也就是说你需要生成MP4的片断，然后Feed给MSE进行播放。

MediaSource对象作为video/audio元素的媒体来源，它可以具有多个SourceBuffer对象。应用程序把数据片段（segment）附加到SourceBuffer中，并可以根据系统性能对数据片段的质量进行适配。SourceBuffer中包含多个track buffer —— 分别对应音频、视频、文本等可播放数据。这些数据被音频、视频解码器解码，然后在屏幕上显示、在扬声器中播放：

<img src="./image/2-198.png" style="zoom:100%" />

```js
基于MSE的框架
wfs
wfs是一个播放原始H.264帧的HTML5播放器，它的工作方式是把H.264 NAL单元封装为 ISO BMFF（MP4）片，然后Feed给MSE处理。

flv.js
flv.js是一个HTML5 Flash视频播放器，基于纯JS，不需要Flash插件的支持。此播放器将FLV流转换为ISO BMFF（MP4）片断，然后把MP4片断提供给video元素使用。

flv.js支持Chrome 43+, FireFox 42+, Edge 15.15048+以上版本的直播流 。

Streamedian
Streamedian是一个HTML5的RTSP播放器。实现了RTSP客户端功能，你可以利用此框架直接播放RTSP直播流。此播放器把RTP协议下的H264/AAC在转换为ISO BMFF供video元素使用。Streamedian支持Chrome 23+, FireFox 42+, Edge 13+，以及Android 5.0+。不支持iOS和IE。
在服务器端，你需要安装Streamedian提供的代理（此代理收费），此代理将RTSP转换为WebSocket。Streamedian处理视频流的流程如下：
```

<img src="./image/2-199.png" style="zoom:100%" />

### 5.WebRTC
WebRTC是一整套API，其中一部分供Web开发者使用，另外一部分属于要求浏览器厂商实现的接口规范。WebRTC解决诸如客户端流媒体发送、点对点通信、视频编码等问题。桌面浏览器对WebRTC的支持较好，WebRTC也很容易和Native应用集成。

使用MSE时，你需要自己构建视频流。使用WebRTC时则可以直接捕获客户端视频流。

使用WebRTC时，大部分情况下流量不需要依赖于服务器中转，服务器的作用主要是：
1. 在信号处理时，转发客户端的数据
2. 配合实现NAT/防火墙穿透
3. 在点对点通信失败时，作为中继器使用

<img src="./image/2-200.png" style="zoom:100%" />

#### Camera.js
```js
Camera.js是对getUserMedia的简单封装，简化了API并提供了跨浏览器支持：
camera.init( {
    width: 640,
    height: 480,
    fps: 30, // 帧率
    mirror: false,  // 是否显示为镜像
    targetCanvas: document.getElementById( 'webcam' ), // 默认null，如果设置了则在画布中渲染
 
    onFrame: function ( canvas ) {
        // 每当新的帧被捕获，调用此回调
    },
 
    onSuccess: function () {
        // 流成功获取后
    },
 
    onError: function ( error ) {
        // 如果初始化失败
    },
 
    onNotSupported: function () {
        // 当浏览器不支持camera.js时
    }
} );
// 暂停
camera.pause();
// 恢复
camera.start();
掠食者视觉是基于Camera实现的一个好玩的例子（移动侦测）。
```

#### 流捕获
主要是捕获客户端摄像头、麦克风。在视频监控领域用处不大，这里大概了解一下。流捕获通过navigator.getUserMedia调用实现.

#### 信号处理
在端点之间（Peer）发送流之前，需要进行通信协调、发送控制消息，即所谓信号处理（Signaling），信号处理牵涉到三类信息：
1. 会话控制信息：初始化、关闭通信，报告错误
2. 网络配置：对于其它端点来说，本机的IP和端口是什么
3. 媒体特性：本机能够处理什么音视频编码、多高的分辨率。本机发送什么样的音视频编码
WebRTC没有对信号处理规定太多，我们可以通过Ajax/WebSocket通信，以SIP、Jingle、ISUP等协议完成信号处理。点对点连接设立后，流的传输并不需要服务器介入。

下面的代表片段包含了一个视频电话的信号处理过程：
```js
// 信号处理通道，底层传输方式和协议自定义
var signalingChannel = createSignalingChannel();
var conn;
 
// 信号通过此回调送达本地，可能分多次送达
signalingChannel.onmessage = function ( evt ) {
    if ( !conn ) start( false );
 
    var signal = JSON.parse( evt.data );
    // 会话描述协议（Session Description Protocol），用于交换媒体配置信息（分辨率、编解码能力）
    if ( signal.sdp )
    // 设置Peer的RTCSessionDescription
        conn.setRemoteDescription( new RTCSessionDescription( signal.sdp ) );
    else
    // 添加Peer的Candidate信息
        conn.addIceCandidate( new RTCIceCandidate( signal.candidate ) );
};
 
// 调用此方法启动WebRTC，获取本地流并显示，侦听连接上的事件并处理
function start( isCaller ) {
    conn = new RTCPeerConnection( { /**/ } );
 
    // 把地址/端口信息发送给其它Peer。所谓Candidate就是基于ICE框架获得的本机可用地址/端口
    conn.onicecandidate = function ( evt ) {
        signalingChannel.send( JSON.stringify( { "candidate": evt.candidate } ) );
    };
 
    // 当远程流到达后，在remoteView元素中显示
    conn.onaddstream = function ( evt ) {
        remoteView.src = URL.createObjectURL( evt.stream );
    };
 
    // 获得本地流
    navigator.getUserMedia( { "audio": true, "video": true }, function ( stream ) {
        // 在remoteView元素中显示
        localView.src = URL.createObjectURL( stream );
        // 添加本地流，Peer将接收到onaddstream事件
        conn.addStream( stream );
 
 
        if ( isCaller )
        // 获得本地的RTCSessionDescription
            conn.createOffer( gotDescription );
        else
        // 针对Peer的RTCSessionDescription生成兼容的本地SDP
            conn.createAnswer( conn.remoteDescription, gotDescription );
 
        function gotDescription( desc ) {
            // 设置自己的RTCSessionDescription
            conn.setLocalDescription( desc );
            // 把自己的RTCSessionDescription发送给Peer
            signalingChannel.send( JSON.stringify( { "sdp": desc } ) );
        }
    } );
}
 
// 通信发起方调用：
start( true );
```

#### 流转发
主要牵涉到的接口是RTCPeerConnection，上面的例子中已经包含了此接口的用法。WebRTC在底层做很多复杂的工作，这些工作对于JavaScript来说是透明的： 
```js
执行解码
屏蔽丢包的影响
点对点通信：WebRTC引入流交互式连接建立（Interactive Connectivity Establishment，ICE）框架。ICE负责建立点对点链路的建立：
	首先尝试直接
	不行的话尝试STUN（Session Traversal Utilities for NAT）协议。此协议通过一个简单的保活机制确保NAT端口映射在会话期间有效
	仍然不行尝试TURN（Traversal Using Relays around NAT）协议。此协议依赖于部署在公网上的中继服务器。只要端点可以访问TURN服务器就可以建立连接
通信安全
带宽适配
噪声抑制
动态抖动缓冲（dynamic jitter buffering），抖动是由于网络状况的变化，缓冲用于收集、存储数据，定期发送
```

#### 任意数据交换
通过RTCDataChannel完成，允许点对点之间任意的数据交换。RTCPeerConnection连接创建后，不但可以传输音视频流，还可以打开多个信道（RTCDataChannel）进行任意数据的交换。RTCDataChanel的特点是：  
类似于WebSocket的API  
支持带优先级的多通道  
超低延迟，因为不需要通过服务器中转  
支持可靠/不可靠传输语义。支持SCTP、DTLS、UDP几种传输协议  
内置安全传输（DTLS）  
内置拥塞控制   
使用RTCDataChannel可以很好的支持游戏、远程桌面、实时文本聊天、文件传输、去中心化网络等业务场景。  

adapter.js  
WebRTC adapter是一个垫片库，使用它开发WebRTC应用时，不需要考虑不同浏览器厂商的API前缀差异。   

#### WebRTC示例
```js
限定分辨率
// 指定分辨率
// adapter.js 支持Promise
navigator.mediaDevices.getUserMedia( { video: { width: { exact: 640 }, height: { exact: 480 } } } ).then( stream => {
    let video = document.createElement( 'video' );
    document.body.appendChild( video );
    video.srcObject = stream;
    video.play();
} ).catch( err => console.log( err ) );

在画布中截图
// video为video元素
canvas.getContext('2d').drawImage(video, 0, 0, canvas.width, canvas.height);
```

<div>
<table class="full-width fixed-word-wrap">
<thead>
<tr>
<td style="width: 20%; text-align: center;">框架</td>
<td style="text-align: center;">说明</td>
</tr>
</thead>
<tbody>
<tr>
<td><a href="http://peerjs.com/">PeerJS </a></td>
<td>
<p>简化WebRTC的点对点通信、视频、音频调用</p>
<p>提供云端的PeerServer，你也可以自己搭建服务器</p>
</td>
</tr>
<tr>
<td><a href="https://github.com/peer5/sharefest">Sharefest</a></td>
<td>基于Web的P2P文件共享</td>
</tr>
<tr>
<td><a href="https://github.com/webRTC-io/webRTC.io">webRTC.io</a></td>
<td>
<p>WebRTC的一个抽象层，同时提供了客户端、服务器端Node.js组件。服务器端组件抽象了STUN</p>
<p>类似的框架还有<a href="https://github.com/andyet/SimpleWebRTC">SimpleWebRTC</a>、<a href="https://github.com/priologic/easyrtc">easyrtc</a></p>
</td>
</tr>
<tr>
<td><a href="https://www.openwebrtc.org/">OpenWebRTC</a></td>
<td>
<p>允许你构建能够和遵循WebRTC标准的浏览器进行通信的Native应用程序，支持Java绑定</p>
</td>
</tr>
<tr>
<td><a href="https://nextrtc.org/">NextRTC</a></td>
<td>
<p>基于Java实现的WebRTC信号处理服务器</p>
</td>
</tr>
<tr>
<td><a href="https://github.com/meetecho/janus-gateway">Janus</a></td>
<td>
<p>这是一个WebRTC网关，纯服务器端组件，目前仅仅支持Linux环境下安装</p>
<p>Janus本身实现了到浏览器的WebRTC连接机制，支持以JSON格式交换数据，支持在服务器端应用逻辑 - 浏览器之间中继RTP/RTCP和消息。特殊化的功能有服务器端插件完成</p>
<p>官网地址：<a href="https://janus.conf.meetecho.com/index.html">https://janus.conf.meetecho.com</a></p>
</td>
</tr>
<tr>
<td><a href="https://www.kurento.org">Kurento</a></td>
<td>
<p>这是一个开源的WebRTC媒体服务器</p>
</td>
</tr>
</tbody>
</table>
</div>

### 6.rtsp web端播放
浏览器无法直接使用RTSP协议，因此，需要有服务器端来处理视频源的RTSP，将其转换为：
```js
1.通过WebSocket发送的视频片断，由客户端的：
    1.JSMpeg/Broadway直接解码，渲染到画布
    2.或者，构造MP4片断Feed给MSE播放
2.或者，通过WebRTC网关，转换后提供给客户端的WebRTC代码处理
3.或者，使用浏览器插件机制，例如Chrome的NaCl
```

#### 实现方式一：MSE
Streamedian的服务器端需要授权，我们选用了另外一个实现。

H5S是一个基于live555实现的开源的HTML5 RTSP网关，支持将RTSP/H264流输入转换为HTML5 MSE支持的H264，客户端基于MSE。

```js
1.服务器  
尝试在容器中运行H5S：
docker create --name ubuntu-16.04 -h ubuntu-16 --network local --dns 172.21.0.1 --ip 172.21.0.6 -it docker.gmem.cc/ubuntu:16.04 bash
docker start ubuntu-16.04
docker exec -it ubuntu-16.04 bash
apt update && apt install wget
wget https://raw.githubusercontent.com/veyesys/release/master/h5stream/H5S-r1.0.1128.16-Ubuntu-16.04-64bit.tar.gz
tar xzf H5S-r1.0.1128.16-Ubuntu-16.04-64bit.tar.gz && mv H5S-r1.0.1128.16-Ubuntu-16.04-64bit h5s-1.0
cd h5s-1.0
export LD_LIBRARY_PATH=`pwd`/lib/:$LD_LIBRARY_PATH
# 指定两次密码，可能H5S存在bug，不这样报身份验证失败
./h5ss rtsp://admin:12345@192.168.0.196:554/ch1/sub/av_stream admin 12345

客户端
使用H5S自带的基于MSE的客户端代码 + Chrome 49，播放后发现画面静止。控制它查看发现解码错误。打开chrome://media-internals/，发现错误Media segment did not begin with key frame. Support for such segments will be available in a future version。看样子是提供给SourceBuffer的数据不是以关键帧开始导致，未来版本的Chrome可能取消此限制。
换成Chrome 50，可以正常播放，但是流畅度较差，播放一段时间后出现卡死的情况。

小结
H5S实现不完善，在不修改源码的情况下，服务器端只能接入一路视频输入。客户端也存在不流畅、卡死的问题，不适合生产环境。
```

#### 实现方式二：JSMpeg
```js
转码进程
在上文中我们已经成功尝试了利用JSMpege + WebSocket的方式，在网页中显示摄像头捕获的视频。ffmpeg转换RTSP也是非常简单的：
ffmpeg -i rtsp://admin:12345@192.168.0.196:554/ch1/main/av_stream -s 427x240 -f mpegts -vcodec mpeg1video -b 800k -r 30 http://127.0.0.1:8800/12345

服务器
可以使用JSMpeg自带的简单Node.js服务器测试：
node ./app/websocket-relay.js 12345 8800 8801 

客户端
下面是客户端代码，默认JSMpeg会基于WebGL渲染，但是我的机器最多开到8画面，开9画面时出现警告：
Too many active WebGL contexts. Oldest context will be lost，且第一画面丢失，简单的通融方法是，第9画面使用Canvas2D渲染：
new JSMpeg.Player( 'ws://127.0.0.1:8801/', {
    canvas: document.getElementById( 'canvas9' ),
    autoplay: true,
    // 浏览器对WebGL context的数量有限制
    disableGl: true
} ); 
小结
这种方式客户端解码压力较大，同时开9画面的352x288视频，我的机器上CPU占用率大概到40%左右，画面变化较为剧烈的时候会出现卡顿现象。
```

#### 实现方式三：Broadway
```js
与JSMpeg类似，Broadway也是JavaScript解码工具。关键之处是，Broadway支持的视频编码是H.264，意味着可能免去消耗服务器资源的视频重编码。

最初的尝试并不顺利，根据IP摄像头的RTSP Describe应答（SDP），我们推断其H.264 Profile为Baseline，但是不转码的情况下Broadway根本无法播放。后来查看ffmpeg的日志输出，发现其实际上使用的Profile是Main。进一步尝试，发现摄像头是可以配置为Baseline的：

只需要把编码复杂度设置为低，H.264的Profile就从Main变为Baseline。

设置完毕后，仍然基于h264-live-player的Demo进行测试，使用如下命令行抽取原始H.264帧：
ffmpeg -i rtsp://admin:12345@192.168.0.196:554/ch1/main/av_stream -c:v copy -f rawvideo 
即可免转码的进行实时视频预览了。 
此实现方式更多细节信息请参考基于Broadway的HTML5视频监控。
```

#### 实现方式四：NaCl
Chrome放弃NPAPI之后，插件开发需要使用PPAPI /NaCl。目前能找到的实现有VXG Chrome Plugin，这是一个商业产品，需要授权。除了RTSP之外，还支持RTMP、HLS等协议。

插件方案的缺点是，需要安装，而且仅仅针对单种浏览器。优势则是灵活性高，理论上性能可以做的很好。

#### 实现方式五：WebRTC
WebRTC相关的框架非常多，经过简单的比较，我们决定从Kurento入手。主要原因是：

容易扩展的模块化设计
提供Java客户端、JS客户端
可以在服务器端合成多画面，这样可以减轻客户端解码压力，特别是那些低配置的客户端
内置对RTSP协议的支持
基于Kurento搭建WebRTC服务器一文详细讨论了这种实现方式。

#### 实现方式六：基于 flv.js 的 RTSP 播放方案
```js
服务器端（主进程）
服务器端采用 express + express-ws 框架进行编写，当有 HTTP 请求发送到指定的地址时，启动 ffmpeg 串流程序，直接将 RTSP 流封装成 FLV 格式的视频流，推送到指定的 WebSocket 响应流中。
import * as express from "express";
import * as expressWebSocket from "express-ws";
import ffmpeg from "fluent-ffmpeg";
import webSocketStream from "websocket-stream/stream";
import WebSocket from "websocket-stream";
import * as http from "http";

function localServer() {
    let app = express();
    app.use(express.static(__dirname));
    expressWebSocket(app, null, {
        perMessageDeflate: true
    });
    app.ws("/rtsp/:id/", rtspRequestHandle)
    app.listen(8888);
    console.log("express listened")
}

function rtspRequestHandle(ws, req) {
    console.log("rtsp request handle");
    const stream = webSocketStream(ws, {
        binary: true,
        browserBufferTimeout: 1000000
    }, {
        browserBufferTimeout: 1000000
    });
    let url = req.query.url;
    console.log("rtsp url:", url);
    console.log("rtsp params:", req.params);
    try {
        ffmpeg(url)
            .addInputOption("-rtsp_transport", "tcp", "-buffer_size", "102400")  // 这里可以添加一些 RTSP 优化的参数
            .on("start", function () {
                console.log(url, "Stream started.");
            })
            .on("codecData", function () {
                console.log(url, "Stream codecData.")
          			// 摄像机在线处理
            })
            .on("error", function (err) {
                console.log(url, "An error occured: ", err.message);
            })
            .on("end", function () {
                console.log(url, "Stream end!");
          			// 摄像机断线的处理
            })
            .outputFormat("flv").videoCodec("copy").noAudio().pipe(stream);
    } catch (error) {
        console.log(error);
    }
}


浏览器端（渲染进程）
示例使用 Vue 框架进行页面设计。
<template>
    <div>
        <video class="demo-video" ref="player"></video>
    </div>
</template>

<script>
import flvjs from "flv.js";
export default {
    props: {
        rtsp: String,
        id: String
    },
    /**
     * @returns {{player: flvjs.Player}}
     */
    data () {
        return {
            player: null
        }
    },
    mounted () {
        if (flvjs.isSupported()) {
            let video = this.$refs.player;
            if (video) {
                this.player = flvjs.createPlayer({
                    type: "flv",
                    isLive: true,
                    url: `ws://localhost:8888/rtsp/${this.id}/?url=${this.rtsp}`
                });
                this.player.attachMediaElement(video);
                try {
                    this.player.load();
                    this.player.play();
                } catch (error) {
                    console.log(error);
                };
            }
        }
    },
    beforeDestroy () {
        this.player.destory();
    }
}
</script>

<style>
    .demo-video {
        max-width: 480px; 
        max-height: 360px;
    }
</style>
```

## <a id="3">3.node+ffmpeg实现rtsp转流</a>
参考链接：
https://juejin.im/post/6844903936797704199   第一种方案   
https://juejin.im/post/6844903949309313037   第二种方案   

一种方案是采用脚本调用ffmpeg拉流rtsp转发到rtmp。   
二种方案是node实现，项目地址：https://github.com/JackFlyL/node-video.git。    

<img src="./image/2-201.png" style="zoom:100%" />

### 技能栈
```js
前端：
jsmpeg.js (连接wbesocket，绘制视频图像)

服务端（node）：
express（静态服务，接口处理）
body-parser（处理接口接收数据）
node-rtsp-stream-jsmpeg （下发FFmpeg指令，创建websocke服务端）
child_process（启动shell脚本指令）

其他：
FFmpeg(视频流转码)
pm2(node进程守护)
云台控制程序（通过视频厂家提供的sdk二次开发，通过ajax调用）


```

## links
  * [目录](<音视频入门到精通目录.md>)