# 3.1音视频库使用说明

## 1.1FFMPEG
### 1.1.1API学习整理
avcodec_register_all：注册全部编码器。有指定编码器得API。  
avformat_network_init：网路库初始化。如果avformat调用网络功能需要使用。  
av_register_all：注册avformat复用器。  
avformat_open_input：打开视频文件。  

## 1.2 ffmpeg问题
### 1.2.1 ffmpeg添加mpeg ps流的pcm的解码支持
https://blog.csdn.net/garefield/article/details/45113313  
ffmpeg原本不支持mpeg ps流，原因有两个。首先在编码时，ffmpeg没有在流中写入psm头；其次，在解码时，只在读文件头的时候判断了流的类型，而ps流中，流类型是在psm头中的，这样ffmpeg会读不到流类型。mpeg.c函数中mpegps_read_packet函数中做修改。  

### 1.2.2 ffmpeg加载慢
Vlc 调用ffmpeg 中avformat_find_stream_info读取信息慢，可以修改probesize = 4096。

## 3.Live555
### 3.1分析live555客户端解析play等命令流程 
首先RTSPClient 新建一个createNew，RTSPClient.cpp/RTSPClient 调用taskScheduler，新建task绑定incomingDataHandler，到handleResponseBytes，到handlePLAYResponse，等待接受到发送的play发送响应。具体rtsp交互流程可查看rtsp中相关章节。

## 2.2 rtsp时间戳计算方法
Rtsp的拖放和快放、慢放都是先pause，再play，根据play消息包中的range和scale参数来实现。  
对应到live555中，pause时调用sendPauseCommand函数。Play时调用unsigned sendPlayCommand(MediaSubsession& subsession, responseHandler* responseHandler,double start = 0.0f, double end = -1.0f, float scale = 1.0f,   Authenticator*authenticator = NULL)，在该函数中可以设置start，end（对应range，以秒为单位）和scale（对应快慢放）。end设为-1表示播放到文件结尾。在pause后的resume播放时，start可以设为-1，表示继续从当前位置播放；在seek时，start设为拖放到的播放时间点（以秒为单位）。  
rtsp客户端对于每个收到的rtp数据包，可以根据时间戳调用doubleMediaSubsession::getNormalPlayTime(struct timeval const& presentationTime)获得当前相对文件开始（从0起始）的播放时间（秒）。 
该函数的计算公式为：PlayStartTime+(currnpt-startnpt)*scale  
PlayStartTIme：从收到服务器play应答中的range域中提取。  
Currnpt：当前rtp包的时间戳  
Startnpt：play后收到的第一个数据包的npt。  
rtspserver中的rtp包时间戳实现：  
每收到一个play请求后（seek和resume的play请求同样如此），取当前时间作为时间戳。以后每个数据包的时间戳计算公式为：play后第一个包的时间戳+当前包与第一个包在文件中的相对时间间隔。注意在Play的响应包中音视频的seq和rtptime分别要取pause时的值，否则客户端的时间条指示会不对。  
在live555的rtspserver中，收到resume的play请求时（range域start为-1），返回的应答包不会置range域start时间，导致客户端提取PlayStartTIme为0。为了避免这种问题，客户端在resume的play请求中最好置start为当前播放时间，这样其实类似于seek操作了。  
在读取文件结束时（音频或视频结束），构造RTCP  BYE包，通过RTCP通道往客户端发送一个BYE包，客户端会结束播放。  

## links
  * [目录](<音视频入门到精通目录.md>)